<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Linear Models</title>
  <meta name="description" content="Course notes of LIMO" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes of LIMO" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Linear Models" />
  
  <meta name="twitter:description" content="Course notes of LIMO" />
  

<meta name="author" content="Olivier Thas" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  


<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path=""><a href="#Ch_Introduction"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path=""><a href="#versions-and-changes"><i class="fa fa-check"></i><b>1.1</b> Versions and Changes</a></li>
<li class="chapter" data-level="1.2" data-path=""><a href="#introduction"><i class="fa fa-check"></i><b>1.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3" data-path=""><a href="#how-to-work-through-the-course-notes"><i class="fa fa-check"></i><b>1.3</b> How to work through the course notes</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#exercise"><i class="fa fa-check"></i>Exercise</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path=""><a href="#two-versions-of-this-course-oc-and-dl"><i class="fa fa-check"></i><b>1.4</b> Two versions of this course: OC and DL</a></li>
<li class="chapter" data-level="1.5" data-path=""><a href="#communication"><i class="fa fa-check"></i><b>1.5</b> Communication</a></li>
<li class="chapter" data-level="1.6" data-path=""><a href="#software"><i class="fa fa-check"></i><b>1.6</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path=""><a href="#Ch:Reg1"><i class="fa fa-check"></i><b>2</b> Simple Linear Regression Analysis</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data"><i class="fa fa-check"></i>Example (Galton’s height data)</a></li>
<li class="chapter" data-level="2.1" data-path=""><a href="#S:RegSimStudy"><i class="fa fa-check"></i><b>2.1</b> Interpretation via simulations</a></li>
<li class="chapter" data-level="2.2" data-path=""><a href="#S:LSE1"><i class="fa fa-check"></i><b>2.2</b> Least squares estimators</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-1"><i class="fa fa-check"></i>Example (Galton’s height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure"><i class="fa fa-check"></i>Exercise: blood pressure</a></li>
<li class="chapter" data-level="2.3" data-path=""><a href="#S:PropLSE"><i class="fa fa-check"></i><b>2.3</b> Properties of the Least Squares Estimator</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path=""><a href="#mean-and-variance-of-the-lse"><i class="fa fa-check"></i><b>2.3.1</b> Mean and variance of the LSE</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study"><i class="fa fa-check"></i>Exercise: simulation study</a>
<ul>
<li class="chapter" data-level="2.3.2" data-path=""><a href="#best-linear-unbiased-estimator-blue"><i class="fa fa-check"></i><b>2.3.2</b> Best Linear Unbiased Estimator (BLUE)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study-1"><i class="fa fa-check"></i>Exercise: Simulation study</a>
<ul>
<li class="chapter" data-level="2.3.3" data-path=""><a href="#sampling-distribution-of-the-lse"><i class="fa fa-check"></i><b>2.3.3</b> Sampling distribution of the LSE</a></li>
<li class="chapter" data-level="2.3.4" data-path=""><a href="#maximum-likelihood-estimator-of-hatmbbeta"><i class="fa fa-check"></i><b>2.3.4</b> Maximum likelihood estimator of <span class="math inline">\(\hat{\mb\beta}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path=""><a href="#an-estimator-of-sigma2"><i class="fa fa-check"></i><b>2.4</b> An Estimator of <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-simulation-study-2"><i class="fa fa-check"></i>Exercise: Simulation study</a></li>
<li class="chapter" data-level="2.5" data-path=""><a href="#sampling-distributions-of-the-standardised-and-the-studentised-lse"><i class="fa fa-check"></i><b>2.5</b> Sampling Distributions of the Standardised and the Studentised LSE</a></li>
<li class="chapter" data-level="2.6" data-path=""><a href="#S:BIReg1"><i class="fa fa-check"></i><b>2.6</b> Confidence Intervals</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-2"><i class="fa fa-check"></i>Example (Galton’s height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure-1"><i class="fa fa-check"></i>Exercise: Blood Pressure</a></li>
<li class="chapter" data-level="2.7" data-path=""><a href="#S:RegTests"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-3"><i class="fa fa-check"></i>Example (Galton’s height data)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-converting-two-sided-p-values"><i class="fa fa-check"></i>Exercise: Converting two-sided p-values</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-muscle-mass"><i class="fa fa-check"></i>Exercise: Muscle mass</a></li>
<li><a href="#exercise-choosing-h_1-after-looking-at-the-data">Exercise: choosing <span class="math inline">\(H_1\)</span> after looking at the data</a></li>
<li class="chapter" data-level="2.8" data-path=""><a href="#S:AssessAssumptions"><i class="fa fa-check"></i><b>2.8</b> Assessment of the Model Assumptions</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height-data-4"><i class="fa fa-check"></i>Example (Galton’s height data)</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#normality-of-the-error-term"><i class="fa fa-check"></i>Normality of the error term</a></li>
<li class="chapter" data-level="" data-path=""><a href="#homoskedasticity"><i class="fa fa-check"></i>Homoskedasticity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-muscle-mass-1"><i class="fa fa-check"></i>Exercise: Muscle mass</a></li>
<li class="chapter" data-level="2.9" data-path=""><a href="#binary-dummy-regressors"><i class="fa fa-check"></i><b>2.9</b> Binary Dummy Regressors</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example"><i class="fa fa-check"></i>Example</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-smoking"><i class="fa fa-check"></i>Exercise: Smoking</a></li>
<li class="chapter" data-level="2.10" data-path=""><a href="#S:Causality"><i class="fa fa-check"></i><b>2.10</b> Association versus Causation</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path=""><a href="#introduction-1"><i class="fa fa-check"></i><b>2.10.1</b> Introduction</a></li>
<li class="chapter" data-level="2.10.2" data-path=""><a href="#causal-inference-and-counterfactuals"><i class="fa fa-check"></i><b>2.10.2</b> Causal inference and counterfactuals</a></li>
<li class="chapter" data-level="2.10.3" data-path=""><a href="#randomised-studies"><i class="fa fa-check"></i><b>2.10.3</b> Randomised studies</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path=""><a href="#S:Reg1BIPI"><i class="fa fa-check"></i><b>2.11</b> Estimation of the Conditional Mean Outcome</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure"><i class="fa fa-check"></i>Example (Blood Pressure)</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#sampling-distribution"><i class="fa fa-check"></i>Sampling Distribution</a></li>
<li class="chapter" data-level="" data-path=""><a href="#confidence-interval-ci"><i class="fa fa-check"></i>Confidence Interval (CI)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-1"><i class="fa fa-check"></i>Example (Blood Pressure)</a></li>
<li class="chapter" data-level="2.12" data-path=""><a href="#S:PI"><i class="fa fa-check"></i><b>2.12</b> Predictions and Prediction Intervals (PI)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-2"><i class="fa fa-check"></i>Example (Blood Pressure)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-3"><i class="fa fa-check"></i>Example (Blood Pressure)</a>
<ul>
<li class="chapter" data-level="2.12.1" data-path=""><a href="#simulation-study"><i class="fa fa-check"></i><b>2.12.1</b> Simulation Study</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-galtons-data"><i class="fa fa-check"></i>Exercise: Galton’s data</a></li>
<li class="chapter" data-level="2.13" data-path=""><a href="#decomposition-of-the-total-sum-of-squares"><i class="fa fa-check"></i><b>2.13</b> Decomposition of the Total Sum of Squares</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-galtons-height"><i class="fa fa-check"></i>Example (Galton’s height)</a></li>
<li><a href="#exercise-r2-and-prediction">Exercise: <span class="math inline">\(R^2\)</span> and prediction</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path=""><a href="#Ch:Reg2"><i class="fa fa-check"></i><b>3</b> Multiple Linear Regression Analysis</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="3.1" data-path=""><a href="#S:AddMeervoudigModel"><i class="fa fa-check"></i><b>3.1</b> The Additive Multiple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#the-statistical-model"><i class="fa fa-check"></i>The Statistical Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-1"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
<li class="chapter" data-level="3.2" data-path=""><a href="#the-non-additive-multiple-linear-regression-model"><i class="fa fa-check"></i><b>3.2</b> The Non-Additive Multiple Linear Regression Model</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path=""><a href="#interaction"><i class="fa fa-check"></i><b>3.2.1</b> Interaction</a></li>
<li class="chapter" data-level="3.2.2" data-path=""><a href="#parameter-estimators"><i class="fa fa-check"></i><b>3.2.2</b> Parameter Estimators</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-2"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-4"><i class="fa fa-check"></i>Example (Blood Pressure)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure-2"><i class="fa fa-check"></i>Exercise: Blood Pressure</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-3"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration-1"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
<li class="chapter" data-level="3.3" data-path=""><a href="#the-anova-table"><i class="fa fa-check"></i><b>3.3</b> The ANOVA Table</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path=""><a href="#sstot-ssr-and-sse"><i class="fa fa-check"></i><b>3.3.1</b> SSTot, SSR and SSE</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path=""><a href="#multicollinearity"><i class="fa fa-check"></i><b>3.4</b> Multicollinearity</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-4"><i class="fa fa-check"></i>Example (Lead concentration)</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path=""><a href="#illustrations-via-simulations"><i class="fa fa-check"></i><b>3.4.1</b> Illustrations via Simulations</a></li>
</ul></li>
<li><a href="#excercise-repeatedly-fitting-model-with-rho0.99">Excercise: repeatedly fitting model with <span class="math inline">\(\rho=0.99\)</span></a>
<ul>
<li class="chapter" data-level="3.4.2" data-path=""><a href="#variance-inflation-factor"><i class="fa fa-check"></i><b>3.4.2</b> Variance Inflation Factor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-5"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="3.5" data-path=""><a href="#leverage"><i class="fa fa-check"></i><b>3.5</b> Leverage</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-6"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="3.6" data-path=""><a href="#assessment-of-the-model-assumptions-and-remedial-measures"><i class="fa fa-check"></i><b>3.6</b> Assessment of the Model Assumptions and Remedial Measures</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#residual-analysis"><i class="fa fa-check"></i>Residual analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-7"><i class="fa fa-check"></i>Example (Lead Concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration-2"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-blood-pressure-3"><i class="fa fa-check"></i>Exercise: Blood pressure</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-bacterial-count"><i class="fa fa-check"></i>Example (Bacterial count)</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#remedial-measures"><i class="fa fa-check"></i>Remedial measures</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-bacterial-count-1"><i class="fa fa-check"></i>Example (Bacterial count)</a></li>
<li class="chapter" data-level="3.7" data-path=""><a href="#model-selection"><i class="fa fa-check"></i><b>3.7</b> Model Selection</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#selection-methods-based-on-hypothesis-testing"><i class="fa fa-check"></i>Selection methods based on hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-8"><i class="fa fa-check"></i>Example (Lead concentration)</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#model-selection-for-building-a-prediction-model"><i class="fa fa-check"></i>Model selection for building a prediction model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-lead-concentration-3"><i class="fa fa-check"></i>Exercise: Lead concentration</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path=""><a href="#Ch:DesignCausal"><i class="fa fa-check"></i><b>4</b> Design-related Topics and Causal Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path=""><a href="#confounding"><i class="fa fa-check"></i><b>4.1</b> Confounding</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path=""><a href="#causal-diagrams-and-controling-for-confounders"><i class="fa fa-check"></i><b>4.1.1</b> Causal diagrams and controling for confounders</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-lead-concentration-9"><i class="fa fa-check"></i>Example (Lead concentration)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-diabetes"><i class="fa fa-check"></i>Example (Diabetes)</a>
<ul>
<li class="chapter" data-level="4.1.2" data-path=""><a href="#causality-and-confounders"><i class="fa fa-check"></i><b>4.1.2</b> Causality and confounders</a></li>
<li class="chapter" data-level="4.1.3" data-path=""><a href="#colliders"><i class="fa fa-check"></i><b>4.1.3</b> Colliders</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-study-time"><i class="fa fa-check"></i>Example (Study time)</a></li>
<li class="chapter" data-level="4.2" data-path=""><a href="#collapsibility"><i class="fa fa-check"></i><b>4.2</b> Collapsibility</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-blood-pressure-5"><i class="fa fa-check"></i>Example (Blood pressure)</a></li>
<li class="chapter" data-level="4.3" data-path=""><a href="#randomisation-restriction"><i class="fa fa-check"></i><b>4.3</b> Randomisation restriction</a></li>
<li class="chapter" data-level="4.4" data-path=""><a href="#sample-size-and-power"><i class="fa fa-check"></i><b>4.4</b> Sample size and power</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-two-01-dummies"><i class="fa fa-check"></i>Example (two 0/1 dummies)</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path=""><a href="#Ch:ANOVA"><i class="fa fa-check"></i><b>5</b> Analysis of Variance</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-post-weaning-diarrhea"><i class="fa fa-check"></i>Example (PWD: Post-weaning diarrhea)</a></li>
<li class="chapter" data-level="5.1" data-path=""><a href="#S:ANOVA1"><i class="fa fa-check"></i><b>5.1</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path=""><a href="#S:ANOVA1Mu3"><i class="fa fa-check"></i><b>5.1.1</b> Comparison of Three Means</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd"><i class="fa fa-check"></i>Example (PWD)</a></li>
<li class="chapter" data-level="5.1.2" data-path=""><a href="#the-anova-model"><i class="fa fa-check"></i><b>5.1.2</b> The ANOVA model</a></li>
<li class="chapter" data-level="5.1.3" data-path=""><a href="#parameter-estimators-1"><i class="fa fa-check"></i><b>5.1.3</b> Parameter Estimators</a></li>
<li class="chapter" data-level="5.1.4" data-path=""><a href="#S:ANOVA1SS"><i class="fa fa-check"></i><b>5.1.4</b> Sum of Squares</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path=""><a href="#S:FTest"><i class="fa fa-check"></i><b>5.2</b> The <span class="math inline">\(F\)</span>-test</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path=""><a href="#introduction-2"><i class="fa fa-check"></i><b>5.2.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2.2" data-path=""><a href="#the-general-linear-hypothesis"><i class="fa fa-check"></i><b>5.2.2</b> The General Linear Hypothesis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-1"><i class="fa fa-check"></i>Example (PWD)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-pwd---fcr"><i class="fa fa-check"></i>Exercise: PWD - FCR</a></li>
<li class="chapter" data-level="5.3" data-path=""><a href="#contrasts"><i class="fa fa-check"></i><b>5.3</b> Contrasts</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path=""><a href="#setting-the-reference-level"><i class="fa fa-check"></i><b>5.3.1</b> Setting the reference level</a></li>
<li class="chapter" data-level="5.3.2" data-path=""><a href="#examples-of-other-contrasts"><i class="fa fa-check"></i><b>5.3.2</b> Examples of other contrasts</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path=""><a href="#S:ANOVA2"><i class="fa fa-check"></i><b>5.4</b> The Two-way ANOVA</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path=""><a href="#the-additive-model"><i class="fa fa-check"></i><b>5.4.1</b> The additive model</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-interpretation-of-the-parameters"><i class="fa fa-check"></i>Exercise: interpretation of the parameters</a></li>
<li class="chapter" data-level="5.4.2" data-path=""><a href="#sum-of-squares"><i class="fa fa-check"></i><b>5.4.2</b> Sum of Squares</a></li>
<li class="chapter" data-level="5.4.3" data-path=""><a href="#S:Ftest2"><i class="fa fa-check"></i><b>5.4.3</b> <span class="math inline">\(F\)</span>-tests</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-2"><i class="fa fa-check"></i>Example (PWD)</a></li>
<li class="chapter" data-level="5.4.4" data-path=""><a href="#the-non-additive-anova-model"><i class="fa fa-check"></i><b>5.4.4</b> The non-additive ANOVA model</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-interpretation-of-the-parameters-1"><i class="fa fa-check"></i>Exercise: interpretation of the parameters</a></li>
<li class="chapter" data-level="5.4.5" data-path=""><a href="#S:FInteraction"><i class="fa fa-check"></i><b>5.4.5</b> Sum of squares and <span class="math inline">\(F\)</span>-test for interaction</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-degrees-of-freedom"><i class="fa fa-check"></i>Exercise: degrees of freedom</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-3"><i class="fa fa-check"></i>Example (PWD)</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path=""><a href="#extra-sum-of-squares"><i class="fa fa-check"></i><b>5.5</b> Extra sum of squares</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path=""><a href="#the-f-test-in-a-one-way-anova"><i class="fa fa-check"></i><b>5.5.1</b> The <span class="math inline">\(F\)</span>-test in a one-way ANOVA</a></li>
<li class="chapter" data-level="5.5.2" data-path=""><a href="#an-f-test-for-nested-models"><i class="fa fa-check"></i><b>5.5.2</b> An <span class="math inline">\(F\)</span>-test for nested models</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-4"><i class="fa fa-check"></i>Example (PWD)</a></li>
<li class="chapter" data-level="5.5.3" data-path=""><a href="#S:TypesSS"><i class="fa fa-check"></i><b>5.5.3</b> types of SS</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-diabetes-1"><i class="fa fa-check"></i>Example (Diabetes)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-diabetes-2"><i class="fa fa-check"></i>Example (Diabetes)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-diabetes-3"><i class="fa fa-check"></i>Example (Diabetes)</a></li>
<li class="chapter" data-level="5.6" data-path=""><a href="#S:Multiplicity"><i class="fa fa-check"></i><b>5.6</b> Multiple comparisons of means</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path=""><a href="#formulation-of-the-multiplicity-problem"><i class="fa fa-check"></i><b>5.6.1</b> Formulation of the multiplicity problem</a></li>
<li class="chapter" data-level="5.6.2" data-path=""><a href="#the-familywise-error-rate"><i class="fa fa-check"></i><b>5.6.2</b> The Familywise Error Rate</a></li>
<li class="chapter" data-level="5.6.3" data-path=""><a href="#the-bonferroni-method"><i class="fa fa-check"></i><b>5.6.3</b> The Bonferroni method</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-5"><i class="fa fa-check"></i>Example (PWD)</a>
<ul>
<li class="chapter" data-level="" data-path=""><a href="#the-sidak-method"><i class="fa fa-check"></i>The Sidak method</a></li>
<li class="chapter" data-level="5.6.4" data-path=""><a href="#the-holm-bonferroni-method"><i class="fa fa-check"></i><b>5.6.4</b> The Holm-Bonferroni method</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-6"><i class="fa fa-check"></i>Example (PWD)</a>
<ul>
<li class="chapter" data-level="5.6.5" data-path=""><a href="#the-tukey-kramer-method"><i class="fa fa-check"></i><b>5.6.5</b> The Tukey-Kramer method</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-7"><i class="fa fa-check"></i>Example (PWD)</a>
<ul>
<li class="chapter" data-level="5.6.6" data-path=""><a href="#simultaneous-ci"><i class="fa fa-check"></i><b>5.6.6</b> Simultaneous CI</a></li>
</ul></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-8"><i class="fa fa-check"></i>Example (PWD)</a></li>
<li class="chapter" data-level="" data-path=""><a href="#exercise-equivalence-simultaneous-ci-and-fwer-control"><i class="fa fa-check"></i>Exercise: equivalence simultaneous CI and FWER control</a></li>
<li class="chapter" data-level="5.7" data-path=""><a href="#assessment-of-model-assumptions"><i class="fa fa-check"></i><b>5.7</b> Assessment of model assumptions</a></li>
<li class="chapter" data-level="" data-path=""><a href="#example-pwd-9"><i class="fa fa-check"></i>Example (PWD)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path=""><a href="#reporting"><i class="fa fa-check"></i><b>6</b> Reporting</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path=""><a href="#app:VecDiff"><i class="fa fa-check"></i><b>A</b> Vector Differentiation</a></li>
<li class="chapter" data-level="B" data-path=""><a href="#app:LinTrans"><i class="fa fa-check"></i><b>B</b> Linear Transformations of MVN</a></li>
<li class="chapter" data-level="C" data-path=""><a href="#app:Slutsky"><i class="fa fa-check"></i><b>C</b> Slutsky’s Theorem</a></li>
<li class="chapter" data-level="D" data-path=""><a href="#types-of-statistical-models"><i class="fa fa-check"></i><b>D</b> Types of Statistical Models</a></li>
<li class="chapter" data-level="E" data-path=""><a href="#r-session-info"><i class="fa fa-check"></i><b>E</b> R Session Info</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Linear Models</h1>
<p class="author"><em>Olivier Thas</em></p>
<p class="date"><em>2021-2022</em></p>
</div>
<!--- For HTML Only --->
<p><span class="math inline">\(\newcommand{\prob}[1]{\text{P}\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\mb}[1]{\boldsymbol{#1}}\)</span>
<span class="math inline">\(\newcommand{\E}[1]{\mbox{E}\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\Ef}[2]{\mbox{E}_{#1}\left\{#2\right\}}\)</span>
<span class="math inline">\(\newcommand{\cov}[1]{\mbox{Cov}\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\cor}[1]{\mbox{Cor}\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\covf}[2]{\mbox{Cov}_{#1}\left\{#2\right\}}\)</span>
<span class="math inline">\(\newcommand{\var}[1]{\mbox{Var}\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\varf}[2]{\mbox{Var}_{#1}\left\{#2\right\}}\)</span>
<span class="math inline">\(\newcommand{\ind}[0]{\perp \!\!\! \perp}\)</span>
<span class="math inline">\(\newcommand{\eps}[0]{\varepsilon}\)</span>
<span class="math inline">\(\newcommand{\SSE}[0]{\text{SSE}}\)</span>
<span class="math inline">\(\newcommand{\iid}[0]{\text{ i.i.d. }}\)</span>
<span class="math inline">\(\newcommand{\convDistr}[0]{\stackrel{d}{\longrightarrow}}\)</span>
<span class="math inline">\(\newcommand{\convProb}[0]{\stackrel{p}{\longrightarrow}}\)</span>
<span class="math inline">\(\newcommand{\QED}[0]{\null\nobreak\hfill\ensuremath{\blacksquare}}\)</span>
<span class="math inline">\(\newcommand{\MSE}[0]{\text{MSE}}\)</span>
<span class="math inline">\(\newcommand{\SSTot}[0]{\text{SSTot}}\)</span>
<span class="math inline">\(\newcommand{\SSR}[0]{\text{SSR}}\)</span>
<span class="math inline">\(\newcommand{\MSR}[0]{\text{MSR}}\)</span>
<span class="math inline">\(\newcommand{\SST}[0]{\text{SST}}\)</span>
<span class="math inline">\(\newcommand{\MST}[0]{\text{MST}}\)</span>
<span class="math inline">\(\newcommand{\SSA}[0]{\text{SSA}}\)</span>
<span class="math inline">\(\newcommand{\MSA}[0]{\text{MSA}}\)</span>
<span class="math inline">\(\newcommand{\SSTA}[0]{\text{SSTA}}\)</span>
<span class="math inline">\(\newcommand{\MSTA}[0]{\text{MSTA}}\)</span>
<span class="math inline">\(\newcommand{\probf}[2]{\text{P}_{#1}\left\{#2\right\}}\)</span>
<span class="math inline">\(\newcommand{\HSim}[0]{\stackrel{H_0}{\sim}}\)</span>
<span class="math inline">\(\newcommand{\ApproxSim}[0]{\stackrel{\cdot}{\sim}}\)</span></p>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre><code>## Warning in (function (kind = NULL, normal.kind = NULL, sample.kind = NULL) :
## non-uniform &#39;Rounding&#39; sampler used</code></pre>
<pre><code>## 
## Attaching package: &#39;plotly&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     last_plot</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     select</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     filter</code></pre>
<pre><code>## The following object is masked from &#39;package:graphics&#39;:
## 
##     layout</code></pre>
<pre><code>## Warning: package &#39;GGally&#39; was built under R version 4.1.1</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;GGally&#39;:
##   method from   
##   +.gg   ggplot2</code></pre>
<pre><code>## Warning: package &#39;dagitty&#39; was built under R version 4.1.1</code></pre>
<pre><code>## Loading required package: carData</code></pre>
<pre><code>## 
## Attaching package: &#39;car&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     recode</code></pre>
<pre><code>## Loading required package: mvtnorm</code></pre>
<pre><code>## Loading required package: survival</code></pre>
<pre><code>## Warning: package &#39;survival&#39; was built under R version 4.1.1</code></pre>
<pre><code>## Loading required package: TH.data</code></pre>
<pre><code>## 
## Attaching package: &#39;TH.data&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:MASS&#39;:
## 
##     geyser</code></pre>

<div id="Ch_Introduction" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Introduction</h1>
<div id="versions-and-changes" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Versions and Changes</h2>
<ul>
<li><p><strong>Version</strong>: September 27, 2021</p></li>
<li><p><strong>Changes</strong>: None</p></li>
</ul>
</div>
<div id="introduction" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Introduction</h2>
<p>This course (<em>Linear Models</em>) is a classical treatment of linear models, which are among the most popular and simple models for statistical data analysis and prediction. The models describe the relationship between the mean of an outcome variable <span class="math inline">\(Y\)</span> and one or more regressors <span class="math inline">\(x\)</span> via a function that is linear in the parameters. The model thus focusses on the conditional expectation <span class="math inline">\(\E{Y \mid x}\)</span>. Here is a brief overview of the content of the course:</p>
<ul>
<li><p>simple linear regression model: here the models are limited to a single regressor. In this chapter most of the concepts and theory will be presented so that in later chapters we can rely on many of the theoretical results of this chapter. Throughout the whole chapter, the methods will be illustrated by means of example datasets, and the meaning of the model and the statistical inference procedures will be demonstrated with Monte Carlo simulation studies. In particular, the following topics will be discussed:</p>
<ul>
<li><p>the model and its interpretation</p></li>
<li><p>parameter estimation (ordinary least squares and maximum likelihood) and the properties of the estimators</p></li>
<li><p>sampling distributions of the estimators. They form the basis of statistical inference.</p></li>
<li><p>confidence intervals of the parameters</p></li>
<li><p>hypothesis tests related to the parameters</p></li>
<li><p>assessment of the model assumptions</p></li>
<li><p>the use of binary dummy regressors to mimic the two-sample problem (two-sample t-test)</p></li>
<li><p>association versus causation. We will give a brief introduction to causal inference.</p></li>
<li><p>linear regression models for prediction purposes</p></li>
</ul></li>
<li><p>the multiple linear regression model: the simple linear regression model is extended to include more than one regressor. For many topics (e.g. parameter estimation, confidence intervals and hypopthesis tests) we will be able to refer to the previous chapter, which will make this chapter less theoretical. Quite some focus will be on the interpretation of the parameters and specific issues that are irrelevant for the simple linear regression model. These topics will be discussed:</p>
<ul>
<li><p>the additive model and its interpretation</p></li>
<li><p>parameter estimation, confidence intervals and hypothesis testing (with a lot of references to the previous chapter)</p></li>
<li><p>interaction effects and the non-additive model</p></li>
<li><p>sum of squares and the ANOVA table</p></li>
<li><p>multicolinearity</p></li>
<li><p>assessment of the model assumptions</p></li>
<li><p>prediction modelling and model selection</p></li>
</ul></li>
<li><p>design-related topics and causal inference. In this chapter we discuss in some more detail the importance of the study design and some further concepts in causal inference. In particular:</p>
<ul>
<li><p>blocking and stratification</p></li>
<li><p>estimability</p></li>
<li><p>randomisation</p></li>
<li><p>confouders</p></li>
<li><p>causality, causal graphs, collapsibility</p></li>
</ul></li>
<li><p>Analysis of variance (ANOVA). The approach taken is this course, is to embed anova models in regression models. So in this chapter you first learn about the conventional formulation of anova models, and subsequently how these models can be rewritten as linear regression models so that the theory and methods from the previous chapters become available. The following topics will be treated:</p>
<ul>
<li><p>cell means and factor effects models and their reformulation as linear regression models</p></li>
<li><p>sum of squares and the ANOVA table</p></li>
<li><p>one way, two way and multiple way models</p></li>
<li><p>contrasts and multiple comparisons of means</p></li>
</ul></li>
<li><p>This course is concluded with a chapter on reporting: how to write a good statistical report. This can be seen as a stand alone chapter and can be read at any time.</p></li>
</ul>
</div>
<div id="how-to-work-through-the-course-notes" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> How to work through the course notes</h2>
<p>These course notes are written with R Bookdown in R Studio, and rendered to an html file that is made available on GitHub. If you wish, you can convert the html to a pdf file. The best way to do this, is via the <em>print</em> function of your browser. Before doing so, you can best first collapse the sidebar (that contains the table of content).</p>
<p>This course is work in progress. It is a new course and <strong>the course notes are still work in progress</strong>. So every week there will be an update. If you access the course notes via the link to GitHub you will see the updates automatically.</p>
<p>Many <strong>data analyses examples</strong> are worked out in detail in the course notes, including the R code. When you access the course notes via GitHub, you can easily copy the R code and paste it to your local R software. Here is an example of a chunck of R code:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>a<span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>b<span class="ot">&lt;-</span><span class="dv">2</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>a<span class="sc">+</span>b</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>If you now move your mouse towards the upper right corner of the box that contains the R code, then you will see an icon appearing in that corner:</p>
<p><img src="figures/copy.png" width="5%" style="display: block; margin: auto;" />
If you click on that icon, the content of the box will be copied to your clipboard so that you can paste it wherever you want (e.g. in a local R file).</p>
<p>I actually advise that you also <strong>work with your own local R Markdown or Notebook file</strong> to which you copy some of the R code of the course notes so that you can <em>play</em> with the data and the R code yourself. All datafiles are also available on Blackboard.</p>
<p>Throughout the course there are several exercises. The introduction to the excercise, as well as the data set and some instructions are given, but you do not directly get to see the solution. However, there is a option to expand the html so that the solution becomes visible. This should invite you to first try to make the exercise yourself (in a local R Markdown or Notebook file) before looking at the solution in the course notes. Here is an illustration:</p>
<div id="exercise" class="section level3 unnumbered">
<h3>Exercise</h3>
<p>Consider the following data.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>dataset<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">x=</span><span class="fu">c</span>(<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>))</span></code></pre></div>
<p>Make a scatter plot of <span class="math inline">\(y\)</span> versus <span class="math inline">\(x\)</span>.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<p>Here is a solution:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dataset<span class="sc">$</span>x,dataset<span class="sc">$</span>y,</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;x&quot;</span>,</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;y&quot;</span>)</span></code></pre></div>
<img src="DASM2_files/figure-html/unnamed-chunk-6-1.png" width="672" />
</details>
</div>
</div>
<div id="two-versions-of-this-course-oc-and-dl" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Two versions of this course: OC and DL</h2>
<p>As for all courses in our Master of Statistics and Data Science program, each course has two versions: the <strong>on-campus (OC)</strong> and the <strong>distance learning (DL)</strong> versions.</p>
<p>The course notes are the same for both groups of students. Each group, however, has another Blackboard course site at which you can find all files and information. I keep these two Blackboard sites separate because the contact modes and hours are different:</p>
<ul>
<li><p>the OC students have live lectures (either in class, on campus, if permitted in corona times, or online) and self study assignments followed by online Q&amp;A sessions. Their lectures and Q&amp;A sessions are scheduled in their official course schedule.</p></li>
<li><p>the DL students will get access to web lectures and they can watch them at their own pace. On Blackboard I will suggest a time schedule. There are also online Q&amp;A sessions which give the students the opportunity to ask questions to the lecturer. The dates of the Q&amp;A sessions are in the Blackboard Calendar.</p></li>
</ul>
<p>The project assignment is the same for the DL and OC students.</p>
</div>
<div id="communication" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Communication</h2>
<p>On Blackboard there is a <strong>discussion forum</strong> that can be used for asking questions.</p>
<p>Questions can also be asked during the <strong>Q&amp;A sessions</strong>, and the OC students can of course also ask questions in class (either physically when we are on-campus, or in the virtual class room when we are online).</p>
<p>You are also welcome to send emails to the lecturer, but the discussion forum is preferred so that other students can also learn from the answers to the questions.</p>
</div>
<div id="software" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Software</h2>
<p>In the course notes all data analyses are demonstrated with the R software, but on Blackboard also documentation on the SAS software will be provided. You will also have to learn how to perform regression analysis and ANOVA with the SAS software.</p>

</div>
</div>
<div id="Ch:Reg1" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Simple Linear Regression Analysis</h1>
<div id="example-galtons-height-data" class="section level2 unnumbered">
<h2>Example (Galton’s height data)</h2>
<p>As a first example, we consider the original dataset from Francis Galton, who invented <em>regression</em> by looking at this dataset. We obtained the data from <a href="https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1/LKC7PJ&amp;version=1.1" class="uri">https://dataverse.harvard.edu/file.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1/LKC7PJ&amp;version=1.1</a>
Here is the reference to the orginal paper of Francis Galton from 1886.</p>
<p>Galton, F. (1886). Regression Towards Mediocrity in Hereditary Stature. Journal of the Anthropological Institute, 15, 246-263</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>Galton<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;Data/Galton.tab&quot;</span>,<span class="at">sep=</span><span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Galton)</span></code></pre></div>
<pre><code>##   family father mother gender height kids male female
## 1      1   78.5   67.0      M   73.2    4    1      0
## 2      1   78.5   67.0      F   69.2    4    0      1
## 3      1   78.5   67.0      F   69.0    4    0      1
## 4      1   78.5   67.0      F   69.0    4    0      1
## 5      2   75.5   66.5      M   73.5    4    1      0
## 6      2   75.5   66.5      M   72.5    4    1      0</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(Galton)</span></code></pre></div>
<pre><code>## Rows: 898
## Columns: 8
## $ family &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;3&quot;, &quot;3&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;~
## $ father &lt;dbl&gt; 78.5, 78.5, 78.5, 78.5, 75.5, 75.5, 75.5, 75.5, 75.0, 75.0, 75.~
## $ mother &lt;dbl&gt; 67.0, 67.0, 67.0, 67.0, 66.5, 66.5, 66.5, 66.5, 64.0, 64.0, 64.~
## $ gender &lt;chr&gt; &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;~
## $ height &lt;dbl&gt; 73.2, 69.2, 69.0, 69.0, 73.5, 72.5, 65.5, 65.5, 71.0, 68.0, 70.~
## $ kids   &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, ~
## $ male   &lt;dbl&gt; 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, ~
## $ female &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, ~</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="fu">skim</span>(Galton)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-7">Table 2.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Galton</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">898</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">6</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">family</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">0</td>
<td align="right">197</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">gender</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="12%" />
<col width="8%" />
<col width="12%" />
<col width="5%" />
<col width="4%" />
<col width="2%" />
<col width="3%" />
<col width="4%" />
<col width="4%" />
<col width="4%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">father</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">69.23</td>
<td align="right">2.47</td>
<td align="right">62</td>
<td align="right">68</td>
<td align="right">69.0</td>
<td align="right">71.0</td>
<td align="right">78.5</td>
<td align="left">&lt;U+2581&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="even">
<td align="left">mother</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">64.08</td>
<td align="right">2.31</td>
<td align="right">58</td>
<td align="right">63</td>
<td align="right">64.0</td>
<td align="right">65.5</td>
<td align="right">70.5</td>
<td align="left">&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="odd">
<td align="left">height</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">66.76</td>
<td align="right">3.58</td>
<td align="right">56</td>
<td align="right">64</td>
<td align="right">66.5</td>
<td align="right">69.7</td>
<td align="right">79.0</td>
<td align="left">&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="even">
<td align="left">kids</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">6.14</td>
<td align="right">2.69</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">6.0</td>
<td align="right">8.0</td>
<td align="right">15.0</td>
<td align="left">&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2582&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="odd">
<td align="left">male</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.52</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="left">&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;</td>
</tr>
<tr class="even">
<td align="left">female</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.48</td>
<td align="right">0.50</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.0</td>
<td align="right">1.0</td>
<td align="right">1.0</td>
<td align="left">&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;</td>
</tr>
</tbody>
</table>
<p>The dataset contains data on heights of parents and their adult children; there can be more than one child per family. For our purpose we will select fathers and one son (we will select the first son that appears in the family).</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>Galton.sons<span class="ot">&lt;-</span>Galton <span class="sc">%&gt;%</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(gender<span class="sc">==</span><span class="st">&quot;M&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(family) <span class="sc">%&gt;%</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice</span>(<span class="dv">1</span>) <span class="sc">%&gt;%</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>  ungroup <span class="sc">%&gt;%</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">father.cm=</span>father<span class="sc">*</span><span class="fl">2.54</span>,</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>         <span class="at">son.cm=</span>height<span class="sc">*</span><span class="fl">2.54</span>) <span class="sc">%&gt;%</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(father.cm, son.cm)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(Galton.sons)</span></code></pre></div>
<pre><code>## Rows: 173
## Columns: 2
## $ father.cm &lt;dbl&gt; 199.390, 175.260, 175.260, 175.260, 175.260, 176.530, 175.26~
## $ son.cm    &lt;dbl&gt; 185.928, 180.848, 190.500, 177.800, 185.420, 179.070, 180.34~</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Galton.sons,</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>father.cm, <span class="at">y=</span>son.cm)) <span class="sc">+</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;length of father (cm)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>))</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This scatter plot suggests a possitive correlation between the heights of the father and the son.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(Galton.sons<span class="sc">$</span>father.cm, Galton.sons<span class="sc">$</span>son.cm)</span></code></pre></div>
<pre><code>## [1] 0.5022938</code></pre>
<p>In this course, however, we are not only interested in the correlation, but we want to quantify how the expected (or average) height of sons varies with the age of the father.</p>
<p>In the context of regression analysis we will use the following terminology:</p>
<ul>
<li><p>height of son is the <strong>outcome</strong> or <strong>response</strong> (or <strong>response variable</strong>) or <strong>dependent variable</strong>. This will be denoted by <span class="math inline">\(Y\)</span>.</p></li>
<li><p>height of father is the <strong>regressor</strong>, <strong>covariate</strong> or <strong>independent variable</strong>. This will be denoted by <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p>One way of looking at the problem, is to consider the outcomes that correspond to fathers of a given height, as a population. In this way, for example, the heights of sons of fathers of height <span class="math inline">\(172.72\)</span>cm can be considered as a random sample of outcomes from the population of heights of sons of fathers of height <span class="math inline">\(172.72\)</span>cm. We can apply this reasoning for all heights of fathers included in the dataset, and in a more abstract way we can think of samples of outcomes for each value of the regressor.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>Galton.sons <span class="sc">%&gt;%</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(father.cm<span class="sc">==</span><span class="fl">172.72</span>) <span class="sc">%&gt;%</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>father.cm,<span class="at">y=</span>son.cm)) <span class="sc">+</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_jitter</span>(<span class="at">position =</span> <span class="fu">position_jitter</span>(<span class="fl">0.2</span>)) <span class="sc">+</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>),</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.title.x=</span><span class="fu">element_blank</span>(),<span class="at">axis.text.x=</span><span class="fu">element_blank</span>())</span></code></pre></div>
<pre><code>## Warning in (function (kind = NULL, normal.kind = NULL, sample.kind = NULL) :
## non-uniform &#39;Rounding&#39; sampler used</code></pre>
<p><img src="DASM2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We will use the following notation:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: total number of observations (subjects, elements) in the dataset.</p></li>
<li><p><span class="math inline">\(x_i\)</span>: the value of the regressor of observation <span class="math inline">\(i=1,\ldots, n\)</span></p></li>
<li><p><span class="math inline">\(Y_i\)</span>: the outcome of observation <span class="math inline">\(i=1,\ldots, n\)</span>.</p></li>
</ul>
<p>Since every <span class="math inline">\(x_i\)</span> can identify another population, we say that <span class="math inline">\(Y_i\)</span> is a random outcome from the population with (cumulative) distribution function (CDF)
<span class="math display">\[
  F_i(y)=F(y;x_i,\mb\beta),
\]</span>
i.e. the distribution is determined by <span class="math inline">\(x_i\)</span> and possibly by a parameter vector <span class="math inline">\(\mb\beta\)</span>. We also assume that all <span class="math inline">\(n\)</span> outcomes are mutually independent. Note that we do not say that the <span class="math inline">\(n\)</span> outcomes are i.i.d. (<em>identically and independently distributed</em>), because not all <span class="math inline">\(F_i\)</span> coincide.</p>
<p>Regression analysis is a method that allows to study the effect of a regressor on the <strong>mean outcome</strong>. We therefore also introduce a notation for the mean of the distribution <span class="math inline">\(F_i\)</span>,
<span class="math display">\[
  \mu_i = \E{Y_i} = \Ef{F_i}{Y_i} = \int_{-\infty}^{+\infty} y dF(y;x_i,\mb\beta) = \E{Y_i \mid x_i}.
\]</span>
This notation stresses that <span class="math inline">\(\mu_i\)</span> is the mean of <span class="math inline">\(Y_i\)</span> and that this mean depends on the value of the regressor <span class="math inline">\(x_i\)</span> because the distribution of <span class="math inline">\(Y_i\)</span> depends on <span class="math inline">\(x_i\)</span>. Therefore, <span class="math inline">\(\mu_i=\E{Y_i \mid x_i}\)</span> is the <strong>conditional mean</strong> of the outcome, given <span class="math inline">\(x_i\)</span>. To stress that the conditional mean is a function of the regressor and possibly of a parameter <span class="math inline">\(\mb\beta\)</span> we write
<span class="math display">\[
  m(x;\mb\beta) = \E{Y \mid x}.
\]</span></p>
<p>In the two previous paragraphs we actually gave a generic description of a <strong>statistical model</strong>. We repeat here the description, with in slightly more general fashion.</p>
<ul>
<li><p>For a given <span class="math inline">\(x_i\)</span>, we provide the conditional distribution of the outcome,
<span class="math display">\[
 Y_i \mid x_i \sim F_i(\cdot;x_i,\mb\beta,\mb\nu)
\]</span>
in which <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span> are two parameter vectors.</p></li>
<li><p>The conditional mean of this distribution is described as
<span class="math display">\[
 \E{Y_i\mid x_i}=m(x_i;\mb\beta).
\]</span></p></li>
<li><p>The <span class="math inline">\(n\)</span> outcomes <span class="math inline">\(Y_i\)</span> are mutually independent.</p></li>
</ul>
<p>Consider the follow special cases:</p>
<ul>
<li><p>Suppose there are no parameters <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span> and the functions <span class="math inline">\(m\)</span> and <span class="math inline">\(F_i\)</span> are not known. Then the model imposes no restriction on the conditional distribution <span class="math inline">\(Y_i \mid x_i\)</span>. This is a <strong>nonparametric model</strong>.</p></li>
<li><p>Suppose that the function <span class="math inline">\(m\)</span> is known, up to the parameter vector <span class="math inline">\(\mb\beta\)</span>, but the CDFs <span class="math inline">\(F_i\)</span> have no further restrictions (it does of course satisfy the restriction <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>). Then the statistical model only imposes restrictions on the conditional mean, but leaves other aspects of the conditional distribution unspecified. This is a <strong>semiparametric model</strong>.</p></li>
<li><p>Suppose that the function <span class="math inline">\(m\)</span> is known, up to the parameter vector <span class="math inline">\(\mb\beta\)</span>, and that the CDFs <span class="math inline">\(F_i\)</span> are also known, up to the parameter vector <span class="math inline">\(\mb\nu\)</span>. In this case, the <span class="math inline">\(n\)</span> CDFs <span class="math inline">\(F_i\)</span> are often equal to one another, i.e. <span class="math inline">\(F_i=F\)</span>. Of course, as for the semiparametric model, the CDF <span class="math inline">\(F\)</span> must be compatible with the restriction <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>. This model is known as a <strong>parametric model</strong>. It basically specifies the full conditional distribution up to finite dimensional parameter vectors <span class="math inline">\(\mb\beta\)</span> and <span class="math inline">\(\mb\nu\)</span>.</p></li>
</ul>
<p>With respect to the parametric models: if the interest is in the model <span class="math inline">\(\E{Y_i\mid x_i}=m(x_i;\mb\beta)\)</span>, i.e. the focus is on parameter <span class="math inline">\(\mb\beta\)</span>, then we call the parameter <span class="math inline">\(\mb\nu\)</span> a <strong>nuisance parameter</strong>.</p>
<p>In later sections we will sometimes refer to semiparametric and parametric models, and then their meaning may become clear.</p>
<p>The general regression model is illustrated in Figure <a href="#fig:RegModel">2.1</a> (left). The focus of a regression analysis is the estimation of the function <span class="math inline">\(m(x;\mb\beta)\)</span>. If the function <span class="math inline">\(m\)</span> is known, then this reduces to the estimation of the parameter <span class="math inline">\(\mb\beta\)</span> using the sample observations. Based on the estimates, regression analysis also aims to formulate conclusions on the relation between the regressor and the conditional mean of the outcome. Sometimes the estimated regression model may also be used for predicting an outcome for a given value of the regressor. These topics will all be discussed later in this course.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegModel"></span>
<img src="DASM2_files/figure-html/RegModel-1.png" alt="Illustration of the regression model. The black line represents the function $m$. The red points are observed outcomes, sampled from the conditional distribution of $Y$ given $x=160$. The red line shows the shape of the density function of this conditional distribution. The blue points are observed outcomes, sampled from the conditional distribution of $Y$ given $x=200$. The blue line shows the shape of the density function of this conditional distribution. Left: $m$ is a non-linear function and the conditional distributions have different shapes. Middle: $m$ is a non-linear function and the conditional distribution functions have equal shapes. Right: $m$ is a linear function and the conditional distribution functions have equal shapes (normal distributions)." width="672" />
<p class="caption">
Figure 2.1: Illustration of the regression model. The black line represents the function <span class="math inline">\(m\)</span>. The red points are observed outcomes, sampled from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=160\)</span>. The red line shows the shape of the density function of this conditional distribution. The blue points are observed outcomes, sampled from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=200\)</span>. The blue line shows the shape of the density function of this conditional distribution. Left: <span class="math inline">\(m\)</span> is a non-linear function and the conditional distributions have different shapes. Middle: <span class="math inline">\(m\)</span> is a non-linear function and the conditional distribution functions have equal shapes. Right: <span class="math inline">\(m\)</span> is a linear function and the conditional distribution functions have equal shapes (normal distributions).
</p>
</div>
<p>We now simplify the model by assuming that the conditional distribution only depends on the regressor via its conditional mean. In other words: for every value of the regressor, the shape of the conditional outcome distribution is the same, and only the location (mean) may depend on the regressor. This is illustrated in Figure <a href="#fig:RegModel">2.1</a> (middle).
This extra assumption allows us to write the statistical model as
<span class="math display" id="eq:Mod1">\[\begin{equation}
  Y_i = m(x_i;\mb\beta) + \eps_i
  \tag{2.1}
\end{equation}\]</span>
with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span>, with <span class="math inline">\(F_\eps(\cdot;\mb\nu)\)</span> the CDF of <span class="math inline">\(\eps_i\)</span>, <span class="math inline">\(\mb\nu\)</span> a parameter vector and
<span class="math display">\[ 
  \E{\eps_i}=\E{\eps_i\mid x_i}=0.
\]</span>
For the latter assumption, we find a useful property:
<span class="math display">\[
  \E{Y\mid x}=m(x;\mb\beta)
\]</span>
i.e. it allows to interpret <span class="math inline">\(m(x;\mb\beta)\)</span> as the conditional mean.
The stochastic variable <span class="math inline">\(\eps_i\)</span> is often referred to as the <strong>error term</strong>. The model thus suggests that the outcome can be decomposed into two components: a <strong>systematic component</strong>, <span class="math inline">\(m(x_i;\mb\beta)\)</span>, and a <strong>stochastic component</strong> (or random component, or error term) <span class="math inline">\(\eps_i\)</span>. The latter gives the deviation between the outcome <span class="math inline">\(Y_i\)</span> and the systematic component <span class="math inline">\(m(x_i;\mb\beta)\)</span>.</p>
<p>It is important, however, to note that this decomposition often does not agree with how the outcomes are generated or realised: the variability of the outcomes <span class="math inline">\(Y_i\)</span> about the conditional mean <span class="math inline">\(m(x_i;\mb\beta)\)</span> is often inherently present in the population (e.g. biological variability). In this sense, <span class="math inline">\(\eps_i\)</span> may not be looked at as an <em>error</em> on the measurement. In other cases, however, part of the variability in <span class="math inline">\(Y_i\)</span> may be due to imprecise measurements and then <span class="math inline">\(\eps_i\)</span> can (at least partly) be considered as a random error term.</p>
<p>Note that the assumption <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb{\nu})\)</span> implies that all error terms have the same distribution and hence, for a fixed <span class="math inline">\(\sigma^2\geq 0\)</span>,
<span class="math display">\[
  \var{\eps_i} = \var{\eps_i \mid x_i} = \sigma^2
\]</span>
for all <span class="math inline">\(i=1,2,\ldots, n\)</span>. This restriction on the variance is referred to as the assumption of <strong>homoskedasticiteit</strong> or <strong>constant variance</strong>. The variance <span class="math inline">\(\sigma^2\)</span> is called the <strong>residual variance</strong>.</p>
<p>In this course we only discuss <strong>linear regression analysis</strong>, for which the function <span class="math inline">\(m(x;\mb\beta)\)</span> is restricted to linear functions of <span class="math inline">\(\mb\beta\)</span>:
<span class="math display">\[
  m(x;\mb\beta) = \beta_0 + \beta_1x,
\]</span>
with <span class="math inline">\(\mb{\beta}^t=(\beta_0, \beta_1)\)</span>.</p>
<p>This equation represents a linear line which is referred to as the <strong>regression line</strong>. We write model <a href="#eq:Mod1">(2.1)</a> now as
<span class="math display" id="eq:Mod3">\[\begin{equation}
  Y_i = \beta_0 + \beta_1x_i + \eps_i
  \tag{2.2}
\end{equation}\]</span>
with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. This is called the <strong>simple linear regression model</strong>. See also Figure <a href="#fig:RegModel">2.1</a> (right).</p>
<p>The interpretation of the parameter <span class="math inline">\(\beta_1\)</span> follows from the identity
<span class="math display">\[
  \E{Y\mid x+1} - \E{Y\mid x} = \left(\beta_0+\beta_1(x+1)\right)-\left(\beta_0+\beta_1 x\right) = \beta_1 .
\]</span>
The parameter <span class="math inline">\(\beta_1\)</span> is thus the average increase in the outcome when the regressor increases with one unit. This parameter is also the <strong>slope</strong> of the regression line. The parameter is often referred to as the <strong>regression coefficient</strong>.</p>
<p>The interpretation of the parameter <span class="math inline">\(\beta_0\)</span> follows from the identity
<span class="math display">\[
  \E{Y\mid x=0} = \beta_0+\beta_1 \times 0 = \beta_0 .
\]</span>
The parameter <span class="math inline">\(\beta_0\)</span> is thus the average outcome when the regressor takes value zero. It is the <strong>intercept</strong> of the regression line.</p>
<p>Sometimes the situation <span class="math inline">\(x=0\)</span> does not have a physical meaning or it falls outside of the <strong>scope</strong> of the model (i.e. the range of <span class="math inline">\(x\)</span>-values that forms the focus of the data analysis). For this reason, the regression model is sometimes formulated as
<span class="math display">\[
  Y_i = \beta_0 + \beta_1(x_i-\bar{x}) + \eps_i
\]</span>
with <span class="math inline">\(\bar{x}\)</span> the sample mean of the regressor observations in the dataset, <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>.
The interpretation of <span class="math inline">\(\beta_0\)</span> now becomes <span class="math inline">\(\beta_0=\E{Y\mid x=\bar{x}}\)</span>. Since the sample mean <span class="math inline">\(\bar{x}\)</span> is often withing the scope of the model, the parameter <span class="math inline">\(\beta_0\)</span> now has a real meaning. The interpretation of <span class="math inline">\(\beta_1\)</span> remains unchanged.</p>
<p>In the Galton example, the scope is approximately <span class="math inline">\([150 \text{cm},200 \text{cm}]\)</span>.</p>
<p>In this chapter we discuss methods for the estimation of the parameters in the linear regression model <a href="#eq:Mod3">(2.2)</a>. We will find the sampling distribution of the parameter estimators. This will form the basis for hypothesis tests and for confidence intervals.</p>
</div>
<div id="S:RegSimStudy" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Interpretation via simulations</h2>
<p>Regression model <a href="#eq:Mod3">(2.2)</a> thus gives an interpretation to the <span class="math inline">\(\beta\)</span> parameters via the conditional expectation of the outcomes. In this section we provide an interpretation via the principle of <strong>repeated sampling</strong>, which can be easily demonstrated with Monte Carlo simulations. Since this is the first simulation in this course, we shall go through it step by step.</p>
<p>We start with model <a href="#eq:Mod3">(2.2)</a>:
<span class="math display">\[
  Y_i = \beta_0 + \beta_1x_i + \eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>. We set <span class="math inline">\(F_\eps\)</span> to the normal distribution with mean zero and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We start with the simulation of a sample of 5 outcomes (son’s heights) at <span class="math inline">\(\mb{x}^t=(165, 170, 175, 180, 185)\)</span> (father’s heights).</p>
<p>Before we can start the simulations, we need to set the parameters to specific values. We choose:
<span class="math display">\[
  \beta_0=90 \;\;\; \beta_1=0.5 \;\;\; \sigma=5.
\]</span>
Thus, we simulate as if we known the truth (i.e. the population). In this context, we refer to this model as the <strong>data generating model</strong>.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">724245</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>x</span></code></pre></div>
<pre><code>## [1] 165 170 175 180 185</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>eps<span class="ot">&lt;-</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 error terms</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>eps</span></code></pre></div>
<pre><code>## [1]  2.6552611 -4.3398292  2.1952809  6.1667742 -0.9250292</code></pre>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span>eps <span class="co"># random sample van uitkomsten</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>y   </span></code></pre></div>
<pre><code>## [1] 175.1553 170.6602 179.6953 186.1668 181.5750</code></pre>
<p>The following R code gives Figure <a href="#fig:RegSim1">2.2</a>.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>),<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegSim1"></span>
<img src="DASM2_files/figure-html/RegSim1-1.png" alt="Scatter plot of 1 simulated sample from the (data generating) regression model. The red line is the linear regression model with the true parameter values." width="672" />
<p class="caption">
Figure 2.2: Scatter plot of 1 simulated sample from the (data generating) regression model. The red line is the linear regression model with the true parameter values.
</p>
</div>
<p>Next we repeat this procedure (experiment) multiple times. Each time other outcomes will be generated. The following R code generates <span class="math inline">\(N=100\)</span> repeated experiments, each with <span class="math inline">\(n=5\)</span> outcomes as described earlier. The resulst are visualised in Figure <a href="#fig:RegSim2">2.3</a>.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">254111</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N100 <span class="co"># number of repeated experiments</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">160</span>,<span class="dv">195</span>))</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>),<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>Data<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">experiment=</span><span class="dv">1</span>,<span class="at">x=</span>x,<span class="at">y=</span>y)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(experiment <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>N) {</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span></span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(x,y,<span class="at">col=</span>experiment)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>    Data<span class="ot">&lt;-</span><span class="fu">rbind</span>(Data,<span class="fu">cbind</span>(experiment,x,y))</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegSim2"></span>
<img src="DASM2_files/figure-html/RegSim2-1.png" alt="Scatter plot of $N=100$ simulated samples (experiments) from a regression model. Each color corresponds to a repeated experiment. The red line is the linear regression line with the true parameter values." width="672" />
<p class="caption">
Figure 2.3: Scatter plot of <span class="math inline">\(N=100\)</span> simulated samples (experiments) from a regression model. Each color corresponds to a repeated experiment. The red line is the linear regression line with the true parameter values.
</p>
</div>
<p>Now we look at the histograms of the repeated samples; each histogram corresponds to another value of <span class="math inline">\(x\)</span>. See Figure <a href="#fig:RegSim3">2.4</a>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">3</span>))</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>  y<span class="ot">&lt;-</span>Data<span class="sc">$</span>y[Data<span class="sc">$</span>x<span class="sc">==</span>x[i]]</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">hist</span>(y,<span class="at">main=</span><span class="fu">paste</span>(<span class="st">&quot;x=&quot;</span>,x[i]),<span class="at">xlab=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v=</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x[i],<span class="at">col=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(y),<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegSim3"></span>
<img src="DASM2_files/figure-html/RegSim3-1.png" alt="Histogram of the $N=100$ repeated experiments. Each histogram corresponds to another value of $x$. The red vertical lines show the average outcomes according to the true regression model and the blue dashed lines are the averages of the repeated outcomes at the corresponding temperatures." width="672" />
<p class="caption">
Figure 2.4: Histogram of the <span class="math inline">\(N=100\)</span> repeated experiments. Each histogram corresponds to another value of <span class="math inline">\(x\)</span>. The red vertical lines show the average outcomes according to the true regression model and the blue dashed lines are the averages of the repeated outcomes at the corresponding temperatures.
</p>
</div>
<p>Every histogram in Figure <a href="#fig:RegSim3">2.4</a> approximately shows a normal distribution. This is expected, because we have sampled from a normal distribution for each value of <span class="math inline">\(x\)</span>. In particular, for a given value of <span class="math inline">\(x\)</span>, we have sampled the outcomes <span class="math inline">\(Y\)</span> from the distribution <span class="math inline">\(N(90+0.5x,25)\)</span>.</p>
<p>Figure <a href="#fig:RegSim3">2.4</a> also shows the sample means of the N=100 repeated outcomes for each given <span class="math inline">\(x\)</span>. If <span class="math inline">\(N\)</span> is very large, then these sample means are approximately equal to the corresponding expected values <span class="math inline">\(\E{Y\mid x}=\beta_0+\beta_1x\)</span>. Also these expected values (with the given parameter values) are depicted in the graph; these are the points on the true regression line. Both lines are very close to one another. If <span class="math inline">\(N\)</span> were much larger then 100, we would expect that the lines would be even closer to one another. The next R code gives the numerical values.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>Results<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">x=</span><span class="cn">NA</span>,<span class="at">EmpMean=</span><span class="cn">NA</span>,<span class="at">ExpectedValue=</span><span class="cn">NA</span>,</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">EmpVariance=</span><span class="cn">NA</span>,<span class="at">Variance=</span><span class="cn">NA</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>  y<span class="ot">&lt;-</span>Data<span class="sc">$</span>y[Data<span class="sc">$</span>x<span class="sc">==</span>x[i]]</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>  Results[i,]<span class="ot">&lt;-</span><span class="fu">c</span>(x[i],<span class="fu">round</span>(<span class="fu">mean</span>(y),<span class="dv">2</span>),<span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x[i],</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>                    <span class="fu">round</span>(<span class="fu">var</span>(y),<span class="dv">2</span>),<span class="dv">25</span>)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>Results</span></code></pre></div>
<pre><code>##     x EmpMean ExpectedValue EmpVariance Variance
## 1 165  172.63         172.5       22.46       25
## 2 170  174.69         175.0       27.60       25
## 3 175  177.92         177.5       23.17       25
## 4 180  181.08         180.0       24.82       25
## 5 185  182.94         182.5       25.68       25</code></pre>
<p>This R code also shows the sample variances of the simulated outcomes for the 5 values of <span class="math inline">\(x\)</span>. The true variance according the data generating model equals 25. Also here we see a good agreement.</p>
</div>
<div id="S:LSE1" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Least squares estimators</h2>
<p>Consider again model <a href="#eq:Mod3">(2.2)</a>,
<span class="math display">\[
  Y_i = \beta_0 + \beta_1x_i + \eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid F_\eps(\cdot;\mb\nu)\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i\mid x_i}=0\)</span>.</p>
<p>The <strong>least squares</strong> estimation method can be applied without knowledge of the exact shape of the distribution of the error term. In this sense, the model is an example of <strong>semiparametric statistical model</strong>: the conditional mean is parameterised (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>) and posesses a restriction on the conditional distributions, but the other moments of the conditional outcome distribution (or error term distribution) remain unspecified.</p>
<p>In this section we aim at estimating the regression parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which will be denoted by <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. The estimated regression line should come <em>as close as possible</em> to the observed outcomes in the sample. We will need a measure for the distance between the estimated regression line and the observed sample data. For given estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>, the estimated regression line is given by
<span class="math display">\[
  \hat{m}(x)=m(x;\hat{\mb\beta})= \hat\beta_0+\hat\beta_1 x.
\]</span>
This line is sometimes referred to as the <strong>fitted</strong> regression line.
For given <span class="math inline">\(x\)</span>, this gives an estimate of the conditional mean of the outcome <span class="math inline">\(Y\)</span>. For the <span class="math inline">\(n\)</span> sample observations, the points on the estimated regression line are denoted by
<span class="math display">\[
  \hat{Y}_i = \hat{m}(x_i)=m(x_i;\hat{\mb\beta})=\hat\beta_0+\hat\beta_1 x_i \;\;\; i=1,\ldots, n.
\]</span>
The <span class="math inline">\(\hat{Y}_i\)</span>s are often called the <strong>predictions</strong> of the outcomes, but in many cases this is a misleading terminology because the points on the estimated regression line should in the first place not be considered as predictions, but rather as estimates of the conditional mean. On the other hand, if no extra information is available, then <span class="math inline">\(\hat{m}(x)\)</span> is a good prediction for an outcome at regression value <span class="math inline">\(x\)</span> (see further). In this course we will stick to the conventional terminology (predictions), but in the first place we consider them as estimates of the conditional means.</p>
<p>Good parameter estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> should make the predictions <span class="math inline">\(\hat{Y}_i\)</span> come as close as possible to the observed outcomes <span class="math inline">\(Y_i\)</span>. This can be quantified by the <strong>least squares criterion</strong></p>
<p><span class="math display" id="eq:SSEReg1">\[\begin{equation}
  \SSE(\hat{\mb\beta}) = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2 = \sum_{i=1}^n \left(Y_i - m(x_i;\hat{\mb\beta})\right)^2.
  \tag{2.3}
\end{equation}\]</span>
SSE is the abbreviation of <strong>sum of squares of the error</strong>, which is also referred to as the <strong>residual sum of squares</strong> or the <strong>sum of squared errors</strong>. This brings us to the following definition.</p>

<div class="definition">
<span id="def:SSE1" class="definition"><strong>Definition 2.1  (Least squares parameter estimator)  </strong></span>The least squares parameter estimator of <span class="math inline">\(\mb\beta\)</span> is given by
<span class="math display">\[
  \hat{\mb\beta} = \text{ArgMin}_{\mb\beta} \SSE(\mb\beta).
\]</span>
</div>
<p>We use the abbreviation LSE for the <strong>least squares estimator</strong>.</p>
<p>Figure <a href="#fig:RegLSE">2.5</a> shows three datasets with fitted regression lines and with the indication of <span class="math inline">\(Y_i - \hat{Y}_i\)</span>. The deviation <span class="math inline">\(Y_i - \hat{Y}_i\)</span> is called the <strong>residual</strong>, which is often written as
<span class="math display">\[
  e_i = Y_i - \hat{Y}_i \;\text{ or }\; e_i(\hat{\mb{\beta}}) =Y_i -m(x_i;\hat{\mb{\beta}}).
\]</span>
With this notation we write
<span class="math display">\[
  \SSE=\sum_{i=1}^n e_i^2 \;\text{ or }\; \SSE(\hat{\mb{\beta}})=\sum_{i=1}^n e_i^2(\hat{\mb{\beta}}).
\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegLSE"></span>
<img src="DASM2_files/figure-html/RegLSE-1.png" alt="Illustration of the estimation of the regession line. The black line shows the function $m$; this function is in practice not known and need to be estimated based on five sample observations (red points). The estimated regression line is depicted as the red dashed line. The vertical lines connect the observed outcomes $y_i$ with the predictions. The lengths of these vertical lines form the basis for SSE. Each of the three graphs start with another (randomly selected) sample. Left: estimates of $\beta_0$ and $\beta_1$ are $51.9$ and $0.71$ with SSE$=52.75$. Middle: $133.3$ and $0.26$ with SSE$=35.94$. Right: $122.3$ and $0.31$ with SSE$=3.18$." width="672" />
<p class="caption">
Figure 2.5: Illustration of the estimation of the regession line. The black line shows the function <span class="math inline">\(m\)</span>; this function is in practice not known and need to be estimated based on five sample observations (red points). The estimated regression line is depicted as the red dashed line. The vertical lines connect the observed outcomes <span class="math inline">\(y_i\)</span> with the predictions. The lengths of these vertical lines form the basis for SSE. Each of the three graphs start with another (randomly selected) sample. Left: estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are <span class="math inline">\(51.9\)</span> and <span class="math inline">\(0.71\)</span> with SSE<span class="math inline">\(=52.75\)</span>. Middle: <span class="math inline">\(133.3\)</span> and <span class="math inline">\(0.26\)</span> with SSE<span class="math inline">\(=35.94\)</span>. Right: <span class="math inline">\(122.3\)</span> and <span class="math inline">\(0.31\)</span> with SSE<span class="math inline">\(=3.18\)</span>.
</p>
</div>
<p>Before we continue, we introduce the <strong>matrix notation</strong> for the model. We introduce the following notation:</p>
<ul>
<li><p>parameter vector <span class="math inline">\(\mb\beta^t=(\beta_0, \beta_1)\)</span> and estimate <span class="math inline">\(\hat{\mb\beta}^t=(\hat\beta_0, \hat\beta_1)\)</span></p></li>
<li><p>outcome vector <span class="math inline">\(\mb{Y}^t=(Y_1,\ldots, Y_n)\)</span></p></li>
<li><p><strong>design matrix</strong> (<span class="math inline">\(n\times 2\)</span> matrix)
<span class="math display">\[
 \mb{X}=\begin{pmatrix}
1 &amp; x_1 \\
1 &amp; x_2 \\
\vdots &amp; \vdots \\
1 &amp; x_n
 \end{pmatrix}.
 \]</span>
The <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{X}\)</span> is represented by <span class="math inline">\(\mb{x}_i^t=(1,x_i)\)</span>.</p></li>
<li><p>error vector <span class="math inline">\(\mb{\eps}^t=(\eps_1,\ldots, \eps_n)\)</span>.</p></li>
</ul>
<p>With the vector and matrix notation we rewrite model <a href="#eq:Mod3">(2.2)</a> as
<span class="math display">\[
  Y_i = \mb{x}_i^t\mb\beta+\eps_i
\]</span>
or as
<span class="math display">\[
  \mb{Y} = \mb{X}\mb\beta + \mb{\eps}
\]</span>
with <span class="math inline">\(\eps_i \iid\)</span> and <span class="math inline">\(\E{\eps_i}=\E{\eps_i \mid x_i}=0\)</span> and <span class="math inline">\(\var{\eps_i}=\sigma^2\)</span>.</p>
<p>With this notation we write SSE from <a href="#eq:SSEReg1">(2.3)</a> as
<span class="math display">\[
  \SSE(\hat{\mb\beta})=\Vert \mb{Y} - \mb{X}\hat{\mb\beta}\Vert^2
\]</span>
and the LSE from Definition <a href="#def:SSE1">2.1</a> becomes
<span class="math display">\[
  \hat{\mb\beta} = \text{ArgMin}_{\mb\beta} \SSE(\mb\beta) = \text{ArgMin}_{\mb\beta} \Vert \mb{Y} - \mb{X}\mb\beta\Vert^2.
\]</span></p>
<p>The solution of the minimisation problem that results in the LSE is given next and formulated as a theorem.</p>

<div class="theorem">
<span id="thm:LSEReg1" class="theorem"><strong>Theorem 2.1  (LSE for simple linear regression)  </strong></span> Assume that model <a href="#eq:Mod3">(2.2)</a> is correct is and that the <span class="math inline">\(n\times 2\)</span> design matrix <span class="math inline">\(\mb{X}\)</span> has rank <span class="math inline">\(2\)</span> heeft. Then, the LSE of <span class="math inline">\(\mb\beta\)</span> is given by
<span class="math display">\[
  \hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}  
\]</span>
and this solution is unique.
</div>
<p><strong>Proof</strong></p>
<p>First we show that <span class="math inline">\(\hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span>.</p>
<p>We find the LSE by means of vector differentiation (see Appendix <a href="#app:VecDiff">A</a>).</p>
<p>First we write
<span class="math display">\[\begin{eqnarray*}
\|\mb{Y} - \mb{X}\mb\beta \|^2 
   &amp;=&amp; (\mb{Y} - \mb{X} \mb\beta)^t (\mb{Y}-\mb{X}\mb\beta)\\
   &amp;=&amp; \mb{Y}^t\mb{Y} - \mb\beta^t \mb{X}^t \mb{Y} - \mb{Y}^t \mb{X} \mb\beta + \mb\beta^t \mb{X}^t \mb{X} \mb\beta .
\end{eqnarray*}\]</span>
Applying vector differentiation, we find
<span class="math display">\[
  \frac{d}{d\mb\beta} \|\mb{Y} - \mb{X} \mb\beta\|^2 = -2\mb{X}^t\mb{Y} + 2\mb{X}^t\mb{X} \mb\beta .
\]</span></p>
<p>The LSE of <span class="math inline">\(\mb\beta\)</span> satisfies</p>
<p><span class="math display">\[\begin{eqnarray*}
  \frac{d}{d\mb\beta} \|\mb{Y}- \mb{X} \mb\beta\|^2 
   &amp;=&amp; 0\\
   &amp;\Updownarrow&amp; \\
  \mb{X}^t \mb{X} \mb\beta 
   &amp;=&amp; \mb{X}^t \mb{Y}
\end{eqnarray*}\]</span>
The solution (<span class="math inline">\(\mb{X}\)</span> has full rank and hence <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is invertible) is thus given by
<span class="math display">\[
   \hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1} \mb{X}^t \mb{Y}.
\]</span>
For demonstrating that this estimate minimises the least squares criterion, we must show that the matrix of partial derivatives of second order is positive definite.
<span class="math display">\[
    \frac{d^2}{d\mb\beta d\mb\beta^t}\|\mb{Y}- \mb{X} \mb\beta\|^2 = \frac{d}{d\mb\beta} (-2\mb{X}^t\mb{Y} + 2\mb{X}^t\mb{X} \mb\beta) = 2\mb{X}^t\mb{X}.
  \]</span>
The <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is positive definite because <span class="math inline">\(\mb{X}\)</span> is of full rank.</p>
<p>Finally we have to prove that we have a unique solution.</p>
<p>Suppose that there are two different solutions (say <span class="math inline">\(\hat{\mb\beta}_1\)</span> and <span class="math inline">\(\hat{\mb\beta}_2\)</span> such that <span class="math inline">\(\hat{\mb\beta}_1 \neq \hat{\mb\beta}_2\)</span>); then it holds that</p>
<p><span class="math display">\[\begin{eqnarray*}
  \mb{X}^t\mb{Y} 
     &amp;=&amp; \mb{X}^t \mb{X} \hat{\mb\beta}_1\\
     &amp;=&amp; \mb{X}^t \mb{X} \hat{\mb\beta}_2.
\end{eqnarray*}\]</span></p>
<p>Hence, <span class="math inline">\(\mb{X}^t\mb{X}(\hat{\mb\beta}_1 - \hat{\mb\beta}_2)=0\)</span>. Because <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is of full rank, the unique solution of the system of equation given by <span class="math inline">\(\mb{X}^t\mb{X}\mb{v} = 0\)</span> is provided by the null solution <span class="math inline">\(\mb{v} = \mb{0}\)</span>. Therefore the following equality must hold true: <span class="math inline">\(\hat{\mb\beta}_1 -\hat{\mb\beta}_2 = \mb{0}\)</span>. Hence, the supposition <span class="math inline">\(\hat{\mb\beta}_1 \neq \hat{\mb\beta}_2\)</span> gives a contradiction and hence <span class="math inline">\(\hat{\mb\beta}_1=\hat{\mb\beta}_2\)</span> must be true, i.e. there is only one unique solution. </p>
<hr />
<p>If we work out the matrix formulation for <span class="math inline">\(\hat{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span> we find</p>
<p><span class="math display">\[\begin{eqnarray*}
  \hat\beta_0 
    &amp;=&amp; \bar{Y} - \hat\beta_1 \bar{x}\\
  \hat\beta_1
    &amp;=&amp; \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}.
 \end{eqnarray*}\]</span></p>
<p>This solution could also be found directly by computing the partial derivatives of SSE w.r.t. the two parameters, setting these derivatives to zero and solving the system of equations for the two parameters (later we will see that our solution via vector differentiation is more general and also applies to multiple linear regression).</p>
<p><span class="math display">\[\begin{eqnarray*}
  \frac{\partial}{\partial \beta_0} \SSE(\mb\beta)
    &amp;=&amp; 0\\
  \frac{\partial}{\partial \beta_1} \SSE(\mb\beta)
    &amp;=&amp; 0.
 \end{eqnarray*}\]</span></p>
<p>This gives</p>
<p><span class="math display" id="eq:LSEEE1" id="eq:LSEEE0">\[\begin{eqnarray}
  \frac{\partial}{\partial \beta_0} \SSE(\mb\beta)
    &amp;=&amp; -2\sum_{i=1}^n (Y_i - \beta_0-\beta_1 x_i) =0 
    \tag{2.4} \\
  \frac{\partial}{\partial \beta_1} \SSE(\mb\beta)
    &amp;=&amp; -2\sum_{i=1}^n x_i(Y_i - \beta_0-\beta_1 x_i)=0.
    \tag{2.5}
 \end{eqnarray}\]</span></p>
<p>Equations <a href="#eq:LSEEE0">(2.4)</a> and <a href="#eq:LSEEE1">(2.5)</a> are called the <strong>estimating equations</strong>. In the context of LSE for linear regression models they are also known as the <strong>normal equations</strong>.</p>
</div>
<div id="example-galtons-height-data-1" class="section level2 unnumbered">
<h2>Example (Galton’s height data)</h2>
<p>We estimate the parameters of the regression line for Galton’s data.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm,<span class="at">data=</span>Galton.sons)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>m</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Coefficients:
## (Intercept)    father.cm  
##     89.8182       0.5077</code></pre>
<p>The estimated (or fitted) regression line is thus given by
<span class="math display">\[
  \hat{m}(x) = 89.8 + 0.51 x.
\]</span></p>
<p>Figure <a href="#fig:GaltonFittedReg">2.6</a> shows the scatter plot and the fitted regression line. Sometimes the fitted regression line is represented as
<span class="math display">\[
  \hat{y}_i = 89.8 + 0.51 x_i .
\]</span>
Interpretation: if the height of the fathers increases with 1cm, then the average height of their sons is estimated to increase with <span class="math inline">\(0.5\)</span>cm.</p>
<p>The intercept, however, has no direct physical interpretation because there are no fathers of height 0cm. This issue can be resolved by first centering the regressor. This is illustrated next.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>Galton.sons<span class="ot">&lt;-</span>Galton.sons <span class="sc">%&gt;%</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">father.cm.centered=</span>father.cm<span class="sc">-</span><span class="fu">mean</span>(father.cm))</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Galton.sons<span class="sc">$</span>father.cm)</span></code></pre></div>
<pre><code>## [1] 175.4905</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Galton.sons<span class="sc">$</span>father.cm.centered)</span></code></pre></div>
<pre><code>## [1] 8.121634e-15</code></pre>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>m2<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm.centered,<span class="at">data=</span>Galton.sons)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>m2</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm.centered, data = Galton.sons)
## 
## Coefficients:
##        (Intercept)  father.cm.centered  
##           178.9070              0.5077</code></pre>
<p>First note that the estimate of the slope remains unchanged after centering the regressor. The intercept is now estimanted by <span class="math inline">\(\hat\beta_0=178.9\)</span>. Hence, when the centered regressor equals 0, i.e. when fathers have average height (175.5cm), their sons have an estimated average height of <span class="math inline">\(178.9\)</span>cm.</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(Galton.sons,</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>father.cm, <span class="at">y=</span>son.cm)) <span class="sc">+</span></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;length of father (cm)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;length of son (cm)&quot;</span>) <span class="sc">+</span></span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span>m<span class="sc">$</span>coefficients[<span class="dv">1</span>],<span class="at">slope=</span>m<span class="sc">$</span>coefficients[<span class="dv">2</span>]) <span class="sc">+</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span><span class="dv">0</span>,<span class="at">slope=</span><span class="dv">1</span>,<span class="at">color=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GaltonFittedReg"></span>
<img src="DASM2_files/figure-html/GaltonFittedReg-1.png" alt="Scatter plot of the Galton data and the fitted regression line (black). The red line is the diagonal line." width="672" />
<p class="caption">
Figure 2.6: Scatter plot of the Galton data and the fitted regression line (black). The red line is the diagonal line.
</p>
</div>
<p><strong>An historical note</strong>: Figure <a href="#fig:GaltonFittedReg">2.6</a> shows the fitted regression line, but also the diagonal line. If the fitted regression line would coincide with the diagonal, then this would indicate that the average height of sons equals the height of their fathers. However, this is obviously not the case for Galton’s dataset. From the graph we see that the smaller fathers have sons that are on average taller than them. On the other hand, the taller fathers have sons that are on average smaller than them. In 1886 Galton also observed this phenomenon, which he called <em>regression towards mediocrity</em>. In his 1886 paper he develop the basis of modern regression analysis (without the statistical inference); the term <em>regression</em> comes from his paper on the analysis of heights of parents and children. Nowadays, <em>regression towards mediocrity</em> is known as <em>regression to the mean</em>.</p>
<p>Finally, note that the estimates computed for this example, do not give any appreciation of the (im)precision with which they were estimated. To what extent can we thrust these estimates? To answer that question, we need the sampling distribution of the estimators. This will be discussed later in this course.</p>
</div>
<div id="exercise-blood-pressure" class="section level2 unnumbered">
<h2>Exercise: blood pressure</h2>
<p>In a small dose-finding study of a blood pressure reducing drug, 40 high blood pressure patients (systolic blood pressure at least 150 mmHg) were randomised over 4 concentrations of the active compound (<em>arginine</em>) in the drug: 0, 2, 5 and 10 mg per day. The outcome is the systolic blood pressure reduction after 2 months, measured in mmHg. The data can be read as shown in the next chunck of R code.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/BloodPressure.RData&quot;</span>)</span></code></pre></div>
<p>Fit a linear regression to the data and interpret the regression coefficient.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<p>First we explore the dataset.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(BloodPressure,</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>dose, <span class="at">y=</span>bp.reduction)) <span class="sc">+</span></span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;dose (mg / day&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;systolic blood pressure reduction (mmHg)&quot;</span>) <span class="sc">+</span></span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>))</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Next we fit the linear regression model.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>m.bloodpressure<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose, <span class="at">data=</span>BloodPressure)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>m.bloodpressure</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ dose, data = BloodPressure)
## 
## Coefficients:
## (Intercept)         dose  
##     0.03304      1.78634</code></pre>
<p>From the output we read <span class="math inline">\(\hat\beta_1=\)</span> 1.7863436. Hence, we conclude that we estimate that on average the systolic blood pressure reducses with 1.8mmHg over a period of two months, with an increase of the daily dose of 1mg.</p>
<p>The next graph shows the estimated regression line.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(BloodPressure,</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>dose, <span class="at">y=</span>bp.reduction)) <span class="sc">+</span></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;dose (mg / day&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;systolic blood pressure reduction (mmHg)&quot;</span>) <span class="sc">+</span></span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span>m.bloodpressure<span class="sc">$</span>coefficients[<span class="dv">1</span>],</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a>              <span class="at">slope=</span>m.bloodpressure<span class="sc">$</span>coefficients[<span class="dv">2</span>]) </span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</details>
</div>
<div id="S:PropLSE" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Properties of the Least Squares Estimator</h2>
<div id="mean-and-variance-of-the-lse" class="section level3" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Mean and variance of the LSE</h3>
<p>The next theorem gives two important properties of the LSE.
</p>

<div class="theorem">
<p><span id="thm:LSEMeanVar" class="theorem"><strong>Theorem 2.2  (Mean and variance of the LSE)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> is correct and that rank(<span class="math inline">\(\mb{X}\)</span>)=<span class="math inline">\(2\)</span> (<span class="math inline">\(2\leq n\)</span>). Then the following holds</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\E{\hat{\mb\beta}}=\mb\beta\)</span> (the LSE is an <strong>unbiased</strong> estimator of <span class="math inline">\(\mb\beta\)</span>)</p></li>
<li><p><span class="math inline">\(\var{\hat{\mb\beta}}= (\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>.</p>
</div></li>
</ol>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Part 1.</p>
<p>The unbiasedness of <span class="math inline">\(\hat{\mb\beta}\)</span> follows from
<span class="math display">\[
   \E{\hat{\mb\beta}} = \E{(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\E{\mb{Y}}
   =(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{X}\mb\beta=\mb\beta.
 \]</span></p>
<p>Part 2.</p>
<p>For the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span> we will need <span class="math inline">\(\var{\mb{Y}}\)</span>. On the diagonal of this matrix we find <span class="math inline">\(\var{Y_i}=\var{\eps_i}=\sigma^2\)</span> and on the off-diagonal positions we need the covariances <span class="math inline">\(\cov{Y_i,Y_j}\)</span> (<span class="math inline">\(i\neq j\)</span>). All these covariances are equal to zero because the independence between outcomes is assumed.
Hence, the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span> becomes</p>
<span class="math display">\[\begin{eqnarray*}
 \var{\hat{\mb\beta}}
  &amp;=&amp; \var{(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}} \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t \var{\mb{Y}} \left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t\right]^t \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t \sigma^2 \mb{I}_n \mb{X}(\mb{X}^t\mb{X})^{-1} \\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{X} (\mb{X}^t\mb{X})^{-1} \sigma^2\\
  &amp;=&amp; (\mb{X}^t\mb{X})^{-1} \sigma^2.
\end{eqnarray*}\]</span>
</div>
<p>We also give the explicit form of <span class="math inline">\(\var{\hat{\mb\beta}}= (\mb{X}^t\mb{X})^{-1} \sigma^2\)</span> (after working out the matrix multiplication and inversion):
<span class="math display" id="eq:SigmaBetaLSE">\[\begin{equation}
   \var{\hat{\mb\beta}} = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix}.
   \tag{2.6}
\end{equation}\]</span></p>
<p>To give you a good understanding of these two properties, we extend the simulation study of Section <a href="#S:RegSimStudy">2.1</a>. For every repeated experiment, we compute the LSE of the two <span class="math inline">\(\beta\)</span>-parameters. We repeat the experiment <span class="math inline">\(N=\)</span> 100 times and then we compute the mean and the variance of the <span class="math inline">\(N=\)</span> 100 parameter estimates. Recall that we set the (true) parameter values to <span class="math inline">\(\beta_0=90\)</span>, <span class="math inline">\(\beta_1=0.5\)</span> and <span class="math inline">\(\sigma=5\)</span>.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">75286</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N100 <span class="co"># number of repeated experiments</span></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>betaHat<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>    betaHat[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb64-9"><a href="#cb64-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb64-10"><a href="#cb64-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-11"><a href="#cb64-11" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(betaHat)</span></code></pre></div>
<pre><code>##          beta0Hat    beta1Hat
## beta0Hat 2209.750 -12.6339951
## beta1Hat  -12.634   0.0723973</code></pre>
<p>The averages of the <span class="math inline">\(N=\)</span> 100 estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are indeed close to the true values. If <span class="math inline">\(N\)</span> were larger, we expect the estimates to be even closer to the true values.</p>
<p>Theorem <a href="#thm:LSEMeanVar">2.2</a> tells us that the variance of <span class="math inline">\(\hat{\mb\beta}\)</span> equals <span class="math inline">\((\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>. The design matrix <span class="math inline">\(\mb{X}\)</span> for our simulation experiment is constructed in the following chunck of R code.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span><span class="dv">5</span>,<span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">1</span>]<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">2</span>]<span class="ot">=</span>x</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>X</span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    1  165
## [2,]    1  170
## [3,]    1  175
## [4,]    1  180
## [5,]    1  185</code></pre>
<p>With this matrix and with <span class="math inline">\(\sigma^2=25\)</span> we find the covariance matrix of <span class="math inline">\(\hat{\mb\beta}\)</span>. See the next chunck of R code.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span><span class="dv">25</span></span></code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 3067.5 -17.5
## [2,]  -17.5   0.1</code></pre>
<p>The results of our simulation study give only a rough approximation to this true covariance matrix. A better approximation is obtained with a larger number of repeated experiments (<span class="math inline">\(N\)</span>). We illustrate this with <span class="math inline">\(N=\)</span> 10^{4}.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6247467</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 <span class="co"># number of repeated experiments</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>betaHat5<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span></span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    betaHat5[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 88.8751318  0.5051252</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>EmpVar5<span class="ot">&lt;-</span><span class="fu">var</span>(betaHat5)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>Var5<span class="ot">&lt;-</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span><span class="dv">25</span></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>EmpVar5</span></code></pre></div>
<pre><code>##            beta0Hat    beta1Hat
## beta0Hat 3080.48493 -17.5710114
## beta1Hat  -17.57101   0.1003883</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>Var5</span></code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 3067.5 -17.5
## [2,]  -17.5   0.1</code></pre>
<p>To get a better understanding of the concept of <span class="math inline">\(\var{\hat{\mb\beta}}\)</span>, we repeat the simulation study but with more observations for each repeated experiment. Before this was <span class="math inline">\(n=5\)</span>. We increase this to <span class="math inline">\(n=50\)</span>. We keep the 5 father’s heights, but for each height we have now 10 observations (as if we have 10 fathers of the same height, and each of these fathers has a son).</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6247467</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 <span class="co"># number of repeated experiments</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">rep</span>(x,<span class="dv">10</span>) <span class="co"># the five father&#39;s heights are replicated 10 times</span></span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>betaHat50<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">50</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span></span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>    betaHat50[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat50)</span></code></pre></div>
<pre><code>##  beta0Hat  beta1Hat 
## 90.178681  0.498924</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span><span class="dv">50</span>,<span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">1</span>]<span class="ot">=</span><span class="dv">1</span></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">2</span>]<span class="ot">=</span>x</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>EmpVar50<span class="ot">&lt;-</span><span class="fu">var</span>(betaHat50)</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>Var50<span class="ot">&lt;-</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span><span class="dv">25</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>EmpVar50</span></code></pre></div>
<pre><code>##            beta0Hat    beta1Hat
## beta0Hat 311.718416 -1.77841831
## beta1Hat  -1.778418  0.01016245</code></pre>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>Var50</span></code></pre></div>
<pre><code>##        [,1]  [,2]
## [1,] 306.75 -1.75
## [2,]  -1.75  0.01</code></pre>
<p>Again we see that the means and variances of the simulated estimates agree with the true means and variances as we found from theory. The results also show that the variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are smaller for <span class="math inline">\(n=50\)</span> than for <span class="math inline">\(n=5\)</span>. The former is a factor 10 smaller than the latter. This factor agrees with the factor with which we increased the sample size (from <span class="math inline">\(n=5\)</span> to <span class="math inline">\(n=50\)</span>). (Try to prove this yourself.)</p>
<p>The difference between the <span class="math inline">\(n=5\)</span> and the <span class="math inline">\(n=50\)</span> scenario is demonstrated in Figure <a href="#fig:SimReg3">2.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SimReg3"></span>
<img src="DASM2_files/figure-html/SimReg3-1.png" alt="histograms of $N=10000$ repeated estimates of  $\beta_1$ for $n=5$ and $n=50$. These form approximations for the sampling distributions of the estimators." width="672" />
<p class="caption">
Figure 2.7: histograms of <span class="math inline">\(N=10000\)</span> repeated estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=50\)</span>. These form approximations for the sampling distributions of the estimators.
</p>
</div>
<p>The figure shows the histograms of the <span class="math inline">\(N=\)</span> 10^{4} estimates of <span class="math inline">\(\beta_1\)</span> for sample sizes of <span class="math inline">\(n=5\)</span> and <span class="math inline">\(n=50\)</span>. This illustrates once more that the estimator <span class="math inline">\(\hat{\mb\beta}_1\)</span> is a stochastic (or random) variable. This is easy to understand: <span class="math inline">\(\hat\beta_1= \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}\)</span> is a (linear) function of <span class="math inline">\(Y_1,\ldots, Y_1\)</span>, which are randomly sampled outcomes. Hence, <span class="math inline">\(\hat\beta_1\)</span> is also a random variable and it can this be described by a distribution. This distribution is referred to as the <strong>sampling distribution</strong>. It will be discussed in some more detail later in this course. In this section we only looked at the mean and the variance of the estimator.</p>
<p>Figure <a href="#fig:SimReg3">2.7</a> demonstrates that the variance of the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> descreases with increasing sample size <span class="math inline">\(n\)</span>. The variance, which is defined as
<span class="math display">\[
  \var{\hat\beta_1} = \E{(\hat\beta_1-\E{\hat\beta_1})^2},
\]</span>
quantifies how the estimates, over the repeated experiments, vary about the true parameter value <span class="math inline">\(\beta_1\)</span> (note that <span class="math inline">\(\beta_1=\E{\hat\beta_1}\)</span>; unbiased estimator). Thus, the smaller the variance, the more frequent (over repeated experiments) the estimate <span class="math inline">\(\hat\beta_1\)</span> is close to the true value <span class="math inline">\(\beta_1\)</span>. In other words (cfr. definition of variance): for a large sample size <span class="math inline">\(n\)</span> we expect the estimates <span class="math inline">\(\hat\beta_1\)</span> an average closer to the true value <span class="math inline">\(\beta_1\)</span> than for a small sample size <span class="math inline">\(n\)</span>.</p>
<p>If the goal of the study is to estimate the parameter with a great <strong>precision</strong>, then the variance of the estimator must be small. We therefore say that the variance of an estimator is to be considered as a measure of the <strong>imprecision</strong> of the estimator.</p>
</div>
</div>
<div id="exercise-simulation-study" class="section level2 unnumbered">
<h2>Exercise: simulation study</h2>
<p>Repeat the previous simulation study, but now with other values of the regressor and other numbers of replicates for each regressor value. In particular, consider the setting with only two different values of the regressor (<span class="math inline">\(x=165\)</span> and <span class="math inline">\(x=185\)</span>) and at each of these values, consider 25 replicates. This makes a total of <span class="math inline">\(n=50\)</span> observations, just like in the simulation study.</p>
<p>Check whether the LSEs of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unbiased and compute the empirical variance of the estimators from the simulation study. How do these variances compare to the variances from the previous simulation study? Can you give an explanation?</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6247467</span>)</span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 <span class="co"># number of repeated experiments</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">185</span>) <span class="co"># two regressor values</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">rep</span>(x,<span class="dv">25</span>) <span class="co"># the two regressor values are replicated 25 times</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>betaHat50<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">50</span>,<span class="at">sd=</span><span class="dv">5</span>) </span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb84-9"><a href="#cb84-9" aria-hidden="true" tabindex="-1"></a>    betaHat50[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb84-10"><a href="#cb84-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb84-11"><a href="#cb84-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-12"><a href="#cb84-12" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat50)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.8835261  0.5006106</code></pre>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(betaHat50)</span></code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat 152.1525837 -0.865844097
## beta1Hat  -0.8658441  0.004943341</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(betaHat50<span class="sc">$</span>beta1Hat,<span class="at">main=</span><span class="st">&quot;n=50&quot;</span>,<span class="at">xlab=</span><span class="st">&quot;beta1-hat&quot;</span>,<span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fl">0.5</span>,<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p>The simulation study demonstrates once more that the LSE of the estimators are unbiased. When looking at the empirical variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> we see that they are now equal to 152.1525837 and 0.0049433, respecitvely. These are smaller than the variances we obtained from the previous simulation study in which we considered 5 equally spaced values of the regressor. The minimum and maximum values of the regressor are still 165 and 185 and the total sample size is also still equal to <span class="math inline">\(n=50\)</span>.
The differences in the variances can be explained by expression <a href="#eq:SigmaBetaLSE">(2.6)</a> for <span class="math inline">\(\var{\hat{\mb\beta}}\)</span>.</p>
<p>Let us first look at <span class="math inline">\(\var{\hat\beta_1}\)</span>, which is given by
<span class="math display">\[
  \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2}.
\]</span>
Recall that <span class="math inline">\(n=50\)</span> and <span class="math inline">\(\sigma^2=25\)</span> are the same here as in the previous simulation study. Also <span class="math inline">\(\bar{x}=175\)</span> is the same as before (less relevant). It is easy to confirm that the denominator <span class="math inline">\(\sum_{i=1}^n (x_i-\bar{x})^2\)</span> is larger here (5000) than for the previous simulation study (2500). For <span class="math inline">\(\var{\hat\beta_0}\)</span> you may also check that the variance for the setting in this exercise is smaller than for the previous setting.</p>
<p>More generally it can be shown that for a given <span class="math inline">\(n\)</span>, a given <span class="math inline">\(\sigma^2\)</span> and a given interval for the regressor, i.e. <span class="math inline">\(x_i \in [x_\text{min}, x_\text{max}]\)</span>, the variances of <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are the smallest when half of the regressors are set to <span class="math inline">\(x_\text{min}\)</span> and the other half is set to <span class="math inline">\(x_\text{max}\)</span>.</p>
</details>
<div id="best-linear-unbiased-estimator-blue" class="section level3" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Best Linear Unbiased Estimator (BLUE)</h3>
<p>In the two previous sections we have investigated the variance of the LSE. We have stressed that the variance is a measure for the imprecission of an estimator. So we like to have an estimator that has a small imprecission or variance.
In this section we shall demonstrate that within a certain class of estimators, and for regression model <a href="#eq:Mod3">(2.2)</a>, the LSE has the smallest variance and hence from this perspective it is the best estimator.</p>
<p>Let’s describe the class of estimators of <span class="math inline">\(\mb\beta\)</span> that can be written as <span class="math inline">\(\mb{AY}\)</span>, with <span class="math inline">\(\mb{A}\)</span> a <span class="math inline">\(p\times n\)</span> matrix that may depend on <span class="math inline">\(\mb{X}\)</span>, but that may not depend on stochastic variables (such as e.g. <span class="math inline">\(\mb{Y}\)</span>). We write <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span>. The class of <strong>linear unbiased estimators</strong> is then given by estimators of the form <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span>, for which it holds that <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span> (i.e. the estimator is unbiased for <span class="math inline">\(\mb\beta\)</span>).</p>
<p>The LSE <span class="math inline">\(\hat{\mb{\beta}}\)</span> is an example of a linear unbiased estimator of <span class="math inline">\(\mb\beta\)</span>. It has the form <span class="math inline">\(\mb{AY}\)</span> with <span class="math inline">\(\mb{A}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\)</span>.</p>
<p>As an example, consider the LSE of <span class="math inline">\(\beta_1\)</span>, which can be written as
<span class="math display">\[
  \hat\beta_1= \frac{\sum_{i=1}^n (Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2}.
\]</span>
Also here you see that the estimator is a linear combination of the <span class="math inline">\(n\)</span> outcomes <span class="math inline">\(Y_i\)</span>.
We consider now an alternative linear estimator:
<span class="math display">\[
  \hat\beta_1^*= \frac{\sum_{i=1}^n w_i(Y_i-\bar{Y})(x_i-\bar{x})}{\sum_{i=1}^n (x_i-\bar{x})^2},
\]</span>
where <span class="math inline">\(w_1,\ldots, w_n\)</span> are non-negative constants for which holds th at <span class="math inline">\(\sum_{i=1}^n w_i=1\)</span> (i.e. <span class="math inline">\(w_i\)</span> are weigths). For <span class="math inline">\(\hat\beta_1^*\)</span> it still holds that it is an unbiased estimator. The variance, however, is different.</p>
<p>Within the class of linear unbiased estimators, the estimator with the smallest variancs is the <strong>best linear unbiased estimator</strong> (BLUE). The next theorem is still more general.
</p>

<div class="theorem">
<span id="thm:BLUE" class="theorem"><strong>Theorem 2.3  (The LSE is the BLUE)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> is correct, and consider the LSE <span class="math inline">\(\hat{\mb\beta}\)</span>. Then it holds for all <span class="math inline">\(\hat{\mb\beta}^*=\mb{AY}\)</span> for which <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span>, that
<span class="math display">\[
   \var{\mb{c}^t\hat{\mb\beta}} \leq \var{\mb{c}^t\hat{\mb\beta}^*} \;\;\;\text{ voor alle } \mb{c}\in\mathbb{R}^p.
\]</span>
</div>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> The estimator <span class="math inline">\(\hat{\mb\beta}^*\)</span> can always be written as
<span class="math display">\[
   \hat{\mb\beta}^* = \left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t+\mb{D}\right]\mb{Y}
\]</span>
with <span class="math inline">\(\mb{D}=\mb{A}-(\mb{X}^t\mb{X})^{-1}\mb{X}^t\)</span>. Thus,
<span class="math display">\[
  \mb{c}^t\hat{\mb\beta}^* = \mb{c}^t\left[(\mb{X}^t\mb{X})^{-1}\mb{X}^t+\mb{D}\right]\mb{Y} = \mb{c}^t \mb{\hat\beta} + \mb{c}^t\mb{DY}.
\]</span></p>
<p>Next we find an expression for the variance of <span class="math inline">\(\mb{c}^t\hat{\mb\beta}^*\)</span>:
<span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} = \var{\mb{c}^t\hat{\mb\beta}} + \var{\mb{c}^t\mb{DY}} + 2\cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}}.
\]</span>
The covariance in this expression becomes
<span class="math display">\[
  \cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}} = \mb{c}^t(\mb{X}^t\mb{X})^{-1}\mb{X}^t\var{\mb{Y}}\mb{D}^t\mb{c} = \mb{c}^t(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{D}^t\mb{c}\sigma^2.
\]</span></p>
<p>Since <span class="math inline">\(\hat{\mb\beta}^*\)</span> is an unbiased estimator, i.e. <span class="math inline">\(\E{\hat{\mb\beta}^*}=\mb\beta\)</span>, we find for all <span class="math inline">\(\mb\beta\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
  \E{\mb{DY}}
  &amp;=&amp; \mb{0} \\
  \E{\mb{D}(\mb{X\beta}+\mb{\eps})}
  &amp;=&amp; \mb{0} \\
  \mb{DX\beta}+\mb{D}\E{\mb{\eps}}
  &amp;=&amp; \mb{0} \\
  \mb{DX\beta}
  &amp;=&amp;\mb{0}.
\end{eqnarray*}\]</span></p>
<p>Since in general <span class="math inline">\(\mb\beta\)</span> is not equal to zero, the unbiasedness of <span class="math inline">\(\hat{\mb{\beta}}^*\)</span> implies that <span class="math inline">\(\mb{DX}=\mb{0}\)</span>. With this identity, we find that the covariance <span class="math inline">\(\cov{\mb{c}^t\hat{\mb\beta},\mb{c}^t\mb{DY}}\)</span> is identical to zero.</p>
<p>The variance of <span class="math inline">\(\mb{c}^t\hat{\mb\beta}^*\)</span> thus reduces to</p>
<p><span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}^*} = \var{\mb{c}^t\hat{\mb\beta}} + \var{\mb{c}^t\mb{DY}}.
\]</span></p>
Finaly, since variances cannot be negative, we find, for all <span class="math inline">\(\mb{c}\in\mathbb{R}^p\)</span>,
<span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}} \leq \var{\mb{c}^t\hat{\mb\beta}^*} .
\]</span>
</div>
</div>
</div>
<div id="exercise-simulation-study-1" class="section level2 unnumbered">
<h2>Exercise: Simulation study</h2>
<p>We refer here to the simulation study in <a href="#S:PropLSE">2.3</a>. Repeat this simulation study with 10000 Monte Carlo simulation runs, but now with another estimator of <span class="math inline">\(\mb\beta\)</span>. With our conventional notation, define the following esimator,
<span class="math display">\[
  \tilde{\mb\beta} = (\mb{X}^t\mb{X}+d\mb{I}_2)^{-1}\mb{X}^t\mb{Y},
\]</span>
with <span class="math inline">\(d&gt;0\)</span> and <span class="math inline">\(\mb{I}_2\)</span> the <span class="math inline">\(2\times 2\)</span> identity matrix. In the simulation study you may set <span class="math inline">\(d=0.005\)</span>. Compare the LSE of <span class="math inline">\(\mb\beta\)</span> with this new estimator in terms of bias and variance. What do you conclude? How does this relate to the BLUE property of the LSE? \
Also compute the <em>Mean Squared Error</em> (MSE) of the estimators, defined here as
<span class="math display">\[
  \E{(\hat\beta_j-\beta_j)^2} \;\;\text{ and }\;\; \E{(\tilde\beta_j-\beta_j)^2},
\]</span>
for <span class="math inline">\(j=0,1\)</span>. Recall that the expectation can be approximated by the average over many Monte Carlo simulation runs. \
What do conclude from these Mean Squared Errors?</p>
<details>
<summary>
Try to solve this problem and then you can expend this page to look at a solution.
</summary>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6247467</span>)</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 <span class="co"># number of repeated experiments</span></span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="dv">1</span>,x) <span class="co"># design matrix</span></span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>betaHatLSE<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a>betaHatNew<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a>MSELSE<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow =</span> N,<span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a>MSENew<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow =</span> N,<span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a>  y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes</span></span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a>  m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb89-12"><a href="#cb89-12" aria-hidden="true" tabindex="-1"></a>  betaHatLSE[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb89-13"><a href="#cb89-13" aria-hidden="true" tabindex="-1"></a>  MSELSE[i,]<span class="ot">&lt;-</span>(<span class="fu">coef</span>(m)<span class="sc">-</span><span class="fu">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb89-14"><a href="#cb89-14" aria-hidden="true" tabindex="-1"></a>  betaTilde<span class="ot">&lt;-</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X<span class="fl">+0.005</span><span class="sc">*</span><span class="fu">diag</span>(<span class="dv">2</span>))<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>y</span>
<span id="cb89-15"><a href="#cb89-15" aria-hidden="true" tabindex="-1"></a>  betaHatNew[i,]<span class="ot">&lt;-</span>betaTilde</span>
<span id="cb89-16"><a href="#cb89-16" aria-hidden="true" tabindex="-1"></a>  MSENew[i,]<span class="ot">&lt;-</span>(betaTilde<span class="sc">-</span><span class="fu">c</span>(<span class="dv">90</span>,<span class="fl">0.5</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb89-17"><a href="#cb89-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb89-18"><a href="#cb89-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-19"><a href="#cb89-19" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHatLSE)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 90.5853872  0.4966336</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHatNew)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 56.1436710  0.6931226</code></pre>
<p>For these averages of the estimates over the Monte Carlo runs, we conclude that the LSEs are unbiased (as we already knew), but the new estimator is biased!</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(<span class="fu">var</span>(betaHatLSE))</span></code></pre></div>
<pre><code>##     beta0Hat     beta1Hat 
## 3080.4849279    0.1003883</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="fu">diag</span>(<span class="fu">var</span>(betaHatNew))</span></code></pre></div>
<pre><code>##     beta0Hat     beta1Hat 
## 1.183233e+03 3.865236e-02</code></pre>
<p>The new, but biased estimator, seems to give smaller variances than the LSE. \
Didn’t we learn that the LSE has the smallest bias (BLUE property)? No, the BLUE property says that the LSE has the smallest variance among all <strong>unbiased</strong> estimators. Thus our new estimator, which is biased, does not belong to the class of unbiased estimators and hence it can theoretically have a smaller variance.</p>
<p>Let’s now look at the Mean Squared errors.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(MSELSE)</span></code></pre></div>
<pre><code>## [1] 3080.5195576    0.1003896</code></pre>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(MSENew)</span></code></pre></div>
<pre><code>## [1] 2.329366e+03 7.594484e-02</code></pre>
<p>The MSEs of the LSEs are larger than the MSEs of the new estimators. This means that on average, over many repeated experiments, the new estimates are closer to the true value of the parameter! This is also a very desirable property!</p>
The new estimator that was introduces here, is known as the <strong>ridge estimator</strong>. It will turn out to be a useful estimator in high dimensional prediction problems.
</details>
<div id="sampling-distribution-of-the-lse" class="section level3" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Sampling distribution of the LSE</h3>
<p>Since <span class="math inline">\(\hat{\mb\beta}\)</span> is a function of the outcome vector <span class="math inline">\(\mb{Y}\)</span> and since the outcome vector is a random variable, the estimtor <span class="math inline">\(\hat{\mb\beta}\)</span> is also a random variable. Its distribution (<strong>sampling distribution</strong>) is determined by the distribution of <span class="math inline">\(\mb{Y}\)</span>. In model <a href="#eq:Mod3">(2.2)</a> we see that <span class="math inline">\(\mb{Y} = \mb{X}\mb\beta + \mb{\eps}\)</span>, but the distribution of <span class="math inline">\(\mb{\eps}\)</span> is not fully specified (only the mean is restricted to zero). This prohibits finding the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span>, unless asymptotically (see further).</p>
<p>We shall introduce an extra distribution assumption in the statistical model, and this will allow for finding the sampling distribution.
Model <a href="#eq:Mod3">(2.2)</a> is extended to (in matrix notation)
<span class="math display" id="eq:Mod4">\[\begin{equation}
  \mb{Y} = \mb{X}\mb\beta + \mb{\eps}
  \tag{2.7}
\end{equation}\]</span>
with <span class="math inline">\(\mb{X}\)</span> an <span class="math inline">\(n\times p\)</span> (<span class="math inline">\(p\leq n\)</span>) matrix of rank <span class="math inline">\(p\)</span>, <span class="math inline">\(\mb{\eps}^t=(\eps_1,\ldots, \eps_n)\)</span> and <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. In this model, the error terms are assumed to be normally distributed. The model will be referred to as the <strong>normal linear regression model</strong>. For this model, the next theorem gives the sampling distribution of the LSE.
</p>

<div class="theorem">
<span id="thm:DistrMod4" class="theorem"><strong>Theorem 2.4  (Sampling distribution of the LSE in the normal linear regression model)  </strong></span>Assume that model <a href="#eq:Mod4">(2.7)</a> holds. This it holds that
<span class="math display">\[
   \hat{\mb\beta} \sim \text{MVN}(\mb\beta,(\mb{X}^t\mb{X})^{-1}\sigma^2). 
\]</span>
</div>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Write <span class="math inline">\(\hat{\mb\beta} = (\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}\)</span> as <span class="math inline">\(\hat{\mb\beta} =\mb{CY}\)</span>. Matrix algebra tells us that
<span class="math display">\[
   \text{rang}(\mb{C})=\text{rang}(\mb{X}^t)=\text{rang}(\mb{X})=p. 
\]</span>
The estimator <span class="math inline">\(\hat{\mb\beta}=\mb{CY}\)</span> is thus a vector of linear combinations of the elements in <span class="math inline">\(\mb{Y}\)</span>, which are jointly multivariate normally distributed (see Lemma <a href="#lem:LinTransNorm">B.1</a> in Appendix <a href="#app:LinTrans">B</a>. The mean and the variance of <span class="math inline">\(\hat{\mb\beta}\)</span> were already given in Theorem <a href="#thm:LSEMeanVar">2.2</a>.</p>
Hence,
<span class="math display">\[
   \hat{\mb\beta} \sim \text{MVN}(\mb\beta, (\mb{X}^t\mb{X})^{-1}\sigma^2).
\]</span>
</div>
<p>We now repeat the simulation study and this time we will use normal QQ-plots to check whether the sampling distribution is indeed a normal distribution. The results are shown in Figure <a href="#fig:SimReg4">2.8</a>. The QQ-plots clearly show that the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> is normal.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SimReg4"></span>
<img src="DASM2_files/figure-html/SimReg4-1.png" alt="Normal QQ-plots of the $N=10000$ repeated estimates of $\beta_1$ for $n=5$ en $n=50$" width="672" />
<p class="caption">
Figure 2.8: Normal QQ-plots of the <span class="math inline">\(N=10000\)</span> repeated estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span> en <span class="math inline">\(n=50\)</span>
</p>
</div>
<p>If the normality assumption of model <a href="#eq:Mod4">(2.7)</a> is violated, but the assumptions of model <a href="#eq:Mod3">(2.2)</a> do hold, then we can still find the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span>, but only for large sample sizes. Without proof, the result is stated in the following theorem. It is an <strong>asymptotic result</strong>, which means that it holds in the limit for sample sizes <span class="math inline">\(n\)</span> going to infinity. Fortunately, such asymptotic results often hold approximately for large, but finite sample sizes.
</p>

<div class="theorem">
<span id="thm:DistrMod3" class="theorem"><strong>Theorem 2.5  (Asymptotic sampling distribution of the LSE)  </strong></span>Assume that (1) model <a href="#eq:Mod3">(2.2)</a> holds, (2) the regressor values are fixed by design and (3) that <span class="math inline">\(\lim_{n\rightarrow \infty} \frac{1}{n}\sum_{i=1}^n\mb{x}_i\mb{x}_i^t\)</span> has rank equal to 2. Then, as <span class="math inline">\(n\rightarrow \infty\)</span>,
<span class="math display">\[
 \sqrt{n}(\hat{\mb\beta}-\mb\beta)\mb\Sigma_n^{-1} \convDistr \text{MVN}(\mb{0},\mb{I}_2)  
\]</span>
with
<span class="math display" id="eq:SigmaBetaLSEAsymp">\[\begin{equation}
   \mb\Sigma_n = \frac{\sigma^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix}.
   \tag{2.8}
\end{equation}\]</span>
</div>
<p>Some notes regarding this theorem:</p>
<ul>
<li><p>the construction <span class="math inline">\(n\rightarrow \infty T_n \convDistr T\)</span> tells us that the distribution of the stochastic variance <span class="math inline">\(T_n\)</span>, which is based on a sample size of <span class="math inline">\(n\)</span>, converges to the distribution of the random variable <span class="math inline">\(T\)</span>, when the sample size goes to infinity. In the theorem, <span class="math inline">\(T_n\)</span> is the LSE based on a sample of size <span class="math inline">\(n\)</span>, and <span class="math inline">\(T\)</span> is a MVN random variable with mean <span class="math inline">\(\mb{0}\)</span> and covariance matrix <span class="math inline">\(\mb{I}_2\)</span> (<span class="math inline">\(2\times 2\)</span> identity matrix).</p></li>
<li><p>the distribution of <span class="math inline">\(T\)</span> (or the MVN distribution in the theorem) is referred to as the <strong>asymptotic sampling distribution</strong> of <span class="math inline">\(T_n\)</span>.</p></li>
<li><p>in contrast to the <em>asymptotic</em> sampling distribution, we use the term <strong>exact sampling distribution</strong> to refer to a sampling distribution that is correct even for small sample sizes <span class="math inline">\(n\)</span>. Such exact sampling distributions often require strong distributional assumptions.</p></li>
<li><p>the second condition in the theorem says that the regressor values must be <strong>fixed by design</strong>. This means that the regressor may not be a random variable (as it is in the Galton example). Here is an example of a <strong>fixed design</strong>: Randomly sample 10 subjects of each of the following ages: 20, 40, 60 and 70 years old (note: these ages are fixed prior to the executation of the study). For each of the <span class="math inline">\(10\times 4=40\)</span> subjects, measure the blood pressure. In this example, the blood pressure is the outcome (random variable) and the age is the regressor. However, since the ages were fixed by design, this is an example of a fixed design.</p></li>
<li><p>although the theorem only gives the asymptotic sampling distribution for fixed designs, a similar theorem exists for <strong>random designs</strong>. The second condition need to be reformulated such that it makes sense for random regressors (details not given here). The sampling distribution is the same as for fixed designs.</p></li>
</ul>
<p>We now demonstrate the practical meaning of the theorem in a simulation study. We repeat the same simulations as before, but now with error terms <span class="math inline">\(\eps_i\)</span> that are not normally distributed. We choose <span class="math inline">\(\eps_i\)</span> to be distributed as an <em>exponential distribution</em>. Figure <a href="#fig:ExpDistr">2.9</a> shows the shape of an exponential distribution with variance 1 en centered such that the mean is equal to zero (this is a requirement of error terms in our linear regression models).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ExpDistr"></span>
<img src="DASM2_files/figure-html/ExpDistr-1.png" alt="Histogram of 10000 error terms from an exponential distribution (centered to make the mean equal to zero)" width="672" />
<p class="caption">
Figure 2.9: Histogram of 10000 error terms from an exponential distribution (centered to make the mean equal to zero)
</p>
</div>
<p>The next chunck or R code gives a simulation study in which we simulate <span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=50\)</span> and <span class="math inline">\(n=200\)</span> outcomes according to Model <a href="#eq:Mod3">(2.2)</a> with a centered exponential distribution for the error term. The results show that the LSE is still unbiased (this property does not require the normality assumption). Figure <a href="#fig:SimReg5">2.10</a> shows three normal QQ-plots of the estimates <span class="math inline">\(\hat\beta_1\)</span> for the three sample sizes. For <span class="math inline">\(n=5\)</span> the sampling distribution of <span class="math inline">\(\hat\beta_1\)</span> is clearly not normal, but as the sample size <span class="math inline">\(n\)</span> increases, the approxiation to a normal distribution becomes better.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">6247467</span>)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="co"># definieren een functie voor het simuleren</span></span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a>simRegressionExp<span class="ot">&lt;-</span><span class="cf">function</span>(<span class="at">N=</span><span class="dv">1000</span>,<span class="at">nRep=</span><span class="dv">1</span>) {</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># N: number of repeated samples</span></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># nRep: number of replicated for each value of the regressor</span></span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true" tabindex="-1"></a>  x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># 5 father&#39;s heights</span></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true" tabindex="-1"></a>  x<span class="ot">&lt;-</span><span class="fu">rep</span>(x,nRep) <span class="co"># the five regrossor values are replaciated nRep times</span></span>
<span id="cb101-11"><a href="#cb101-11" aria-hidden="true" tabindex="-1"></a>  x</span>
<span id="cb101-12"><a href="#cb101-12" aria-hidden="true" tabindex="-1"></a>  betaHat<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta0Hat=</span><span class="cn">NA</span>,<span class="at">beta1Hat=</span><span class="cn">NA</span>)</span>
<span id="cb101-13"><a href="#cb101-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb101-14"><a href="#cb101-14" aria-hidden="true" tabindex="-1"></a>      y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span>(<span class="fu">rexp</span>(<span class="dv">5</span><span class="sc">*</span>nRep)<span class="sc">-</span><span class="dv">1</span>) <span class="co"># random sample of outcomes</span></span>
<span id="cb101-15"><a href="#cb101-15" aria-hidden="true" tabindex="-1"></a>      m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb101-16"><a href="#cb101-16" aria-hidden="true" tabindex="-1"></a>      betaHat[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb101-17"><a href="#cb101-17" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb101-18"><a href="#cb101-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb101-19"><a href="#cb101-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(betaHat)</span>
<span id="cb101-20"><a href="#cb101-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb101-21"><a href="#cb101-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb101-22"><a href="#cb101-22" aria-hidden="true" tabindex="-1"></a><span class="co"># for n=5</span></span>
<span id="cb101-23"><a href="#cb101-23" aria-hidden="true" tabindex="-1"></a>betaHat5<span class="ot">&lt;-</span><span class="fu">simRegressionExp</span>(<span class="at">N=</span>N1000,<span class="at">nRep=</span><span class="dv">1</span>)</span>
<span id="cb101-24"><a href="#cb101-24" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat5)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.6311933  0.5020559</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span><span class="dv">5</span>,<span class="at">ncol=</span><span class="dv">2</span>)</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">1</span>]<span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">2</span>]<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>EmpVar<span class="ot">&lt;-</span><span class="fu">var</span>(betaHat5)</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>Var<span class="ot">&lt;-</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span><span class="dv">1</span> </span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>EmpVar</span></code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat 124.3967926 -0.712668233
## beta1Hat  -0.7126682  0.004089645</code></pre>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>Var</span></code></pre></div>
<pre><code>##       [,1]   [,2]
## [1,] 122.7 -0.700
## [2,]  -0.7  0.004</code></pre>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for n=50</span></span>
<span id="cb107-2"><a href="#cb107-2" aria-hidden="true" tabindex="-1"></a>betaHat50<span class="ot">&lt;-</span><span class="fu">simRegressionExp</span>(<span class="at">N=</span>N1000,<span class="at">nRep=</span><span class="dv">10</span>)</span>
<span id="cb107-3"><a href="#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat50)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 90.0568710  0.4997006</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(betaHat50)</span></code></pre></div>
<pre><code>##             beta0Hat      beta1Hat
## beta0Hat 11.71799942 -0.0668749613
## beta1Hat -0.06687496  0.0003822612</code></pre>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="co"># for n=200</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true" tabindex="-1"></a>betaHat200<span class="ot">&lt;-</span><span class="fu">simRegressionExp</span>(<span class="at">N=</span>N1000,<span class="at">nRep=</span><span class="dv">40</span>)</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(betaHat200)</span></code></pre></div>
<pre><code>##   beta0Hat   beta1Hat 
## 89.9934692  0.5000403</code></pre>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(betaHat200)</span></code></pre></div>
<pre><code>##             beta0Hat     beta1Hat
## beta0Hat  3.08244132 -0.017619947
## beta1Hat -0.01761995  0.000100889</code></pre>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(betaHat5<span class="sc">$</span>beta1Hat,<span class="at">main=</span><span class="st">&quot;n=5&quot;</span>)</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(betaHat5<span class="sc">$</span>beta1Hat)</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(betaHat50<span class="sc">$</span>beta1Hat,<span class="at">main=</span><span class="st">&quot;n=50&quot;</span>)</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(betaHat50<span class="sc">$</span>beta1Hat)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(betaHat200<span class="sc">$</span>beta1Hat,<span class="at">main=</span><span class="st">&quot;n=200&quot;</span>)</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(betaHat200<span class="sc">$</span>beta1Hat)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SimReg5"></span>
<img src="DASM2_files/figure-html/SimReg5-1.png" alt="Histograms of the $N=1000$ herhaalde estimates of $\beta_1$ for $n=5$, $n=50$ and $n=200$, with centered exponentially distributed error terms." width="672" />
<p class="caption">
Figure 2.10: Histograms of the <span class="math inline">\(N=1000\)</span> herhaalde estimates of <span class="math inline">\(\beta_1\)</span> for <span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=50\)</span> and <span class="math inline">\(n=200\)</span>, with centered exponentially distributed error terms.
</p>
</div>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
</div>
<div id="maximum-likelihood-estimator-of-hatmbbeta" class="section level3" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Maximum likelihood estimator of <span class="math inline">\(\hat{\mb\beta}\)</span></h3>
<p>The LSE does need distributional assumptions, such as normality. The method of least squares is thus a parameter estimation method that can be applied to <strong>semiparametric statistical models</strong> such as Model <a href="#eq:Mod3">(2.2)</a>.</p>
<p>For fully parametric statistical model, such as Model <a href="#eq:Mod4">(2.7)</a>, the method of <strong>maximum likelihood</strong> becomes applicable for parameter estimation. With normally distributed error terms, as in Model <a href="#eq:Mod4">(2.7)</a>, it can be shows that the LSE is equivalent to the <strong>maximum likelihood estimator</strong> (MLE). This is demonstrated in this section.</p>
<p>Model <a href="#eq:Mod4">(2.7)</a> is equivalent to
<span class="math display">\[
  Y_i \mid x_i \sim N(\beta_0+\beta_1 x_i,\sigma^2).
\]</span>
This normal distribution has density function
<span class="math display">\[
  f(y;x,\beta_0,\beta_1,\sigma^2) = (2\pi\sigma^2)^{-1/2} \exp\left[-\frac{1}{2}\frac{(y-\beta_0-\beta_1 x )^2}{\sigma^2} \right].
\]</span></p>
<p>Since it is assumed that all <span class="math inline">\(n\)</span> outcomes are mutually independent, the likelihood function becomes
<span class="math display">\[
L(\beta_0,\beta_1,\sigma^2) = \prod_{i=1}^n f(Y_i;x_i,\beta_0,\beta_1,\sigma^2) 
=(2\pi\sigma^2)^{-n/2} \exp\left[-\frac{1}{2} \sum_{i=1}^n\frac{(Y_i-\beta_0-\beta_1 x_i)^2}{\sigma^2} \right].
\]</span>
Hence, the log-likelihood function is given by
<span class="math display">\[
  l(\beta_0,\beta_1,\sigma^2) =\ln L(\beta_0,\beta_1,\sigma^2)
  = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2}\sum_{i=1}^n\frac{(Y_i-\beta_0-\beta_1 x_i)^2}{\sigma^2}.
\]</span>
In matrix notation this becomes
<span class="math display">\[
  l(\beta_0,\beta_1,\sigma^2) = -\frac{n}{2}\ln(2\pi\sigma^2)-\frac{1}{2\sigma^2} (\mb{Y}-\mb{X}\mb\beta)^t(\mb{Y}-\mb{X}\mb\beta).
\]</span>
The MLE of <span class="math inline">\(\mb\beta\)</span> is defined as
<span class="math display">\[
  \hat{\mb\beta} = \text{ArgMax}_{\mb\beta \in \mathbb{R}^2} l(\mb\beta,\sigma^2).
\]</span>
We therefore need the partial derivative of the log-likelihood w.r.t. <span class="math inline">\(\mb\beta\)</span>,
<span class="math display">\[
\frac{\partial}{\partial \mb\beta} l(\mb\beta,\sigma^2) = \frac{1}{\sigma^2}\mb{X}^t(\mb{Y}-\mb{X}\mb\beta).
\]</span>
Setting this partial derivative equal to zero, gives exactly the normal equations of the LSE (see Theorem <a href="#thm:LSEReg1">2.1</a>).</p>
<p>The MLE of the parameter <span class="math inline">\(\sigma^2\)</span> is the solution to the equation
<span class="math display">\[
  \frac{\partial}{\partial \sigma^2} l(\mb\beta,\sigma^2) = 0.
\]</span>
After some algebra, we find the MLE
<span class="math display" id="eq:MLESigma2">\[\begin{equation}
  \hat\sigma^2=\frac{1}{n}\sum_{i=1}^n (Y_i - \hat\beta_0-\hat\beta_1 x_i)^2.
  \tag{2.9}
\end{equation}\]</span>
This estimator, however, is not unbiased (without proof), but it is <strong>asymptotically unbiased</strong>, i.e. 
<span class="math display">\[
  \lim_{n\rightarrow \infty} \E{\hat\sigma^2} = \sigma^2.
\]</span></p>
<p>In the next section we will develop an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. However, we finish this section by illustrating some more concepts of maximum likelihood estimation for the linear model.</p>
<p>The <strong>score function</strong> for the <span class="math inline">\(\mb\beta\)</span> parameters is given by
<span class="math display">\[
  \mb{s}_\beta(y;x,\mb\beta,\sigma^2) = \frac{\partial}{\partial \mb\beta} \ln f(y;x,\mb\beta,\sigma^2) 
  = \frac{1}{\sigma^2}\begin{pmatrix}
  1 &amp; x
  \end{pmatrix} (y-\beta_0-\beta_1).
\]</span>
With this notation, the <strong>estimating equation</strong> for <span class="math inline">\(\mb\beta\)</span> becomes
<span class="math display">\[
   \frac{\partial}{\partial \mb\beta} l(\mb\beta,\sigma^2) = \sum_{i=1}^n \mb{s}_\beta(Y_i;x_i,\mb\beta,\sigma^2) = \mb{0}.
\]</span>
A shorter expression is obtained by introducing the <strong>score statistic</strong> for <span class="math inline">\(\mb\beta\)</span>,
<span class="math display">\[
 \mb{S}_\beta(\mb\beta,\sigma^2) = \sum_{i=1}^n \mb{s}_\beta(Y_i;x_i,\mb\beta,\sigma^2) ,
\]</span>
so that the estimating equation for <span class="math inline">\(\mb\beta\)</span> becomes
<span class="math display">\[
  \mb{S}_\beta(\mb\beta,\sigma^2) = \mb{0}.
\]</span></p>
<p>Maximum likelihood theory states that the (asymptotic) variance of the MLE <span class="math inline">\(\hat{\mb\beta}\)</span> is related to the <strong>expected Fisher information</strong>. The latter is defined as
<span class="math display">\[
  I_\beta(\mb\beta,\sigma^2) = \E{\mb{s}_\beta(Y;x,\mb\beta,\sigma^2)\mb{s}^t_\beta(Y;x,\mb\beta,\sigma^2)}
\]</span>
which becomes for the linear regression model,
<span class="math display">\[
  I_\beta(\mb\beta,\sigma^2)
  = \E{\frac{1}{\sigma^4}\begin{pmatrix}
  1 &amp; x
  \end{pmatrix} (Y-\beta_0-\beta_1)^2 (1 \;\; x)} 
  = \frac{1}{\sigma^4}\E{(Y-\beta_0-\beta_1)^2} \begin{pmatrix}
  1  &amp; x \\
  x  &amp; x^2 \\
  \end{pmatrix}.
\]</span></p>
</div>
</div>
<div id="an-estimator-of-sigma2" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> An Estimator of <span class="math inline">\(\sigma^2\)</span></h2>
<p>We now know the sampling distribution of the LSE of <span class="math inline">\(\mb\beta\)</span>, but this distribution depends on the variance of the error term, <span class="math inline">\(\sigma^2\)</span>, and this variance is still unknown. To turn the sampling distribution of <span class="math inline">\(\hat{\mb\beta}\)</span> into an instrument that can be used with real data (e.g. for calculating confidence intervals and performing hypothesis tests), we will also need an estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The estimator (and its unbiasedness) are given in the following theorem. We give the result without a proof, but note that it does not require the normality assumption. Als note that the estimator is similar to the MLE of Equation <a href="#eq:MLESigma2">(2.9)</a>, except that the MLE has a factor <span class="math inline">\(1/n\)</span> instead of a factor <span class="math inline">\(1/(n-2)\)</span>. The estimator of <span class="math inline">\(\sigma^2\)</span> can be denoted by <span class="math inline">\(\hat\sigma^2\)</span>, but it is also known as MSE, which stands for the <strong>mean squared error</strong>. This terminology will become clear later.
</p>

<div class="theorem">
<span id="thm:ExpectationMSE" class="theorem"><strong>Theorem 2.6  (An unbiased estimator of <span class="math inline">\(\sigma^2\)</span>)  </strong></span>Assume that model <a href="#eq:Mod3">(2.2)</a> holds true, and let <span class="math inline">\(\mb{x}_i^t\)</span> denote the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{X}\)</span> (<span class="math inline">\(i=1,\ldots, n\)</span>). Then
<span class="math display">\[
  \MSE = \frac{\SSE}{n-2} = \frac{\sum_{i=1}^n (Y_i-\mb{x}_i^t\hat{\mb\beta})^2}{n-2} =
    \frac{(\mb{Y}-\mb{X}\hat{\mb\beta})^t (\mb{Y}-\mb{X}\hat{\mb\beta})}{n-2}
\]</span>
is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>.
</div>
<p>For further purposes, we will also need the sampling distribution of MSE. Note that for the property of unbiasedness the normality assumption was not required, but it will be for developping the sampling distribution. The next theorem is given without proof.
</p>

<div class="theorem">
<span id="thm:DistrMSE" class="theorem"><strong>Theorem 2.7  (Sampling distribution of MSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> is correct. Then,
<span class="math display">\[
 \frac{(n-2) \MSE}{\sigma^2} \sim \chi^2_{n-2}.
\]</span>
</div>
</div>
<div id="exercise-simulation-study-2" class="section level2 unnumbered">
<h2>Exercise: Simulation study</h2>
<p>Set up a simulation study to empirically demonstrate that the MLE of <span class="math inline">\(\sigma^2\)</span> is asymptotically unbiased. So we want you to repeat a simulation study for several choices of the sample size <span class="math inline">\(n\)</span> so as to show that the bias reduces as <span class="math inline">\(n\)</span> increases.</p>
<details>
<summary>
Try to solve this problem and then you can expend this page to look at a solution.
</summary>
<p>Since we have to repeat a simulation study for several choices of the sample size <span class="math inline">\(n\)</span>, I will write an R function to perform the simulation study. This is given in the next chunck of R code.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>simulate.regression<span class="ot">&lt;-</span><span class="cf">function</span>(<span class="at">n=</span><span class="dv">10</span>,<span class="at">sigma2=</span><span class="dv">1</span>,<span class="at">beta0=</span><span class="dv">1</span>,<span class="at">beta1=</span><span class="dv">1</span>,<span class="at">N=</span><span class="dv">100</span>) {</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># function that simulates data from a regression model. </span></span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># The result of the function contains the averages of the MLE estimates of sigma^2, </span></span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#    as well as of the unbiased estimates MSE</span></span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true" tabindex="-1"></a>  <span class="co"># n: sample size</span></span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># sigma2: variance of the error term</span></span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true" tabindex="-1"></a>  <span class="co"># beta0 and beta1: regression parameters</span></span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># N: number of Monte Carlo simulations</span></span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true" tabindex="-1"></a>  sigma2.MLE<span class="ot">&lt;-</span><span class="fu">c</span>() <span class="co"># initiation of vector that will contain the MLEs</span></span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true" tabindex="-1"></a>  sigma2.MSE<span class="ot">&lt;-</span><span class="fu">c</span>() <span class="co"># initiation of vector that will contain the MSEs</span></span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true" tabindex="-1"></a>  x<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="at">length.out =</span> n) <span class="co"># vector with n equally spaced regressor values between 1 and 10</span></span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb117-15"><a href="#cb117-15" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span><span class="fu">sqrt</span>(sigma2)) <span class="co"># simulate outcome data</span></span>
<span id="cb117-16"><a href="#cb117-16" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb117-17"><a href="#cb117-17" aria-hidden="true" tabindex="-1"></a>    sigma2.MLE<span class="ot">&lt;-</span><span class="fu">c</span>(sigma2.MLE,</span>
<span id="cb117-18"><a href="#cb117-18" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">mean</span>(<span class="fu">residuals</span>(m)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb117-19"><a href="#cb117-19" aria-hidden="true" tabindex="-1"></a>    sigma2.MSE<span class="ot">&lt;-</span><span class="fu">c</span>(sigma2.MSE,</span>
<span id="cb117-20"><a href="#cb117-20" aria-hidden="true" tabindex="-1"></a>                  <span class="fu">sum</span>(<span class="fu">residuals</span>(m)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(n<span class="dv">-2</span>))</span>
<span id="cb117-21"><a href="#cb117-21" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb117-22"><a href="#cb117-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">MLE=</span><span class="fu">mean</span>(sigma2.MLE),<span class="at">MSE=</span><span class="fu">mean</span>(sigma2.MSE)))</span>
<span id="cb117-23"><a href="#cb117-23" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we will apply the function (i.e. perform the simulation study) for sample sizes ranging from <span class="math inline">\(n=3\)</span> to <span class="math inline">\(n=100\)</span> and plot the results.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">91869</span>)</span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true" tabindex="-1"></a>sample.sizes<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">50</span>,<span class="dv">75</span>,<span class="dv">100</span>) </span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true" tabindex="-1"></a>sigma2.MLE<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true" tabindex="-1"></a>sigma2.MSE<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(n <span class="cf">in</span> sample.sizes) {</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true" tabindex="-1"></a>  s2<span class="ot">&lt;-</span><span class="fu">simulate.regression</span>(<span class="at">n=</span>n,<span class="at">N=</span>N1000)</span>
<span id="cb118-8"><a href="#cb118-8" aria-hidden="true" tabindex="-1"></a>  sigma2.MLE<span class="ot">&lt;-</span><span class="fu">c</span>(sigma2.MLE,s2<span class="sc">$</span>MLE)</span>
<span id="cb118-9"><a href="#cb118-9" aria-hidden="true" tabindex="-1"></a>  sigma2.MSE<span class="ot">&lt;-</span><span class="fu">c</span>(sigma2.MSE,s2<span class="sc">$</span>MSE)</span>
<span id="cb118-10"><a href="#cb118-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb118-11"><a href="#cb118-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-12"><a href="#cb118-12" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb118-13"><a href="#cb118-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(sample.sizes,sigma2.MLE,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),</span>
<span id="cb118-14"><a href="#cb118-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;MLE estimate of sigma^2&quot;</span>,</span>
<span id="cb118-15"><a href="#cb118-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;MLE&quot;</span>)</span>
<span id="cb118-16"><a href="#cb118-16" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb118-17"><a href="#cb118-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb118-18"><a href="#cb118-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(sample.sizes,sigma2.MSE,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="fl">1.2</span>),</span>
<span id="cb118-19"><a href="#cb118-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;MSE estimate of sigma^2&quot;</span>,</span>
<span id="cb118-20"><a href="#cb118-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;MSE&quot;</span>)</span>
<span id="cb118-21"><a href="#cb118-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">1</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>The results illustrate that the MLE is biased for small sample sizes, but the bias disappears for larger sample sizes. The MSE, on the other hand, is unbiased even for very small sample sizes.</p>
</details>
</div>
<div id="sampling-distributions-of-the-standardised-and-the-studentised-lse" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Sampling Distributions of the Standardised and the Studentised LSE</h2>
<p>In the previous sections we have developped the sampling distributions of the LSE and of the unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. For the construction of confidence intervals and hypothesis tests we will often work with a transformation of the LSE such that the sampling distribution of the transformed LSE does no longer depend on parameters that need to be estimated.</p>
<p>We introduce the notation <span class="math inline">\(\sigma_{\beta_j}^2=\var{\hat\beta_j}\)</span> for the variance of <span class="math inline">\(\hat\beta_j\)</span> (<span class="math inline">\(j=0,1\)</span>). This is thus the appropriate diagonal element of <span class="math inline">\(\var{\hat{\mb\beta}}=(\mb{X}^t\mb{X})^{-1}\sigma^2\)</span>. The latter is often denoted by <span class="math inline">\(\mb\Sigma_{\mb\beta}\)</span>.</p>
<p>With this notation, the <strong>standardised</strong> parameter estimator of <span class="math inline">\(\beta_j\)</span> is then given by
<span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}}.
\]</span>
The following corollary is given without proof.
</p>

<div class="corollary">
<span id="cor:DistrStandBeta" class="corollary"><strong>Corollary 2.1  (Sampling distribution of the standardised LSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> holds true. Then,
<span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}} \sim N(0,1).
\]</span>
Assume that Model <a href="#eq:Mod3">(2.2)</a> holds true. Then, as <span class="math inline">\(n \rightarrow \infty\)</span>,
<span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\sigma_{\beta_j}} \convDistr N(0,1).
\]</span>
</div>
<p>When the variance <span class="math inline">\(\sigma^2\)</span> is not known, it can be replaced by its estimator MSE. The estimator of <span class="math inline">\(\var{\hat{\mb\beta}}=(\mb{X}^t\mb{X})^{-1}\sigma^2\)</span> is often denoted by <span class="math inline">\(\hat{\mb\Sigma}_{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\MSE\)</span> and the estimator of <span class="math inline">\(\sigma^2_{\beta_j}\)</span> by <span class="math inline">\(\hat\sigma^2_{\beta_j}\)</span> or by <span class="math inline">\(S^2_{\beta_j}\)</span>. The square root of <span class="math inline">\(S^2_{\beta_j}\)</span>, i.e. <span class="math inline">\(S_{\beta_j}\)</span>, is also known as the <strong>standard error</strong> (SE or se) of the paramater estimator <span class="math inline">\(\hat\beta_j\)</span>.</p>
<p>The <strong>studentised</strong> estimator of <span class="math inline">\(\beta_j\)</span> is then defined as
<span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{\hat\sigma_{\beta_j}}=\frac{\hat\beta_j-\beta_j}{S_{\beta_j}}.
\]</span>
The following theory gives the (asymptotic) sampling distribution of the stundentised estimators (without proof).
</p>

<div class="theorem">
<span id="thm:DistrStudBeta" class="theorem"><strong>Theorem 2.8  (Sampling Distribution of the studentised LSE)  </strong></span>Assume that Model <a href="#eq:Mod4">(2.7)</a> holds true. Then it holds that
<span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{S_{\beta_j}} \sim t_{n-2}.
\]</span>
Assume that Model <a href="#eq:Mod3">(2.2)</a> holds true. Then, as <span class="math inline">\(n \rightarrow \infty\)</span>,
<span class="math display">\[
  \frac{\hat\beta_j-\beta_j}{S_{\beta_j}} \convDistr N(0,1).
\]</span>
</div>
</div>
<div id="S:BIReg1" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Confidence Intervals</h2>
<p>Theorem <a href="#thm:DistrStudBeta">2.8</a> gives the (asymptotic) sampling distribution of the estimators <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span>. This forms the basis for confidence intervals. In this section we give the result for the normal linear regression model <a href="#eq:Mod4">(2.7)</a>, for which we have developped the exact sampling distributions.</p>
<p>We will use the notation <span class="math inline">\(\sigma^2_{\beta_0}=\var{\hat\beta_0}\)</span> and <span class="math inline">\(\sigma^2_{\beta_1}=\var{\hat\beta_1}\)</span>. These are the diagonal elements of the covariance matrix <span class="math inline">\(\mb\Sigma_\beta\)</span>. Theorem <a href="#thm:DistrStudBeta">2.8</a> implies</p>
<p><span class="math display" id="eq:tmp998765615">\[\begin{equation}
  \frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}} \sim t_{n-2} .
  \tag{2.10}
\end{equation}\]</span></p>
<p>For a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, say <span class="math inline">\(T\sim t_{n-2}\)</span>, it follows by definition that
<span class="math display">\[
   \prob{-t_{n-2;1-\alpha/2} &lt; T &lt; t_{n-2;1-\alpha/2}} = 1-\alpha.
\]</span>
Hence, with <span class="math inline">\(T=\frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}}\sim t_{n-2}\)</span>, the identity
<span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} &lt; \frac{\hat\beta_1 - \beta_1}{\hat\sigma_{\beta_1}} &lt; t_{n-2;1-\alpha/2}} = 1-\alpha
\]</span>
implies that
<span class="math display">\[
  \prob{\hat\beta_1-t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}&lt;  \beta_1 &lt; \hat\beta_1+t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}} = 1-\alpha.
\]</span>
From this equality, the <span class="math inline">\(1-\alpha\)</span> <strong>confidence interval</strong> (CI) of <span class="math inline">\(\beta_1\)</span> follows directly:
<span class="math display">\[
 \left[\hat\beta_1-t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}, \hat\beta_1+t_{n-2;1-\alpha/2} \hat\sigma_{\beta_1}\right].
\]</span>
The construction of the confidence interval of <span class="math inline">\(\beta_0\)</span> is analogous.</p>
<p>The interpretation of a confidence interval is now demonstrated by means of repeated sampling in a simulation study. We start with a small sample size of <span class="math inline">\(n=5\)</span>.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">267213</span>)</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true" tabindex="-1"></a>simRegressionBI<span class="ot">&lt;-</span><span class="cf">function</span>(<span class="at">N=</span><span class="dv">10000</span>,<span class="at">nRep=</span><span class="dv">1</span>) {</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># N: number of repeated samples</span></span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># nRep: number of replicated for each value of the regressor</span></span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true" tabindex="-1"></a>  x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># 5 father&#39;s heights</span></span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true" tabindex="-1"></a>  x<span class="ot">&lt;-</span><span class="fu">rep</span>(x,nRep) <span class="co"># the five regrossor values are replaciated nRep times</span></span>
<span id="cb120-9"><a href="#cb120-9" aria-hidden="true" tabindex="-1"></a>  x</span>
<span id="cb120-10"><a href="#cb120-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb120-11"><a href="#cb120-11" aria-hidden="true" tabindex="-1"></a>  Results<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">beta1Hat=</span><span class="cn">NA</span>,<span class="at">CI.lower=</span><span class="cn">NA</span>,<span class="at">CI.upper=</span><span class="cn">NA</span>,</span>
<span id="cb120-12"><a href="#cb120-12" aria-hidden="true" tabindex="-1"></a>                      <span class="at">cover=</span><span class="cn">NA</span>)</span>
<span id="cb120-13"><a href="#cb120-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb120-14"><a href="#cb120-14" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb120-15"><a href="#cb120-15" aria-hidden="true" tabindex="-1"></a>      y<span class="ot">&lt;-</span><span class="dv">90</span><span class="fl">+0.5</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span><span class="sc">*</span>nRep,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of outcomes</span></span>
<span id="cb120-16"><a href="#cb120-16" aria-hidden="true" tabindex="-1"></a>      m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb120-17"><a href="#cb120-17" aria-hidden="true" tabindex="-1"></a>      Results[i,<span class="dv">1</span>]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)[<span class="dv">2</span>]</span>
<span id="cb120-18"><a href="#cb120-18" aria-hidden="true" tabindex="-1"></a>      Results[i,<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]<span class="ot">&lt;-</span><span class="fu">confint</span>(m)[<span class="dv">2</span>,] <span class="co"># default is a 95% CI</span></span>
<span id="cb120-19"><a href="#cb120-19" aria-hidden="true" tabindex="-1"></a>      Results[i,<span class="dv">4</span>]<span class="ot">&lt;-</span>(<span class="fl">0.5</span><span class="sc">&lt;</span>Results[i,<span class="dv">3</span>])<span class="sc">&amp;</span>(Results[i,<span class="dv">2</span>]<span class="sc">&lt;</span> <span class="fl">0.5</span>) <span class="co"># 0.5 is true parameter value</span></span>
<span id="cb120-20"><a href="#cb120-20" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb120-21"><a href="#cb120-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-22"><a href="#cb120-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(Results)</span>
<span id="cb120-23"><a href="#cb120-23" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-24"><a href="#cb120-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-25"><a href="#cb120-25" aria-hidden="true" tabindex="-1"></a>plotBI<span class="ot">&lt;-</span><span class="cf">function</span>(SimBI,<span class="at">nPlot=</span><span class="fu">nrow</span>(SimBI),<span class="at">mn=</span><span class="fu">min</span>(SimBI<span class="sc">$</span>CI.lower)</span>
<span id="cb120-26"><a href="#cb120-26" aria-hidden="true" tabindex="-1"></a>                 ,<span class="at">mx=</span><span class="fu">max</span>(SimBI<span class="sc">$</span>CI.upper),...) {</span>
<span id="cb120-27"><a href="#cb120-27" aria-hidden="true" tabindex="-1"></a>  <span class="co"># SimBI: results of the function SimRegressionBI</span></span>
<span id="cb120-28"><a href="#cb120-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># nPlot: number of repeated experiments that need to be plotted</span></span>
<span id="cb120-29"><a href="#cb120-29" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mn: lower limit horizontal axis</span></span>
<span id="cb120-30"><a href="#cb120-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mx: upper limit horizontal axis</span></span>
<span id="cb120-31"><a href="#cb120-31" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb120-32"><a href="#cb120-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="at">xlim=</span><span class="fu">c</span>(mn,mx),<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">0</span>,nPlot),<span class="at">xlab=</span><span class="st">&quot;&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;experiment&quot;</span>,</span>
<span id="cb120-33"><a href="#cb120-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">type=</span><span class="st">&quot;n&quot;</span>,...)</span>
<span id="cb120-34"><a href="#cb120-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>nPlot) {</span>
<span id="cb120-35"><a href="#cb120-35" aria-hidden="true" tabindex="-1"></a>      <span class="fu">arrows</span>(SimBI<span class="sc">$</span>CI.lower[i],i,SimBI<span class="sc">$</span>CI.upper[i],i,</span>
<span id="cb120-36"><a href="#cb120-36" aria-hidden="true" tabindex="-1"></a>             <span class="at">code=</span><span class="dv">0</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb120-37"><a href="#cb120-37" aria-hidden="true" tabindex="-1"></a>             <span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">col=</span><span class="dv">2</span><span class="sc">-</span>SimBI<span class="sc">$</span>cover[i])</span>
<span id="cb120-38"><a href="#cb120-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">points</span>(SimBI<span class="sc">$</span>beta1Hat[i],i)</span>
<span id="cb120-39"><a href="#cb120-39" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb120-40"><a href="#cb120-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">abline</span>(<span class="at">v=</span><span class="fl">0.5</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">1</span>)</span>
<span id="cb120-41"><a href="#cb120-41" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb120-42"><a href="#cb120-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb120-43"><a href="#cb120-43" aria-hidden="true" tabindex="-1"></a>SimBI5<span class="ot">&lt;-</span><span class="fu">simRegressionBI</span>(<span class="at">N=</span>N1000,<span class="at">nRep=</span><span class="dv">1</span>)</span>
<span id="cb120-44"><a href="#cb120-44" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(SimBI5<span class="sc">$</span>cover) <span class="co"># empirical coverage</span></span></code></pre></div>
<pre><code>## [1] 0.955</code></pre>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plotBI</span>(SimBI5,<span class="at">nPlot=</span><span class="dv">30</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SimBI5"></span>
<img src="DASM2_files/figure-html/SimBI5-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.11: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<p>From the output we read the <strong>empirical coverage</strong> of the <span class="math inline">\(95\%\)</span> confidence interval: 0.955. This is based on 1000 repeated experiments. For a large number of repeated experiments, the empirical coverage is a good approximation of the true coverage probability.
In our simulation study, the empirical coverage is (approximately) equal to the <strong>nominal</strong> <span class="math inline">\(95\%\)</span> confidence level, which (empirically) demonstrates that the theory is correct (the CI has its correct probabilistic interpretation).
The results are visualised in Figure <a href="#fig:SimBI5">2.11</a>: the first 30 CIs are shown. Of these 30 intervals, 29 cover the true parameter value <span class="math inline">\(\beta_1=0.5\)</span>. That gives thus an empirical coverage of <span class="math inline">\(29/30=96.7\%\)</span>. Note that the R output shows the empirical coverage of all 1000 repeated experiments.</p>
<p>Figure <a href="#fig:SimBI5b">2.12</a> shows the results of the first 100 repeated experiments.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SimBI5b"></span>
<img src="DASM2_files/figure-html/SimBI5b-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 100 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.12: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 100 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">267213</span>)</span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plotBI</span>(SimBI5,<span class="at">nPlot=</span><span class="dv">30</span>,<span class="at">mn=</span><span class="sc">-</span><span class="fl">0.2</span>,<span class="at">mx=</span><span class="dv">1</span>,<span class="at">main=</span><span class="st">&quot;n=5&quot;</span>)</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true" tabindex="-1"></a>SimBI10<span class="ot">&lt;-</span><span class="fu">simRegressionBI</span>(<span class="at">N=</span>N1000,<span class="at">nRep=</span><span class="dv">2</span>)</span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plotBI</span>(SimBI10,<span class="at">nPlot=</span><span class="dv">30</span>,<span class="at">mn=</span><span class="sc">-</span><span class="fl">0.2</span>,<span class="at">mx=</span><span class="dv">1</span>,<span class="at">main=</span><span class="st">&quot;n=10&quot;</span>)</span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true" tabindex="-1"></a>SimBI50<span class="ot">&lt;-</span><span class="fu">simRegressionBI</span>(<span class="at">N=</span>N1000,<span class="at">nRep=</span><span class="dv">10</span>)</span>
<span id="cb123-10"><a href="#cb123-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plotBI</span>(SimBI50,<span class="at">nPlot=</span><span class="dv">30</span>,<span class="at">mn=</span><span class="sc">-</span><span class="fl">0.2</span>,<span class="at">mx=</span><span class="dv">1</span>,<span class="at">main=</span><span class="st">&quot;n=50&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:SimBIAll"></span>
<img src="DASM2_files/figure-html/SimBIAll-1.png" alt="95\% confidence intervals from repeated sampling ($n=5$, $n=10$ and $n=50$). The points represent the point estimates $\hat\beta_1$. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: $\beta_1=0.5$." width="672" />
<p class="caption">
Figure 2.13: 95% confidence intervals from repeated sampling (<span class="math inline">\(n=5\)</span>, <span class="math inline">\(n=10\)</span> and <span class="math inline">\(n=50\)</span>). The points represent the point estimates <span class="math inline">\(\hat\beta_1\)</span>. Only the results of the first 30 repeated experiments are shown. The vertical line indicates the true parameter value: <span class="math inline">\(\beta_1=0.5\)</span>.
</p>
</div>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>We repeat the simulation study for larger sample sizes. The results are shown in Figure <a href="#fig:SimBIAll">2.13</a> (only for the first 30 repeated experiments). The empirical coverages based on 1000 repeated experiments are:</p>
<ul>
<li><p><span class="math inline">\(n=5\)</span>: 0.955</p></li>
<li><p><span class="math inline">\(n=10\)</span>: 0.958</p></li>
<li><p><span class="math inline">\(n=50\)</span>: 0.948</p></li>
</ul>
<p>For all sample sizes <span class="math inline">\(n\)</span> the empirical covarages are very close to the nominal confidence level of <span class="math inline">\(95\%\)</span>.</p>
<p>Figure <a href="#fig:SimBIAll">2.13</a> demonstrates that the lengths of the confidence intervals become smaller as the sample size increases. This follows directly from the theory: the length of a <span class="math inline">\(95\%\)</span> is <span class="math inline">\(2t_{n-2;1-0.05/2} \hat\sigma_{\beta_1}\)</span> and this decreases because <span class="math inline">\(\hat\sigma_{\beta_1}\)</span> decreases in expectation with increasing sample size (<span class="math inline">\(\sigma_{\beta_1} \propto \frac{1}{\sqrt{n}}\)</span>).
Thus, on average the lenght of the intervals decreases as <span class="math inline">\(n\)</span> increases, and still the coverage remains <span class="math inline">\(95\%\)</span>. This phenomenon is related to the variability of the estimates: with increasing sample size <span class="math inline">\(n\)</span>, the variability of the estimates decreases and thus the CI can be smaller while still giving a coverage of <span class="math inline">\(95\%\)</span>.</p>
</div>
<div id="example-galtons-height-data-2" class="section level2 unnumbered">
<h2>Example (Galton’s height data)</h2>
<p>We repeat the regression analysis for Galton’s data. This time we look at the standard errors and the <span class="math inline">\(95\%\)</span> confidence interval of the regression coefficient <span class="math inline">\(\beta_1\)</span>.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm,<span class="at">data=</span>Galton.sons)</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<p>The standard errors of the parameter estimates are
<span class="math display">\[
   \hat\sigma_{\beta_0} =  11.7 \;\;\text{ and }\;\; \hat\sigma_{\beta_1} =  0.0668.
 \]</span>
From the R output we also read the MSE (square of the ``Residual standard error"):
<span class="math display">\[
   \MSE = 5.661^2 = 32.05
 \]</span>
and the residual number of degrees of freedom is <span class="math inline">\(n-2=173-2=171\)</span>.</p>
<p>Next we use the standard error of <span class="math inline">\(\hat\beta_1\)</span> for computing the <span class="math inline">\(95\%\)</span> confidence interval of <span class="math inline">\(\beta_1\)</span>. We also need <span class="math inline">\(t_{n-2;1-\alpha/2}=t_{171;0.975}\)</span> nodig.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a><span class="co"># quantile of t-distribution</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.975</span>,<span class="at">df=</span><span class="dv">171</span>)</span></code></pre></div>
<pre><code>## [1] 1.973934</code></pre>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a><span class="co"># lower bound of 95% CI</span></span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true" tabindex="-1"></a><span class="fl">0.50766</span><span class="sc">-</span><span class="fu">qt</span>(<span class="fl">0.975</span>,<span class="at">df=</span><span class="dv">171</span>)<span class="sc">*</span><span class="fl">0.0668</span></span></code></pre></div>
<pre><code>## [1] 0.3758012</code></pre>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="co"># upper bound of 95% CI</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="fl">0.50766</span><span class="sc">+</span><span class="fu">qt</span>(<span class="fl">0.975</span>,<span class="at">df=</span><span class="dv">171</span>)<span class="sc">*</span><span class="fl">0.0668</span></span></code></pre></div>
<pre><code>## [1] 0.6395188</code></pre>
<p>The <span class="math inline">\(95\%\)</span> confidence interval of <span class="math inline">\(\beta_1\)</span> thus becomes
<span class="math display">\[
   [0.376 , 0.640] .
 \]</span>
Hence, with a probability of 95% we expect that the regression coefficient <span class="math inline">\(\beta_1\)</span> is somewhere between <span class="math inline">\(0.376\)</span> and <span class="math inline">\(0.640\)</span>. Thus, if the father’s height increases with 1cm, we expect with a probability of 95% that the average son’s height increases with <span class="math inline">\(0.376\)</span> cm to <span class="math inline">\(0.640\)</span> cm. Equivalently, we could say that if the father’s height increases with 5cm, we expect with a probability of 95% that the average son’s height increases with <span class="math inline">\(5\times 0.376=1.88\)</span> cm to <span class="math inline">\(5 \times 0.640=3.2\)</span> cm.</p>
<p>Note that all values within the CI are positive. The data are thus consistent with a positive effect of father’s height on the expected son’s height.</p>
<p>The next R code can also be used for the computation of the CI.</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m)</span></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 66.6519190 112.9844701
## father.cm    0.3757362   0.6395761</code></pre>
<p>Finally we note that the correct probabilistic interpretation of the CI depends on the distributional assumption that is part of the statistical model. In particular: correct specification of the the conditional mean <span class="math inline">\(m(x)\)</span> as a linear function of <span class="math inline">\(x\)</span>; normality of the error term; homoskedasticity; mutual independence of the outcomes. Later we will introduce methods that can be used for assessing these assumptions.</p>
</div>
<div id="exercise-blood-pressure-1" class="section level2 unnumbered">
<h2>Exercise: Blood Pressure</h2>
<p>Consider again the blood pressure dataset and calculate and interpret a <span class="math inline">\(95\%\)</span> confidence interval of the regression slope.</p>
<details>
<summary>
Try to make this exercise and expand the page to see the soluation.
</summary>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the data</span></span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/BloodPressure.RData&quot;</span>)</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true" tabindex="-1"></a><span class="co"># fit the model</span></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure)</span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 95% confidence intervals</span></span>
<span id="cb135-8"><a href="#cb135-8" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m, <span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -1.948188 2.014267
## dose         1.437469 2.135218</code></pre>
<p>Based on this output we conclude that with a probability of <span class="math inline">\(95\%\)</span> we expect the blood pressure to be reduced with 1.4 to 2.1 mmHg when the dose increases with 1mg per day.</p>
</details>
</div>
<div id="S:RegTests" class="section level2" number="2.7">
<h2><span class="header-section-number">2.7</span> Hypothesis Tests</h2>
<p>A frequent reason for performing a regression analysis, is to give an answer to the research question whether or not the average outcome is linearly associated with a regressor. In the Galton’s data example, Galton wanted to know whether or not the average height of son’s linearly depends on their father’s height. In terms of regression model <a href="#eq:Mod4">(2.7)</a> this research question translates into the null hypothesis
<span class="math display">\[
  H_0: \beta_1 = 0.
\]</span>
The alternative hypothesis depends on the exact formulation of the research question, or on prior knowledge. In the Galton’s data example there may be two sensible situations one could think of:</p>
<ul>
<li><p>Galton did not have a clue as to what the relation between father’s and son’s heights could be, because there is also the mother and environmental factors (<em>nature versus nurture</em>). In this setting, the alternative hypothesis becomes <span class="math inline">\(H_1: \beta_1\neq 0\)</span>. (<em>two-sided alternative</em>)</p></li>
<li><p>Galton had prior knowledge (from literature, from discussions with other scientists, or from other independent datasets) that a negative relation (i.e. <span class="math inline">\(\beta_1&lt;0\)</span>) is not plausible. In this setting, the alternative hypothesis would be formulated as <span class="math inline">\(H_1: \beta_1&gt; 0\)</span>. (<em>one-sided alternative to the right</em>)</p></li>
<li><p>For completeness, we also give this third option, which does probably not make much sense for the Galton data example. Galton could not have been interested in detecting a negative association, in which case he would again have chosen for the alternative <span class="math inline">\(H_1: \beta_1&gt; 0\)</span>.</p></li>
</ul>
<p>We propose the test statistic
<span class="math display">\[
  T =  \frac{\hat\beta_1}{\hat\sigma_{\beta_1}} .
\]</span>
From this expression, it is evident that this statistic is sensitive for deviations from <span class="math inline">\(H_0\)</span> in the direction of the two-sided as well as the one-sided alternatives.</p>
<p>If we assume that the normal regression model <a href="#eq:Mod4">(2.7)</a> holds, then from Equation <a href="#eq:tmp998765615">(2.10)</a> it follows that
<span class="math display">\[
 T \HSim t_{n-2}.
\]</span>
(i.e. under the null hypothesis the test statistic <span class="math inline">\(T\)</span> has a <span class="math inline">\(t_{n-2}\)</span> <strong>null distribution</strong>).
Based on this null distribution, <span class="math inline">\(p\)</span>-values and rejection regions can be computed. In particular:</p>
<ul>
<li><p>The one-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1&gt;0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for large values of <span class="math inline">\(T\)</span>. Hence,
<span class="math display">\[
 p = \probf{0}{T\geq t} = 1-\probf{0}{T\leq t} = 1-F_T(t;n-2),
 \]</span>
with <span class="math inline">\(F_T(.;n-2)\)</span> the CDF of <span class="math inline">\(t_{n-2}\)</span>.<br />
The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from
<span class="math display">\[
 \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{T&gt;t_{n-2;1-\alpha}}.
 \]</span>
The rejection region is thus <span class="math inline">\([t_{n-2;1-\alpha},+\infty[\)</span>. We also say that <span class="math inline">\(t_{n-2;1-\alpha}\)</span> is the <em>critical value</em>.</p></li>
<li><p>The one-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1&lt;0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for small values of <span class="math inline">\(T\)</span> (i.e. large, but negative values of <span class="math inline">\(T\)</span>). Hence,
<span class="math display">\[
 p = \probf{0}{T\leq t} = F_T(t;n-2).
 \]</span>
The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from
<span class="math display">\[
 \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{T&lt;t_{n-2;\alpha}}=\probf{0}{T&lt;-t_{n-2;1-\alpha}},
 \]</span>
and is thus given by <span class="math inline">\(]-\infty, -t_{n-2;1-\alpha}]\)</span>. Or, equivalently, the critical value is <span class="math inline">\(t_{n-2;\alpha}=-t_{n-2;1-\alpha}\)</span>.</p></li>
<li><p>The two-sided alternative hypothesis <span class="math inline">\(H_1:\beta_1\neq 0\)</span>.<br />
We wish to reject <span class="math inline">\(H_0\)</span> in favor of <span class="math inline">\(H_1\)</span> for large and small values of <span class="math inline">\(T\)</span> (i.e. large positive and negative values of <span class="math inline">\(T\)</span>). Hence,
<span class="math display">\[
 p = \probf{0}{|T|\geq |t|} = \probf{0}{T\leq -|t| \text{ or } T \geq |t|} = 2 \probf{0}{T\geq |t|}=2(1-F_T(|t|;n-2)).
 \]</span>
The rejection region for the test at the <span class="math inline">\(\alpha\)</span> significance level follows from
<span class="math display">\[
 \alpha= \prob{\text{type I error}}=\probf{0}{\text{reject }H_0}=\probf{0}{|T|&gt;t_{n-2;1-\alpha/2}},
 \]</span>
and it is thus given by <span class="math inline">\(]-\infty,-t_{n-2;1-\alpha/2}] \cup [t_{n-2;1-\alpha/2},+\infty[\)</span>, or, equivalently, the critical value for the test based on <span class="math inline">\(|T|\)</span> is given by <span class="math inline">\(t_{n-2;1-\alpha/2}\)</span>.</p></li>
</ul>
</div>
<div id="example-galtons-height-data-3" class="section level2 unnumbered">
<h2>Example (Galton’s height data)</h2>
<p>Let’s look again at the results of the regression analysis in R.</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm,<span class="at">data=</span>Galton.sons)</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<p>We want to test the null hypothesis <span class="math inline">\(H_0: \beta_1=0\)</span> against the alternative hypothesis <span class="math inline">\(H_1: \beta_1\neq 0\)</span>. In the output we read on the line of <em>father.cm</em>: <span class="math inline">\(t=7.596\)</span> and <span class="math inline">\(p=1.91\times 10^{-12}\)</span>. We verify these results in the next chunck.</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a><span class="co"># observed test statistic</span></span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true" tabindex="-1"></a>t.obs<span class="ot">&lt;-</span><span class="fl">0.50766</span> <span class="sc">/</span> <span class="fl">0.06683</span> </span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true" tabindex="-1"></a>t.obs</span></code></pre></div>
<pre><code>## [1] 7.596289</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a><span class="co"># two-sided p-value</span></span>
<span id="cb141-2"><a href="#cb141-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(<span class="fu">abs</span>(t.obs),<span class="at">df=</span><span class="dv">171</span>))</span></code></pre></div>
<pre><code>## [1] 1.905587e-12</code></pre>
<p>Hence, at the 5% level of significance we conclude that there is a positive effect of the father’s height on the average height of their sons (<span class="math inline">\(p&lt;0.0001\)</span>). From previous analyses we know that the effect is estimated as <span class="math inline">\(\hat\beta_1=0.51\)</span> with a 95% confidence interval of <span class="math inline">\(0.376\)</span> to <span class="math inline">\(0.640\)</span>.</p>
<p>Suppose that the researchers know that the effect of the father’s height on the average son’s height can never be negative. Then the alternative hypothesis becomes <span class="math inline">\(H_1: \beta_1&gt;0\)</span>. The R output from the <em>lm</em> function, however, always gives the <span class="math inline">\(p\)</span>-value for a two-sided alternative hypothesis. Upon using the symmetry of the <span class="math inline">\(t_{n-2}\)</span> null distribution we can easily convert the two-sided <span class="math inline">\(p\)</span>-value to a one-sided <span class="math inline">\(p\)</span>-value. In particular, the R output gives the two-sided <span class="math inline">\(p\)</span>-value, which is defined as
<span class="math display">\[
   2\probf{0}{T\geq |t|} .
 \]</span>
For the one-sided test we need
<span class="math display">\[
   \probf{0}{T\geq t}.
\]</span>
If <span class="math inline">\(t&gt;0\)</span>, then <span class="math inline">\(t=+|t|\)</span> and thus
<span class="math display">\[
   p=\probf{0}{T\geq t} = \probf{0}{T\geq |t|} ,
\]</span>
Consequently, the one-sided <span class="math inline">\(p\)</span>-value equals half of the two-sided <span class="math inline">\(p\)</span>-value if <span class="math inline">\(t\)</span> is positive. Thus,<br />
<span class="math display">\[
   p = \frac{1}{2}\times 1.9\times 10^{-12} = 9.5\times 10^{-13}.
\]</span>
For this data example we come to the same conclusion as before.</p>
</div>
<div id="exercise-converting-two-sided-p-values" class="section level2 unnumbered">
<h2>Exercise: Converting two-sided p-values</h2>
<p>As an exercise, derive the formula to find the <span class="math inline">\(p\)</span>-value for the one-sided testing problem with <span class="math inline">\(H_1: \beta_1&lt;0\)</span>, starting from the <span class="math inline">\(p\)</span>-value for the two-sided alternative, <span class="math inline">\(2\probf{0}{T\geq |t|}\)</span>. Derive the conversion for both a positive and a negative observed test statistic <span class="math inline">\(t\)</span>. Calculate this one-sided <span class="math inline">\(p\)</span>-value for the Galton’s height data.</p>
<details>
<summary>
Try to make this exercise and expand the page to see the soluation.
</summary>
<p>For the one-sided test we need
<span class="math display">\[
   \probf{0}{T\leq t}.
\]</span></p>
<p>If <span class="math inline">\(t&lt;0\)</span>, then <span class="math inline">\(t=-|t|\)</span> and thus
<span class="math display">\[
   p=\probf{0}{T\leq t} = \probf{0}{T\leq -|t|} = \probf{0}{-T\geq |t|} = \probf{0}{T\geq |t|} ,
\]</span>
in which the last step made use of the symmetry of the null distrubution of <span class="math inline">\(T\)</span> (i.e. the distribution of <span class="math inline">\(-T\)</span> equals the distribution of <span class="math inline">\(T\)</span> under <span class="math inline">\(H_0\)</span>). Consequently, the one-sided <span class="math inline">\(p\)</span>-value equals half of the two-sided <span class="math inline">\(p\)</span>-value if <span class="math inline">\(t\)</span> is negative. With <span class="math inline">\(p_2=2\probf{0}{T\geq |t|}\)</span>, this one-sided <span class="math inline">\(p\)</span>-value thus equals <span class="math inline">\(p_2/2\)</span>.</p>
<p>If <span class="math inline">\(t&gt;0\)</span>, then <span class="math inline">\(t=|t|\)</span> and thus
<span class="math display">\[
   p=\probf{0}{T\leq t} = \probf{0}{T\leq |t|} = 1-\probf{0}{T\geq |t|} .
\]</span>
With <span class="math inline">\(p_2=2\probf{0}{T\geq |t|}\)</span>, the one-sided <span class="math inline">\(p\)</span>-value can thus be calculated as
<span class="math display">\[
  p = 1- \frac{p_2}{2}.
\]</span></p>
In the Galton’s height example, the observed <span class="math inline">\(t\)</span>-value equals <span class="math inline">\(7.596&gt;0\)</span>, and hence the <span class="math inline">\(p\)</span>-value that corresponds to <span class="math inline">\(H_1:\beta_1&lt;0\)</span> becomes
<span class="math display">\[
   p = 1-\frac{1}{2}\times 1.9\times 10^{-12} \approx 1.
\]</span>
</details>
</div>
<div id="exercise-muscle-mass" class="section level2 unnumbered">
<h2>Exercise: Muscle mass</h2>
<p>Scientists suspect that the muscle mass of people starts declining from a certain age onwards. To verify this research question, a nutritionist randomly sampled 59 women, aged between 41 and 78. For these women; also the muscle mass was measured (we actually only have a proxy based on bioelectrical impedance measurements).</p>
<p>Perform a regression analysis and formulate an answer to this research question (including parameter estimates, confidence interval and hypothesis test). You may use the next chunk of R code for reading the data.</p>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>muscles<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;Data/muscles.txt&quot;</span>, <span class="at">sep=</span><span class="st">&quot; &quot;</span>)</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(muscles)<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="st">&quot;muscle.mass&quot;</span>,<span class="st">&quot;age&quot;</span>)</span>
<span id="cb143-3"><a href="#cb143-3" aria-hidden="true" tabindex="-1"></a><span class="fu">skim</span>(muscles)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-40">Table 2.2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">muscles</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">59</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="12%" />
<col width="8%" />
<col width="12%" />
<col width="5%" />
<col width="5%" />
<col width="2%" />
<col width="4%" />
<col width="3%" />
<col width="4%" />
<col width="4%" />
<col width="36%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">muscle.mass</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">84.61</td>
<td align="right">16.11</td>
<td align="right">52</td>
<td align="right">73.0</td>
<td align="right">84</td>
<td align="right">96.5</td>
<td align="right">119</td>
<td align="left">&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2583&gt;</td>
</tr>
<tr class="even">
<td align="left">age</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">60.27</td>
<td align="right">11.68</td>
<td align="right">41</td>
<td align="right">51.5</td>
<td align="right">60</td>
<td align="right">70.0</td>
<td align="right">78</td>
<td align="left">&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2586&gt;&lt;U+2587&gt;</td>
</tr>
</tbody>
</table>
<details>
<summary>
Try to make this exercise and expand the page to see the soluation.
</summary>
<p>First we fit the linear regression model and make a graph of the data and the fitted regression line.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(muscle.mass<span class="sc">~</span>age, <span class="at">data=</span>muscles)</span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = muscle.mass ~ age, data = muscles)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -16.121  -6.373  -0.674   6.968  23.455 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 156.22438    5.68612   27.48   &lt;2e-16 ***
## age          -1.18820    0.09265  -12.82   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.244 on 57 degrees of freedom
## Multiple R-squared:  0.7426, Adjusted R-squared:  0.7381 
## F-statistic: 164.5 on 1 and 57 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m)</span></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) 144.838119 167.610636
## age          -1.373721  -1.002678</code></pre>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(muscles,</span>
<span id="cb148-2"><a href="#cb148-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>age, <span class="at">y=</span>muscle.mass)) <span class="sc">+</span></span>
<span id="cb148-3"><a href="#cb148-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb148-4"><a href="#cb148-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;age (years)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;muscle mass&quot;</span>) <span class="sc">+</span></span>
<span id="cb148-5"><a href="#cb148-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb148-6"><a href="#cb148-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span>m<span class="sc">$</span>coefficients[<span class="dv">1</span>],<span class="at">slope=</span>m<span class="sc">$</span>coefficients[<span class="dv">2</span>]) </span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>From the output we can formulate the following conclusions.</p>
The <span class="math inline">\(p\)</span>-value that is reported in the R output refers to the two-sided alternative. Here we translate the research question into the one-sided alternative <span class="math inline">\(H_1: \beta_1&lt;0\)</span>. Since the observed test statistic <span class="math inline">\(t\)</span>=-12.83 is negative the <span class="math inline">\(p\)</span>-value becomes 1.9249708^{-18}/2<span class="math inline">\(&lt;0.0001\)</span>.
Hence, on average the muscle mass of women aged between 41 and 78 years is significantly negatively associated with the age (<span class="math inline">\(p&lt;0.001\)</span>) at the <span class="math inline">\(5\%\)</span> level of significance. We estimate that the mean muscle mass decreases with 1.19 (SE=0.09) units with an increase of age of 1 year. The <span class="math inline">\(95\%\)</span> confidence interval of this estimate is -1.37 to -1 units of muscle mass per increase of age with one year.
</details>
</div>
<div id="exercise-choosing-h_1-after-looking-at-the-data" class="section level2 unnumbered">
<h2>Exercise: choosing <span class="math inline">\(H_1\)</span> after looking at the data</h2>
<p>Set up a simulation study to demonstrate that the following hypothesis testing procedure does not control the type I error rate at the nominal <span class="math inline">\(\alpha\)</span> level.</p>
<p>This is the procedure to consider in the context of the simple linear regression model:</p>
<ul>
<li><p>generate the data and estimate the regression parameters</p></li>
<li><p>if <span class="math inline">\(\hat\beta_1&lt;0\)</span>, set the alternative hypothesis to <span class="math inline">\(H_1: \beta_1&lt;0\)</span>, otherwise set the alternative hypothesis to <span class="math inline">\(H_1:\beta_1&gt;0\)</span></p></li>
<li><p>compute the <span class="math inline">\(p\)</span>-value for the test for testing <span class="math inline">\(H_0:\beta_1=0\)</span> versus the alternative selected in the previous step.</p></li>
</ul>
<details>
<summary>
Try to make this exercise and expand the page to see the soluation.
</summary>
<p>In the next chunck of R code data are simulated under the null hypothesis <span class="math inline">\(\beta_1=0\)</span>, because the type I error rate is the probability to reject the null hypothesis, given that the null hypothesis is true.
For each repeated sample we compute the p-value for the one-sided alternative that is determined by the sign of the parameter estimate <span class="math inline">\(\hat\beta_1\)</span>.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">87175</span>)</span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 <span class="co"># number of repeated experiments</span></span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">165</span>,<span class="dv">170</span>,<span class="dv">175</span>,<span class="dv">180</span>,<span class="dv">185</span>) <span class="co"># five father&#39;s heights</span></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="dv">1</span>,x) <span class="co"># design matrix</span></span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>pvalues<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb149-6"><a href="#cb149-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb149-7"><a href="#cb149-7" aria-hidden="true" tabindex="-1"></a>  y<span class="ot">&lt;-</span><span class="dv">90</span><span class="sc">+</span><span class="dv">0</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">5</span>,<span class="at">sd=</span><span class="dv">5</span>) <span class="co"># random sample of 5 outcomes -- we simulate under H_0:beta1=0</span></span>
<span id="cb149-8"><a href="#cb149-8" aria-hidden="true" tabindex="-1"></a>  m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb149-9"><a href="#cb149-9" aria-hidden="true" tabindex="-1"></a>  beta1Hat<span class="ot">&lt;-</span><span class="fu">coef</span>(m)[<span class="dv">2</span>]</span>
<span id="cb149-10"><a href="#cb149-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(beta1Hat<span class="sc">&lt;</span><span class="dv">0</span>) {</span>
<span id="cb149-11"><a href="#cb149-11" aria-hidden="true" tabindex="-1"></a>    p<span class="ot">&lt;-</span><span class="fu">summary</span>(m)<span class="sc">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]<span class="sc">/</span><span class="dv">2</span> <span class="co"># if t&lt;0, the p-value for H_1: beta1&lt;0 needs to be devided by 2</span></span>
<span id="cb149-12"><a href="#cb149-12" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb149-13"><a href="#cb149-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(beta1Hat<span class="sc">&gt;</span><span class="dv">0</span>) {</span>
<span id="cb149-14"><a href="#cb149-14" aria-hidden="true" tabindex="-1"></a>    p<span class="ot">&lt;-</span><span class="fu">summary</span>(m)<span class="sc">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>]<span class="sc">/</span><span class="dv">2</span> <span class="co"># if t&gt;0, the p-value for H_1: beta1&gt;0 needs to be devided by 2</span></span>
<span id="cb149-15"><a href="#cb149-15" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb149-16"><a href="#cb149-16" aria-hidden="true" tabindex="-1"></a>  pvalues<span class="ot">&lt;-</span><span class="fu">c</span>(pvalues,p)</span>
<span id="cb149-17"><a href="#cb149-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb149-18"><a href="#cb149-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb149-19"><a href="#cb149-19" aria-hidden="true" tabindex="-1"></a><span class="co"># empirical type I error rate for alpha=0.05</span></span>
<span id="cb149-20"><a href="#cb149-20" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(pvalues<span class="sc">&lt;</span><span class="fl">0.05</span>)</span></code></pre></div>
<pre><code>## [1] 0.0982</code></pre>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="co"># histogram </span></span>
<span id="cb151-2"><a href="#cb151-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(pvalues, <span class="at">cex.lab=</span><span class="fl">1.5</span>, <span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb151-3"><a href="#cb151-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>))</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
<p>This simulation study shows that the type I error rate is approximated by 0.0982, which is larger than the nominal value of <span class="math inline">\(\alpha=0.05\)</span>. This testing procedure thus does not control the type I error rate at its nominal level. The histogram no longer shows a uniform distribution between 0 and 1 (which we would expect if the test was correctly constructed).</p>
</details>
</div>
<div id="S:AssessAssumptions" class="section level2" number="2.8">
<h2><span class="header-section-number">2.8</span> Assessment of the Model Assumptions</h2>
<p>The normal simple linear regression model <a href="#eq:Mod4">(2.7)</a> is the most restrictive than the semiparametric linear regression model <a href="#eq:Mod3">(2.2)</a> in the sense that it requires the largest numer of assumptions. In this section we discuss the importance of the model assumptions and how they can be verified based on the observed data. We will use Galton’s height data to illustrate the methods.</p>
</div>
<div id="example-galtons-height-data-4" class="section level2 unnumbered">
<h2>Example (Galton’s height data)</h2>
<p>We will verify one-by-one the model assumption of model <a href="#eq:Mod4">(2.7)</a>.</p>
<div id="linearity-of-the-regression-model" class="section level4 unnumbered">
<h4>Linearity of the regression model</h4>
<p>The conditional mean of the outcome must satisfy
<span class="math display">\[
   \E{Y\mid x} = m(x;\mb\beta) = \beta_0+\beta_1 x.
 \]</span>
If the parameters are known, then this is equivalent to the condition
<span class="math display">\[
   0 = \E{\eps \mid x} = \E{Y-m(x;\mb\beta)\mid x}=\E{Y-\beta_0-\beta_1 x\mid x} .
 \]</span></p>
<p>If there are replicated outcomes available for a given <span class="math inline">\(x\)</span>, then <span class="math inline">\(\E{Y-\beta_0-\beta_1 x\mid x}\)</span> can be (unbiasedly) estimated as the sample mean of the residuals <span class="math inline">\(e_i=y_i-\hat\beta_0-\hat\beta_1x_i\)</span> for which <span class="math inline">\(x_i=x\)</span>. These avarage residuals can be computed for all <span class="math inline">\(x\in \{x_1,\ldots, x_n\}\)</span>. Note that for Galton’s data, for some of the regressor values (father’s heights) there is only one observed outcome. For these regressor values, the sample mean of the residuals equals the residual.
Figure <a href="#fig:GaltonRes1">2.14</a> shows the result for the Galton data.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm,<span class="at">data=</span>Galton.sons)</span>
<span id="cb152-2"><a href="#cb152-2" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span>m<span class="sc">$</span>residuals</span>
<span id="cb152-3"><a href="#cb152-3" aria-hidden="true" tabindex="-1"></a>x.all<span class="ot">&lt;-</span><span class="fu">unique</span>(Galton.sons<span class="sc">$</span>father.cm)</span>
<span id="cb152-4"><a href="#cb152-4" aria-hidden="true" tabindex="-1"></a>ave.e<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb152-5"><a href="#cb152-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(x <span class="cf">in</span> x.all) {</span>
<span id="cb152-6"><a href="#cb152-6" aria-hidden="true" tabindex="-1"></a>    ave.e<span class="ot">&lt;-</span><span class="fu">c</span>(ave.e,<span class="fu">mean</span>(e[Galton.sons<span class="sc">$</span>father.cm<span class="sc">==</span>x]))</span>
<span id="cb152-7"><a href="#cb152-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb152-8"><a href="#cb152-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Galton.sons<span class="sc">$</span>father.cm,e,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb152-9"><a href="#cb152-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;residuals&quot;</span>)</span>
<span id="cb152-10"><a href="#cb152-10" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x.all,ave.e,<span class="at">col=</span><span class="dv">2</span>,<span class="at">pch=</span><span class="dv">8</span>)</span>
<span id="cb152-11"><a href="#cb152-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GaltonRes1"></span>
<img src="DASM2_files/figure-html/GaltonRes1-1.png" alt="Scatterplot of the residuals against the father's heights (Galton's height example). The red stars represent the sample means of the residuals for a given father's height." width="672" />
<p class="caption">
Figure 2.14: Scatterplot of the residuals against the father’s heights (Galton’s height example). The red stars represent the sample means of the residuals for a given father’s height.
</p>
</div>
<p>Since the sample means of the residuals are only estimates of <span class="math inline">\(\E{Y-\beta_0-\beta_1 x\mid x}\)</span>, we cannot expect that these sample means are exactly equal to zero (i.e. the sample means also show sampling variability). The larger the number of replicates on which such a sample mean is computed, the smaller the sampling variability and the closer we expect the sample mean to be close to zero.</p>
<p>We hope that the average residuals do not show a systematic pattern as a function of the regressor. No systematic pattern would agree with the model assumption <span class="math inline">\(\E{\eps \mid x}=0\)</span>. Figure <a href="#fig:GaltonRes1">2.14</a> shows no such systematic pattern, and therefore we conclude that the linear relation between the regressor (father’s height) and the mean response (son’s height) is linear.</p>
<p>If there are no replicates at the regressor values, then the plot can be constructed, but with no sample means of the residuals. Such graphs (with or without the average residuals) are known as <strong>residual plots</strong>.</p>
</div>
<div id="normality-of-the-error-term" class="section level3 unnumbered">
<h3>Normality of the error term</h3>
<p>The model implies that
<span class="math display">\[ 
  \eps_i=Y_i-m(x_i;\mb\beta) \mid x_i \sim N(0,\sigma^2). 
\]</span></p>
<p>To some extent the residuals <span class="math inline">\(e_i=Y_i-m(x_i;\hat{\mb\beta})\)</span> can be considered as ``estimates’’ of <span class="math inline">\(\eps_i\)</span>. Therefore we will use the residuals for assessing the normality assumption. We could use histograms, boxplots and normal QQ-plots for this purpose. Particularly the normal QQ-plots are informative, because they are specifically developped for this assessing normality.</p>
<p>Figure <a href="#fig:GaltonQQResid">2.15</a> shows the normal QQ-plot of the residuals of the Galton example. Most of the points in the QQ-plot are close to the straight line with no systematic pattern, and with only a few larger deviations in the right hand tail of the distribution. However, the number of outliers (2 or 3) is very small as compared to the sample size (173). The plot does also not reveal systematic deviations from the straight line.</p>
<p>Finally, note that the normality assumption is not very important for this example, because the rather large sample size of <span class="math inline">\(n=173\)</span> tells us that the parameter estimators will be approximately normally distributed thanks to the central limit theorem. So only a very strong deviation from normality would have been worrisome. Also recall that the <span class="math inline">\(p\)</span>-value for the two-sided test for <span class="math inline">\(H_0:\beta_1=0\)</span> was very small (<span class="math inline">\(p&lt;0.001\)</span>); thus even a small deviation from normality would not have caused doubt over the conclusion of the statistical test.</p>
<p>The R code for the QQ-plot in Figure <a href="#fig:GaltonQQResid">2.15</a> is shown below.</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(m<span class="sc">$</span>resid,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab=</span><span class="st">&quot;expected quantiles&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;residuals&quot;</span>,<span class="at">main=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(m<span class="sc">$</span>resid)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GaltonQQResid"></span>
<img src="DASM2_files/figure-html/GaltonQQResid-1.png" alt="Normal QQ-plot of the residuals of the Galton example." width="672" />
<p class="caption">
Figure 2.15: Normal QQ-plot of the residuals of the Galton example.
</p>
</div>
</div>
<div id="homoskedasticity" class="section level3 unnumbered">
<h3>Homoskedasticity</h3>
<p>Model <a href="#eq:Mod4">(2.7)</a> implies that
<span class="math display">\[
  \var{\eps_i}=\var{\eps_i\mid x_i} =\var{Y_i \mid x_i}=\sigma^2,
\]</span>
i.e. the variance of the outcomes (and of the error terms) is constant and does not depend on the values of the regressor.</p>
<p>If there are replicated observations for each value of the regressor in the dataset, then the sample variance of the outcome can be calculated for unique observed value of the regressor. The sample variances can then be plotted against the regressor <span class="math inline">\(x\)</span>. If this graph shows no clear pattern that deviates from the assumption of a constant variance, then the graph suggests that the constant-variance assumption is satisfied.</p>
<p>If there are no replicated observations for the unique regressor values, then one may plot <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span>. If we reason that the residuals <span class="math inline">\(e_i\)</span> are (approximately) estimates for the error terms <span class="math inline">\(\eps_i\)</span>, and because <span class="math inline">\(\var{\eps_i} = \E{(\eps_i-\E{\eps_i})^2} = \E{\eps_i^2}\)</span>, we expect that <span class="math inline">\(\E{e_i^2} \approx \E{\eps_i^2}\)</span> and hence the plot of <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span> should not indicate a systematic pattern under the assumption of constant-variance.</p>
<p>If there are regressor values with and without replicates, then one could either choose to simply plot <span class="math inline">\(e_i^2\)</span> for all <span class="math inline">\(i=1,\ldots, n\)</span>, or for the replicated observations one may plot the average of the <span class="math inline">\(e_i^2\)</span> (note that this is different from the sample variances because of the factor <span class="math inline">\(1/(n-1)\)</span> in the calculation of the sample variance, and the factor <span class="math inline">\(1/n\)</span> in the calculation of the average). The reason for using <span class="math inline">\(1/n\)</span> in this case, is to make these averages more comparable to the individually plotted <span class="math inline">\(e_i^2\)</span>.</p>
<p>Figure <a href="#fig:GaltonResidVar">2.16</a> shows the plot for the Galton example. No systematic pattern can be observed, except for some outliers, particularly in the plot in the left panel (variance). The plot in the right panel is less extreme because he square-root transformation is applied to the (average of the) <span class="math inline">\(e_i^2\)</span> (this is this plotted at the scale of the standard deviation and the scale of the outcomes). Note that particularly in the graph in the left panel we expect strong skewness to the right, because the sampling distribution of the sample variance is related to a <span class="math inline">\(\chi^2\)</span> distribution.</p>
<p>We conclude that there is no indication for a violation of the constant-variance assumption.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>heights<span class="ot">&lt;-</span><span class="fu">unique</span>(Galton.sons<span class="sc">$</span>father.cm)</span>
<span id="cb154-3"><a href="#cb154-3" aria-hidden="true" tabindex="-1"></a>var.y<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb154-4"><a href="#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(x <span class="cf">in</span> heights) {</span>
<span id="cb154-5"><a href="#cb154-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">#var.y&lt;-c(var.y,var(Galton.sons$son.cm[Galton.sons$father.cm==x]))</span></span>
<span id="cb154-6"><a href="#cb154-6" aria-hidden="true" tabindex="-1"></a>  var.y<span class="ot">&lt;-</span><span class="fu">c</span>(var.y,<span class="fu">sum</span>(m<span class="sc">$</span>residuals[Galton.sons<span class="sc">$</span>father.cm<span class="sc">==</span>x]<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb154-7"><a href="#cb154-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb154-8"><a href="#cb154-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(heights,var.y,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb154-9"><a href="#cb154-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,</span>
<span id="cb154-10"><a href="#cb154-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;sample variance&quot;</span>,<span class="at">col=</span><span class="dv">2</span>,<span class="at">pch=</span><span class="dv">8</span>)</span>
<span id="cb154-11"><a href="#cb154-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(m<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">173</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb154-12"><a href="#cb154-12" aria-hidden="true" tabindex="-1"></a><span class="co"># note that the reference line is at MSE (n-2)/n</span></span>
<span id="cb154-13"><a href="#cb154-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-14"><a href="#cb154-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(heights,<span class="fu">sqrt</span>(var.y),<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb154-15"><a href="#cb154-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,</span>
<span id="cb154-16"><a href="#cb154-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;sample standard deviation&quot;</span>,</span>
<span id="cb154-17"><a href="#cb154-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">col=</span><span class="dv">2</span>,<span class="at">pch=</span><span class="dv">8</span>)</span>
<span id="cb154-18"><a href="#cb154-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sqrt</span>(<span class="fu">sum</span>(m<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="dv">173</span>),<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GaltonResidVar"></span>
<img src="DASM2_files/figure-html/GaltonResidVar-1.png" alt="Sample variances (left) en standard deviations (right) against the  regressor (father's height). The horizontal reference line corresponds to the MSE (left) and en $\sqrt{\MSE}$ (right)." width="672" />
<p class="caption">
Figure 2.16: Sample variances (left) en standard deviations (right) against the regressor (father’s height). The horizontal reference line corresponds to the MSE (left) and en <span class="math inline">\(\sqrt{\MSE}\)</span> (right).
</p>
</div>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="co"># note that the reference line is at MSE (n-2)/n</span></span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>The next two graphs in Figure <a href="#fig:GaltonResid2">2.17</a> show the squared residuals <span class="math inline">\(e_i^2\)</span> versus <span class="math inline">\(x_i\)</span> (thus no averaging, even at regressor values with replicated observations) and the absolute values <span class="math inline">\(\vert e_i \vert\)</span> against <span class="math inline">\(x_i\)</span>. The latter graph is also supposed to give no systematic pattern if the assumption of constant-variance holds, and it is less sensitive to outliers as compared to the plot with the squared residuals. We also expect no skewness in the plot of the absolute values.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span>m<span class="sc">$</span>residuals</span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Galton.sons<span class="sc">$</span>father.cm,e<span class="sc">^</span><span class="dv">2</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span>,</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Squared residuals&quot;</span>)</span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(m<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">173-2</span>),<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span>m<span class="sc">$</span>residuals</span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Galton.sons<span class="sc">$</span>father.cm,<span class="fu">abs</span>(e),<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb156-10"><a href="#cb156-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Father&#39;s height (cm)&quot;</span></span>
<span id="cb156-11"><a href="#cb156-11" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">ylab=</span><span class="st">&quot;absolute value of the residuals&quot;</span>)</span>
<span id="cb156-12"><a href="#cb156-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(<span class="fu">abs</span>(m<span class="sc">$</span>residuals))<span class="sc">/</span><span class="dv">173</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:GaltonResid2"></span>
<img src="DASM2_files/figure-html/GaltonResid2-1.png" alt="Scatter plots of $e_i^2$ against $x_i$ and of $| e_i |$ against $x_i$ for the Galton example. The horizontal reference lines correspond to MSE (left) and $\frac{1}{n}\sum_{i=1}^n | e_i |$ (right)." width="672" />
<p class="caption">
Figure 2.17: Scatter plots of <span class="math inline">\(e_i^2\)</span> against <span class="math inline">\(x_i\)</span> and of <span class="math inline">\(| e_i |\)</span> against <span class="math inline">\(x_i\)</span> for the Galton example. The horizontal reference lines correspond to MSE (left) and <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n | e_i |\)</span> (right).
</p>
</div>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
</div>
</div>
<div id="exercise-muscle-mass-1" class="section level2 unnumbered">
<h2>Exercise: Muscle mass</h2>
<p>Assess the assumption for the regression analysis of the muscle mass example.</p>
<details>
<summary>
Try to make this exercise and expand the page to see the soluation.
</summary>
<p>First we fit the regression model.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(muscle.mass<span class="sc">~</span>age, <span class="at">data=</span>muscles)</span></code></pre></div>
<p><strong>Linearity of the regression model</strong></p>
<p>We can assess the linearity by simple plotting the residuals versus the regressor (age). If there are multiple observations for an age, we can also compute and plot the average residuals.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span>m<span class="sc">$</span>residuals</span>
<span id="cb159-2"><a href="#cb159-2" aria-hidden="true" tabindex="-1"></a>x.all<span class="ot">&lt;-</span><span class="fu">unique</span>(muscles<span class="sc">$</span>age)</span>
<span id="cb159-3"><a href="#cb159-3" aria-hidden="true" tabindex="-1"></a>ave.e<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb159-4"><a href="#cb159-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(x <span class="cf">in</span> x.all) {</span>
<span id="cb159-5"><a href="#cb159-5" aria-hidden="true" tabindex="-1"></a>    ave.e<span class="ot">&lt;-</span><span class="fu">c</span>(ave.e,<span class="fu">mean</span>(e[muscles<span class="sc">$</span>age<span class="sc">==</span>x]))</span>
<span id="cb159-6"><a href="#cb159-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb159-7"><a href="#cb159-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(muscles<span class="sc">$</span>age,e,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb159-8"><a href="#cb159-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;residuals&quot;</span>)</span>
<span id="cb159-9"><a href="#cb159-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x.all,ave.e,<span class="at">col=</span><span class="dv">2</span>,<span class="at">pch=</span><span class="dv">8</span>)</span>
<span id="cb159-10"><a href="#cb159-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>This graph does not suggest any deviation from linearity because we do not observe a systematic pattern of the (average) residuals versus the age.</p>
<p>One may also add a <em>nonparametric smoother</em> to the residual plot. Although you may perhaps still do not know what is a nonparametric smoother (outside of the scope of this course), you may simply interpret it as a nonparametric estimate of <span class="math inline">\(\E{E \mid x}\)</span>, with <span class="math inline">\(E\)</span> the residual. This is illustrated next.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(muscles<span class="sc">$</span>age,e,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb160-2"><a href="#cb160-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;residuals&quot;</span>)</span>
<span id="cb160-3"><a href="#cb160-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb160-4"><a href="#cb160-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(<span class="fu">lowess</span>(muscles<span class="sc">$</span>age,e))</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>The smooth line again does not suggest a deviation from linearity.</p>
<p><strong>Normality of the error terms</strong></p>
<p>We make a normal QQ-plot of the residuals, as well as a boxplot. The normal QQ-plot shows small systematic deviations in the two tails of the distribution, but the shape of the distribution is still quite symmetric (see also the boxplot). Given the moderately large sample size of <span class="math inline">\(n=59\)</span>, we do consider this a problematic deviation from the normality assumption and we can still trust the results for confidence intervals and the hypothesis test (moreover, the <span class="math inline">\(p\)</span>-value is extremely small).</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(m<span class="sc">$</span>resid,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab=</span><span class="st">&quot;expected quantiles&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;residuals&quot;</span>,<span class="at">main=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(m<span class="sc">$</span>resid)</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(m<span class="sc">$</span>residuals, <span class="at">ylab=</span><span class="st">&quot;residuals&quot;</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p><strong>Homoskedasticity</strong></p>
<p>In the next chunck of R code two plots are produced (squared residuals against regressor and absolute value against regressor). These graphs indicate no violation against the constant-variance assumption.</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span>m<span class="sc">$</span>residuals</span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(muscles<span class="sc">$</span>age,e<span class="sc">^</span><span class="dv">2</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,</span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Squared residuals&quot;</span>)</span>
<span id="cb163-6"><a href="#cb163-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(m<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">173-2</span>),<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb163-7"><a href="#cb163-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-8"><a href="#cb163-8" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span>m<span class="sc">$</span>residuals</span>
<span id="cb163-9"><a href="#cb163-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(muscles<span class="sc">$</span>age,<span class="fu">abs</span>(e),<span class="at">cex.lab=</span><span class="fl">1.5</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,</span>
<span id="cb163-10"><a href="#cb163-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;Age (years)&quot;</span></span>
<span id="cb163-11"><a href="#cb163-11" aria-hidden="true" tabindex="-1"></a>     ,<span class="at">ylab=</span><span class="st">&quot;absolute value of the residuals&quot;</span>)</span>
<span id="cb163-12"><a href="#cb163-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(<span class="fu">abs</span>(m<span class="sc">$</span>residuals))<span class="sc">/</span>(<span class="dv">173-2</span>),<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
</details>
</div>
<div id="binary-dummy-regressors" class="section level2" number="2.9">
<h2><span class="header-section-number">2.9</span> Binary Dummy Regressors</h2>
<p>In this section we will demonstrate how the <strong>two-sample problem</strong> is a special case of simple linear regression. In the two sample problem, we are interested in comparing two means. For example, consider the dose finding study, but we only look at the patients that received 0mg (control or placebo group) and 2mg per day. We want to know the mean difference in blood pressure reduction between these two doses. Thus we basically have two samples of patients: a sample of 10 patients receiving placebo and a sample of 10 patients receiving 2mg/day. We are not interested in assessing a linear effect of the dose. We now consider the 0mg/day and 2mg/day as two treatments (placebo and active treatment).</p>
<p>More formally, we consider two groups, populations or treatments (whatever term you prefer), say treatment U and treatment V. The interest is in estimating
<span class="math display">\[
  \mu_U = \E{Y \mid U} \;\; \text{ and } \;\; \mu_V=\E{Y \mid V}
\]</span>
and the treatment <em>effect size</em>
<span class="math display">\[
 \delta = \mu_V-\mu_U.
\]</span></p>
<p>Of course we know the solution:
<span class="math display">\[
  \hat\mu_U = \bar{Y}_U \;\;\text{ and } \;\; \hat\mu_V=\bar{Y}_V \;\;\text{ and }\;\; \hat\delta = \hat\mu_V-\hat\mu_V,
\]</span>
in which we used the obvious notation of <span class="math inline">\(\bar{Y}_U\)</span> and <span class="math inline">\(\bar{Y}_V\)</span> representing the sample means of the outcomes in the <span class="math inline">\(U\)</span> and the <span class="math inline">\(V\)</span> sample, respectively. Confidence intervals can be easily obtained as well as hypothesis tests for testing <span class="math inline">\(H_0: \delta=0\)</span> against one-sided or two-sided alternatives (this is the well known two-sample <span class="math inline">\(t\)</span>-test).</p>
<p>For the confidence intervals and two-sample hypothesis test, we often assume normality in the two treatment groups (unless the samples sizes are large), i.e.
<span class="math display">\[
  Y_i \mid U \sim N(\mu_U, \sigma^2) \;\;\text{ and }\;\; Y_i \mid V \sim N(\mu_V, \sigma^2).
\]</span></p>
<p>This two-sample problem can also be formulated as a regression model, which will allow us to apply all theory that we have seen before.</p>
<p>Define a <strong>dummy regressor</strong> <span class="math inline">\(x_i\)</span> as</p>
<p><span class="math display">\[\begin{eqnarray*}
  x_i 
    &amp;=&amp; 1 \text{ if observation i belongs to treatment V} \\
    &amp;=&amp; 0 \text{ if observation i belongs to treatment U} .
\end{eqnarray*}\]</span></p>
<p>With this definition, we build the conventional regression model,
<span class="math display">\[
  Y_i = \beta_0 + \beta_1 x_i + \eps_i
\]</span>
with <span class="math inline">\(\eps_i\)</span> i.i.d. <span class="math inline">\(N(0,\sigma^2)\)</span>. Or, equivalently,
<span class="math display">\[
 Y_i \mid x_i \sim N(\beta_0+\beta_1 x_i , \sigma^2).
\]</span>
Since <span class="math inline">\(x_i\)</span> can take only two values we can explicitely look at the <span class="math inline">\(x_i=1\)</span> and <span class="math inline">\(x_i=0\)</span> possibilities:
<span class="math display">\[
  Y_i \mid x_i=0 \sim N(\beta_0 , \sigma^2)
\]</span>
and
<span class="math display">\[
  Y_i \mid x_i=1 \sim N(\beta_0 + \beta_1 , \sigma^2).
\]</span>
Comparing these expressions with our earlier formulation of the two-sample problem, we find for outcomes in treatment group U:
<span class="math display">\[
  \mu_U = \E{Y\mid U} = \E{Y \mid x=0} = \beta_0 
\]</span>
and for for outcomes in group V:
<span class="math display">\[
  \mu_V = \E{Y\mid V} = \E{Y \mid x=1} = \beta_0 +\beta_1.
\]</span>
This immediately gives
<span class="math display">\[
  \delta=\mu_V-\mu_U = \beta_1.
\]</span>
Thus the regression parameter <span class="math inline">\(\beta_1\)</span> in the linear regression model with the 0/1 binary dummy variable can be directly interpreted as the effect size <span class="math inline">\(\delta=\mu_V-\mu_U\)</span>. The methods for confidence intervals and hypothesis testing for the parameter <span class="math inline">\(\beta_1\)</span> can now be directly applied.</p>
<p>Also note that the regression parameter <span class="math inline">\(\beta_0=\mu_U\)</span>.</p>
</div>
<div id="example" class="section level2 unnumbered">
<h2>Example</h2>
<p>Consider now the blood pressure example with only the 0mg/day and 2mg/day observations, and define
<span class="math display">\[\begin{eqnarray*}
  x_i 
    &amp;=&amp; 1 \text{ if observation i belongs to the 2mg/day group} \\
    &amp;=&amp; 0 \text{ if observation i belongs to the 0mg/day group (placebo)} .
\end{eqnarray*}\]</span></p>
<p>The next chunck of R code shows the analysis with R. We start with subsetting the blood pressure dataset and with defining the binary dummy regressor.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract the 0mg/day and 2mg/day observations</span></span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>BloodPressure2<span class="ot">&lt;-</span>BloodPressure <span class="sc">%&gt;%</span></span>
<span id="cb165-3"><a href="#cb165-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(dose<span class="sc">==</span><span class="dv">0</span><span class="sc">|</span>dose<span class="sc">==</span><span class="dv">2</span>)</span>
<span id="cb165-4"><a href="#cb165-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb165-5"><a href="#cb165-5" aria-hidden="true" tabindex="-1"></a><span class="co"># define binary dummy</span></span>
<span id="cb165-6"><a href="#cb165-6" aria-hidden="true" tabindex="-1"></a>BloodPressure2<span class="sc">$</span>x<span class="ot">&lt;-</span><span class="fu">ifelse</span>(BloodPressure2<span class="sc">$</span>dose<span class="sc">==</span><span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb165-7"><a href="#cb165-7" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(BloodPressure2<span class="sc">$</span>dose,BloodPressure2<span class="sc">$</span>x)</span></code></pre></div>
<pre><code>##    
##      0  1
##   0 10  0
##   2  0 10</code></pre>
<p>The table demonstrates that we correctly defined the regressor. Now we fit the linear regression model.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>x,<span class="at">data=</span>BloodPressure2)</span>
<span id="cb167-2"><a href="#cb167-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ x, data = BloodPressure2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.10  -1.75   0.20   1.50   5.90 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.900      1.168  -0.771  0.45083   
## x              5.400      1.651   3.270  0.00425 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.692 on 18 degrees of freedom
## Multiple R-squared:  0.3727, Adjusted R-squared:  0.3378 
## F-statistic: 10.69 on 1 and 18 DF,  p-value: 0.004252</code></pre>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m)</span></code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept) -3.353076 1.553076
## x            1.930827 8.869173</code></pre>
<p>And now the same analysis but with the t.test function in R.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure2,</span>
<span id="cb171-2"><a href="#cb171-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">var.equal=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  bp.reduction by dose
## t = -3.2702, df = 18, p-value = 0.004252
## alternative hypothesis: true difference in means between group 0 and group 2 is not equal to 0
## 95 percent confidence interval:
##  -8.869173 -1.930827
## sample estimates:
## mean in group 0 mean in group 2 
##            -0.9             4.5</code></pre>
<p>The agreement between the results can be seen directly.</p>
</div>
<div id="exercise-smoking" class="section level2 unnumbered">
<h2>Exercise: Smoking</h2>
<p>We are presented with a sample of 654 youths, aged 3 to 19 years, in the area of East Boston
during middle to late 1970’s. Interest concerns the relationship between smoking and FEV (forced expiratory volume; it measures how much air a person can exhale during a forced breath, measured in liters). In the dataset, the smoke variable is already coded as a dummy variable: smoke=0 refers to non-smokers and smoke=1 refers to smokers.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>fev<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;data/fevdata.txt&quot;</span>,<span class="at">sep=</span><span class="st">&quot; &quot;</span>)</span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(fev)<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="st">&quot;age&quot;</span>,<span class="st">&quot;fev&quot;</span>,<span class="st">&quot;height&quot;</span>,<span class="st">&quot;sex&quot;</span>,<span class="st">&quot;smoke&quot;</span>)</span></code></pre></div>
<p>Fit a linear regression model with the dummy <em>smoke</em> as regressor and interpret the model fit (estimated regression coefficient, <span class="math inline">\(95\%\)</span> confidence interval, and hypohtesis test for <span class="math inline">\(H_0:\beta_1=0\)</span> versus <span class="math inline">\(H_1: \beta_1\neq 0\)</span> at the <span class="math inline">\(5\%\)</span> level of significance).</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(fev<span class="sc">$</span>fev<span class="sc">~</span>fev<span class="sc">$</span>smoke, <span class="at">ylab=</span><span class="st">&quot;FEV (liters)&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;smoke&quot;</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(fev<span class="sc">~</span>smoke,<span class="at">data=</span>fev)</span>
<span id="cb175-2"><a href="#cb175-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = fev ~ smoke, data = fev)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.140  -3.491  -2.093   4.467  13.467 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   5.5329     0.2022   27.36   &lt;2e-16 ***
## smoke         6.5597     0.3820   17.17   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.384 on 651 degrees of freedom
##   (345 observations deleted due to missingness)
## Multiple R-squared:  0.3117, Adjusted R-squared:  0.3107 
## F-statistic: 294.9 on 1 and 651 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m)</span></code></pre></div>
<pre><code>##                2.5 %   97.5 %
## (Intercept) 5.135845 5.930036
## smoke       5.809588 7.309810</code></pre>
<p>We conclude at the <span class="math inline">\(5\%\)</span> level of significance that smokers have on average a significant larger FEV (<span class="math inline">\(p&lt;0.001\)</span>). We estimate that on average the FEV of smokers is 6.56 (SE=0.38) liters larger than among the non-smokers. The corresponding <span class="math inline">\(95\%\)</span> confidence interval ranges from <span class="math inline">\(5.81\)</span> to <span class="math inline">\(7.31\)</span> liters.</p>
</details>
</div>
<div id="S:Causality" class="section level2" number="2.10">
<h2><span class="header-section-number">2.10</span> Association versus Causation</h2>
<div id="introduction-1" class="section level3" number="2.10.1">
<h3><span class="header-section-number">2.10.1</span> Introduction</h3>
<p>Recall some of the examples that we have seen before:</p>
<ul>
<li><p>Galton’s data: we found a significant positive effect of the height of the father on the average height of their sons <span class="math inline">\((p&lt;0.001\)</span>).</p></li>
<li><p>Blood pressure data: we found a significant positive effect of the dose on the average blood pressure reduction (<span class="math inline">\(p=0.003\)</span>).</p></li>
<li><p>Muscle mass data: we found a significant negative effect of the age on the mean muscle mass (<span class="math inline">\(p&lt;0.001\)</span>).</p></li>
<li><p>FEV data: we found a significant larger FEV among the smokers as compared to the non-smokers (<span class="math inline">\(p&lt;0.001\)</span>)</p></li>
</ul>
<p>The conclusions all refer to <strong>associations</strong> between a regressor and the (mean) outcome, but they do not necessarily imply a <strong>causation</strong>. Think about the following questions:</p>
<ul>
<li><p>Can we conclude that increasing fathers’ heights (e.g. by means of a better diet in the fathers’ youth) causes the average height of their sons to become larger?</p></li>
<li><p>Can we conclude that increasing the daily dose of the active compound in the blood pressure lowering medication causes the average blood pressure reduction to increase?</p></li>
<li><p>Can we conclude that smoking causes the FEV to become larger on average (i.e. smoking causes the lung function to become better)?</p></li>
</ul>
<p>In this section we will argue that for the blood pressure reduction example, we do have established a causal relationship, but for the other examples we do not have demonstrated causality (only association).</p>
<p>All examples listed above, except for the blood pressure study, are <strong>observational studies</strong>. This means that the regressor <span class="math inline">\(X_i\)</span> and the outcome <span class="math inline">\(Y_i\)</span> variables are sampled together, without the researcher having control over the decision which subject <span class="math inline">\(i\)</span> is assigned to what value of <span class="math inline">\(X_i\)</span>. In the blood pressure study, on the other hand, the research planned how the subjects would be assigned to what daily dose. In particular, the subjects in the study were <strong>randomised</strong> over the doses. This is an example of an <strong>experimental study</strong>. We will demonstrate that in the latter case, associations can be interpreted as causal effects.</p>
</div>
<div id="causal-inference-and-counterfactuals" class="section level3" number="2.10.2">
<h3><span class="header-section-number">2.10.2</span> Causal inference and counterfactuals</h3>
<p><strong>Causal inference</strong> is a discipline in statistics that aims to develop methods that can be used to assess causal relationships. Although it is only rather recent (last 20 years) that it has become a very active research area, concepts of causality were already proposed by Sir Ronald Fisher and Jerzy Neyman in the 1920s. The modern revival started in the early 1980 with e.g. the work of Donald Rubin.</p>
<p>In this section we explain some of the basic concepts of causal inference. As an example we will consider the blood pressure reduction study with only the 0mg/day (placebo) (<span class="math inline">\(X_i=0\)</span>) and the 2mg/day treatment groups (<span class="math inline">\(X_i=1\)</span>) (two-sample problem).</p>
<p>We now define two <strong>counterfactual</strong> outcomes:</p>
<ul>
<li><p><span class="math inline">\(Y_i(1)\)</span> is the outcome of subject <span class="math inline">\(i\)</span> if subject <span class="math inline">\(i\)</span> would receive treatment <span class="math inline">\(X_i=1\)</span></p></li>
<li><p><span class="math inline">\(Y_i(0)\)</span> is the outcome of subject <span class="math inline">\(i\)</span> if subject <span class="math inline">\(i\)</span> would receive treatment <span class="math inline">\(X_i=0\)</span>.</p></li>
</ul>
<p>In reality a subject can only receice a single treatment, either <span class="math inline">\(X_i=1\)</span> or <span class="math inline">\(X_i=0\)</span>, and hence both counterfactuals can never be observed simultaneously. Still we can think of <span class="math inline">\(Y_i(1)\)</span> and <span class="math inline">\(Y_i(0)\)</span> as inherent properties of subject <span class="math inline">\(i\)</span>.</p>
<p>Thus, what we observe is <span class="math inline">\((X_i,Y_i)\)</span>, with <span class="math inline">\(X_i\)</span> (0 or 1) the actual treatment that is received by subject <span class="math inline">\(i\)</span> and <span class="math inline">\(Y_i\)</span> the observed outcome under treatment <span class="math inline">\(X_i\)</span>.
Note that the observed <span class="math inline">\(Y_i\)</span> can be written as
<span class="math display">\[
  Y_i = X_i Y_i(1) + (1-X_i) Y_i(0).
\]</span></p>
<p>In the context of the blood pressure example:</p>
<ul>
<li><p><span class="math inline">\(Y_i(0)\)</span> is the blood pressure reduction if subject <span class="math inline">\(i\)</span> would have received placebo</p></li>
<li><p><span class="math inline">\(Y_i(1)\)</span> is the blood pressure reduction if subject <span class="math inline">\(i\)</span> would have received 2mg/day.</p></li>
</ul>
<p>Thus conceptually we can also think of <span class="math inline">\(Y_i(1)-Y_i(0)\)</span> as the <strong>causal effect</strong> of the treatment for subject <span class="math inline">\(i\)</span>. This could then serve as the basis for testing the following null hypothesis:
<span class="math display">\[
  H_0: Y_i(1)=Y_i(0) \;\;\text{ for all subjects i in the population}.
\]</span>
This is known as the <strong>sharp causal null hypothesis</strong>. However, we will be interested in the <strong>average causal effect</strong>, defined in terms of population averages:
<span class="math display">\[
  \E{Y(1)-Y(0)}=\E{Y(1)}- \E{Y(0)}.
\]</span>
In the remainder of this course, we will simply call this the <strong>causal effect</strong>. In the context of the two-sample problem, it is also known as the <strong>average treatment effect</strong> (ATE).</p>
<p>The question is now: do we have unbiased estimators of <span class="math inline">\(\E{Y(1)}\)</span> and <span class="math inline">\(\E{Y(0)}\)</span>? If yes, we also have an unbiased estimator of the causal effect. It may be tempting to consider <span class="math inline">\(\bar{Y}_1\)</span> (sample mean of observed outcomes in the <span class="math inline">\(X=1\)</span> group) as an estimator of <span class="math inline">\(\E{Y(1)}\)</span>, and <span class="math inline">\(\bar{Y}_0\)</span> (sample mean of observed outcomes in the <span class="math inline">\(X=0\)</span> group) as an estimator of <span class="math inline">\(\E{Y(0)}\)</span>. We will show that this holds true for studies that involve complete randomisation, but it does not hold in general.</p>
<p>Suppose we would use <span class="math inline">\(\bar{Y}_a\)</span> (<span class="math inline">\(a=0,1\)</span>). What is the <em>estimand</em> of this estimator (i.e. what population parameter is estimated by the estimator). Thus we need to look at <span class="math inline">\(\E{\bar{Y}_a}\)</span>. To be more precise, we write <span class="math inline">\(\Ef{XY}{\bar{Y}_a}\)</span> to stress that both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> may be random.</p>
<p>Let <span class="math inline">\(N_a\)</span> denote the number of sample observations in treatment group <span class="math inline">\(a\)</span> (<span class="math inline">\(a=0,1\)</span>), which can be written as</p>
<p><span class="math display">\[
  N_1 = \sum_{i=1}^n X_i  \;\;\text{ and }\;\; N_0 = \sum_{i=1}^n (1-X_i).
\]</span>
Then,
<span class="math display">\[\begin{eqnarray*}
  \Ef{XY}{\bar{Y}_a}
    &amp;=&amp; \Ef{XY}{\frac{1}{N_a}\sum_{i: X_i=a} Y_i} \\
    &amp;=&amp; \Ef{X}{\Ef{Y\mid X}{\frac{1}{N_a}\sum_{i: X_i=a} Y_i}} \\
    &amp;=&amp; \Ef{X}{\frac{1}{N_a} \sum_{i: X_i=a} \Ef{Y\mid X}{Y_i \mid X_i=a}} \\
    &amp;=&amp; \Ef{X}{\frac{1}{N_a} N_a\Ef{Y\mid X}{Y \mid X=a}} \\
    &amp;=&amp; \Ef{Y\mid X}{Y \mid X=a}.
\end{eqnarray*}\]</span></p>
<p>Hence, for <span class="math inline">\(\bar{Y}_a\)</span> to be an unbiased estimator of <span class="math inline">\(\E{Y(a)}\)</span>, we need to demonstrate that
<span class="math display">\[
  \E{Y \mid X=a} = \E{Y(a)}.
\]</span>
The conceptual differnce between these two expectation is illustrated in Figure <a href="#fig:Causal1">2.18</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Causal1"></span>
<img src="figures/Causal1.png" alt="Difference between means of counterfactuals and conditional means (Figure 1.1. from Hern\'an and Robins, Causal Inference: What if, 2020)."  />
<p class="caption">
Figure 2.18: Difference between means of counterfactuals and conditional means (Figure 1.1. from Hern'an and Robins, Causal Inference: What if, 2020).
</p>
</div>
<p>We will need the following assumptions:</p>
<ul>
<li><p><strong>consistency</strong>: For all subjects <span class="math inline">\(i\)</span>, it must hold that <span class="math inline">\(X_i=a\)</span> implies that <span class="math inline">\(Y_i(a)=Y_i\)</span>. In other words, when a subject receives treatment a, the observed outcome of the subject must be equal to the counterfacual for that treatment.</p></li>
<li><p><strong>mean exchangeability</strong>: <span class="math inline">\(\E{Y(a) \mid X=0} = \E{Y(a) \mid X=1} = \E{Y(a)}\)</span>, for <span class="math inline">\(a=0/1\)</span>. This condition tells us that the populations of subjects that receive placebo (<span class="math inline">\(X=0\)</span>) and that receive treatment (<span class="math inline">\(X=1\)</span>), have the same mean value for the counterfactual <span class="math inline">\(Y(a)\)</span> for treatment <span class="math inline">\(a\)</span> (<span class="math inline">\(a=0/1\)</span>), and, as a consequence these two conditional means are equal to the marginal mean <span class="math inline">\(\E{Y(a)}\)</span>. This means that the treatment allocation <span class="math inline">\(X\)</span> is not associated with the mean of the counterfactual outcomes.</p></li>
</ul>
<p>A property that is even stronger than mean exchangeability, is <strong>full exchangeability</strong>. This states that the treatment assignment <span class="math inline">\(X\)</span> is stochastically independent of the vector of counterfactual outcomes of a subject, <span class="math inline">\((Y(0),Y(1))\)</span>, i.e. 
<span class="math display">\[
  (Y(0), Y(1)) \ind X.
\]</span>
This implies that the treatment <span class="math inline">\(X_i\)</span> given to subject <span class="math inline">\(i\)</span>, is jointly independent of the counterfactual outcomes <span class="math inline">\(Y_i(0)\)</span> and <span class="math inline">\(Y_i(1)\)</span>.
Full exchangeability implies mean exchangeability.</p>
<p>Now we are ready to prove the identity <span class="math inline">\(\E{Y \mid X=a} = \E{Y(a)}\)</span>. For <span class="math inline">\(a=0,1\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\E{Y(a)}=\E{Y(a) \mid X=a}\)</span> (implied by mean exchangeability)</p></li>
<li><p><span class="math inline">\(\E{Y(a) \mid X=a} = \E{Y \mid X=a}\)</span> (implied by consistency).</p></li>
</ol>
</div>
<div id="randomised-studies" class="section level3" number="2.10.3">
<h3><span class="header-section-number">2.10.3</span> Randomised studies</h3>
<p>Full exchangeability is implied by a <strong>completely randomised study</strong>, i.e. a study in which subjects are randomly assigned to the treatment groups. For example, the binary treatment indicator can be Bernoulli distributed with
<span class="math display">\[
  \prob{X_i=1}=\frac{1}{2}.
\]</span>
This may come with or without the extra condition <span class="math inline">\(\sum_{i=1}^n X_i=n_1\)</span> (fixed group sample sizes).
This randomisation scheme is independent of the counterfactual outcomes and hence it implies full exchangeability.</p>
<p>Recall that the blood pressure reduction study is a randomised study: 20 subjects were randomised over the placebo and 2mg/day treatment. The conclusions that we have reached in our previous statistical analysis can thus be causally interpreted. The difference in sample means is thus an estimate of the ATE.</p>
<p>The reasoning that we have build in this section does not only apply to 2 treatment groups. It can easily be extended to more than 2 treatment groups or to the setting of linear regression.</p>
<p>Let us reconsider a few earlier examples:</p>
<ul>
<li><p>The full blood pressure reduction study, with the 4 treatment groups, that was previously analysed with a linear regression analysis also allows for causal conclusions, because all 40 students were randomised over the 4 treatment groups (doses). It is still a randomised study.</p></li>
<li><p>The study on the effect of smoking on the FEV consisted of a sample of 654 subjects, and for these subjects the smoking status (<span class="math inline">\(X_i\)</span>) and the FEV outcome (<span class="math inline">\(Y_i\)</span>) were observed. At best the sample may be a random sample from a larger population, but the <em>treatment</em> `smoking’ was not randomly assigned to the subjects. This would of course not be ethical! So this is an <strong>observational study</strong>, and the full or mean exchangeability condition is not satisfied. Our conclusion that the average FEV is larger among the smokers than among the non-smokers (suggesting that smoking is beneficial for the lung fuction) is thus not a causal conclusion. Modern causal inference methods provide other data analysis methods that could allow for causal conclusions in observational studies (outside the scope of this course).</p></li>
<li><p>The study designs of the Galton and the muscle mass example are also observational, and hence the conclusions for these studies are also only in terms of association rather than in terms of causation.</p></li>
</ul>
<p>To conclude we give an example that demonstrates how the full exchangeability condition can be violated, even in experimental studies (but without proper randomisation).</p>
</div>
</div>
<div id="S:Reg1BIPI" class="section level2" number="2.11">
<h2><span class="header-section-number">2.11</span> Estimation of the Conditional Mean Outcome</h2>
</div>
<div id="example-blood-pressure" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>Now that we have fitted the regression model for the blood pressure example, we could be interested in estimating the avere blood pressure reduction for patients that would receive 3mg/day of the medication. We are thus interested in estimating
<span class="math display">\[
  \E{Y \mid x} = m(x) = \beta_0 + \beta_1 x
\]</span>
with <span class="math inline">\(x=3\)</span> mg/day.</p>
<p>With the estimates of the regression parameters, we can estimate this conditional mean as
<span class="math display">\[
  \hat{m}(x) = \hat\beta_0 +\hat\beta_1 x.
\]</span>
With <span class="math inline">\(x=3\)</span> and with <span class="math inline">\(\hat\beta_0=0.033\)</span> and <span class="math inline">\(\hat\beta_1=1.79\)</span>, we thus estimate the average blood reduction for patients that take 3 mg/day as
<span class="math display">\[
  0.033 + 1.79 \times 3 = 5.4 \text{ mmHg}.
\]</span></p>
<p>Just as before, we need to realise that this estimate does not give any information on the (im)precision of this estimate. So there is again need for a standard error and a confidence interval. So again we need to know the sampling distribution of our estimator <span class="math inline">\(\hat{m}(x)\)</span>. Note that the estimator is a function of <span class="math inline">\(x\)</span>, and hence also the standard error and the sampling distribution of the estimator may depend on <span class="math inline">\(x\)</span>.</p>
<div id="sampling-distribution" class="section level3 unnumbered">
<h3>Sampling Distribution</h3>
<p>Here we give results for the normal linear regression model <a href="#eq:Mod4">(2.7)</a> for which we can have the exact sampling distributions of the parameter estimators, and hence also the exact sampling distribution of the estimator of the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span>.
</p>

<div class="theorem">
<span id="thm:DistrMHat" class="theorem"><strong>Theorem 2.9  (Sampling distribution of <span class="math inline">\(\hat{m}(x)\)</span> with normal error terms)  </strong></span> Assume that model <a href="#eq:Mod4">(2.7)</a> holds. Then, for a given <span class="math inline">\(x\)</span>,
<span class="math display">\[
   \hat{m}(x) \sim N\left(\beta_0 + \beta_1 x, \sigma_{m}^2(x)\right)
 \]</span>
with
<span class="math display">\[
    \sigma^2_{{m}}(x) = \begin{pmatrix} 1 &amp; x \end{pmatrix}
    \mb\Sigma_\beta \begin{pmatrix} 1\\ x \end{pmatrix} = \sigma_{\beta_0}^2 + x^2 \sigma_{\beta_1}^2 + 2x\cov{\hat\beta_0,\hat\beta_1}
 \]</span>
with (see Equation <a href="#eq:SigmaBetaLSE">(2.6)</a>)
<span class="math display">\[ 
   \mb\Sigma_\beta = \frac{\sigma^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix} .
 \]</span>
</div>

<div class="proof">
 <span class="proof"><em>Proof. </em></span>  We write
<span class="math display">\[
   \hat{m}(x) = \hat\beta_0+\hat\beta_1 x= \hat{\mb{\beta}}^t \mb{x}
 \]</span>
with
<span class="math display">\[
    \hat{\mb{\beta}}^t = (\hat\beta_0, \hat\beta_1) \;\;\text{ and }\;\; \mb{x}^t=(1, x).
 \]</span>
Theorem <a href="#thm:DistrMod4">2.4</a> now gives
<span class="math display">\[
     \hat{\mb{\beta}} \sim N({\mb{\beta}},\mb\Sigma_\beta) .
 \]</span>
Since <span class="math inline">\(\hat{m}(x)=\hat{\mb{\beta}}^t \mb{x}\)</span> is a linear combination of the elements of <span class="math inline">\(\hat{\mb{\beta}}\)</span>, the estimator <span class="math inline">\(\hat{m}(x)\)</span> is also normally distributed with mean
<span class="math display">\[
   \E{\hat{m}(x)}=\E{\hat{\mb{\beta}}^t \mb{x}} = \E{\hat{\mb{\beta}}}^t\mb{x}=\mb\beta^t\mb{x} = \beta_0+\beta_1x
 \]</span>
and variance
<span class="math display">\[
   \var{\hat{m}(x)} = \var{\hat{\mb{\beta}}^t \mb{x}} = \mb{x}^t \var{\hat{\mb{\beta}}}\mb{x}=\mb{x}^t \mb\Sigma_\beta\mb{x}=\sigma_m^2(x).
 \]</span>
</div>
<hr />
<p>Without the normality assumption (i.e. model <a href="#eq:Mod3">(2.2)</a>) we become the next theorem (without proof). Since the theorem expresses an asymptotic result, it would have been better to use notation that stresses the dependence on the sample size <span class="math inline">\(n\)</span>, but we ignore this for now. As before we will use asymptotic results for approximate inference for <em>large</em> samples.
</p>

<div class="theorem">
<span id="thm:DistrMHatAsym" class="theorem"><strong>Theorem 2.10  (Sampling distribution of <span class="math inline">\(\hat{m}(x)\)</span>)  </strong></span> Assume that model <a href="#eq:Mod3">(2.2)</a> holds. Then, for a given <span class="math inline">\(x\)</span>, and as <span class="math inline">\(n \rightarrow \infty\)</span>, it holds that
<span class="math display">\[
   \sqrt{n}\frac{\hat{m}(x)-m(x)}{\sigma_{m*}(x)} \convDistr N\left(0, 1\right)
 \]</span>
with
<span class="math display">\[
    \sigma^2_{m*}(x) = \begin{pmatrix} 1 &amp; x \end{pmatrix}
    \mb\Sigma_n
    \begin{pmatrix} 1\\ x \end{pmatrix}.
 \]</span>
and
<span class="math display">\[
   \mb\Sigma_n = \frac{\sigma^2}{\frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2} 
   \begin{pmatrix}
   \frac{1}{n}\sum_{i=1}^n x_i^2   &amp; - \bar{x} \\
   -\bar{x}                                      &amp; 1 \end{pmatrix} .
 \]</span>
Or (approximately for large <span class="math inline">\(n\)</span>)
<span class="math display">\[
   \hat{m}(x) \ApproxSim N(m(x),\sigma_m^2(x))
 \]</span>
with <span class="math inline">\(\sigma_m^2(x) = \sigma_{m*}^2(x)/n\)</span>.
</div>
</div>
<div id="confidence-interval-ci" class="section level3 unnumbered">
<h3>Confidence Interval (CI)</h3>
<p>The <span class="math inline">\(1-\alpha\)</span> confidence interval (CI) of the conditional mean <span class="math inline">\(m(x)=\E{Y\mid x}\)</span> (for a given <span class="math inline">\(x\)</span>) in model <a href="#eq:Mod4">(2.7)</a> follows from theorem <a href="#thm:DistrMHat">2.9</a>. First we write
<span class="math display">\[
  \frac{\hat{m}(x)-m(x)}{\sigma_m(x)} \sim N(0,1).
\]</span>
The only unknown in <span class="math inline">\(\sigma^2_m(x)\)</span> is the residual variance <span class="math inline">\(\sigma^2\)</span> that can be estimated by the MSE. Upon using MSE we obtain the estimator <span class="math inline">\(\hat\sigma_m^2(x)\)</span>.</p>
<p>Theorem <a href="#thm:DistrMSE">2.7</a> relates MSE to a chi-squared distribution with <span class="math inline">\(n-2\)</span> degrees of freedom, and hence (similar to Theorem <a href="#thm:DistrStudBeta">2.8</a>)
<span class="math display">\[
  \frac{\hat{m}(x)-m(x)}{\hat\sigma_m(x)} \sim t_{n-2}.
\]</span>
Form the identify
<span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} &lt; \frac{\hat{m}(x) - m(x)}{\hat\sigma_m(x)} &lt; t_{n-2;1-\alpha/2}} = 1-\alpha
\]</span>
it follows that
<span class="math display">\[
  \prob{\hat{m}(x)-t_{n-2;1-\alpha/2} \hat\sigma_m(x)&lt;  m(x) &lt; \hat{m}(x)+t_{n-2;1-\alpha/2} \hat\sigma_m(x)} = 1-\alpha
\]</span>
from which the <span class="math inline">\(1-\alpha\)</span> confindence interval of <span class="math inline">\(m(x)=\E{Y\mid x}\)</span> follows:
<span class="math display">\[
 \left[\hat{m}(x)-t_{n-2;1-\alpha/2} \hat\sigma_m(x), \hat{m}(x)+t_{n-2;1-\alpha/2} \hat\sigma_m(x)\right].
\]</span></p>
</div>
</div>
<div id="example-blood-pressure-1" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>The following R code gives the estimate of the average blood pressure reduction of patients that recieved a dose of 3 mg/day. We also give the <span class="math inline">\(95\%\)</span> confidence interval.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure)</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">dose=</span><span class="dv">3</span>))</span></code></pre></div>
<pre><code>##       1 
## 5.39207</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">dose=</span><span class="dv">3</span>),<span class="at">interval=</span><span class="st">&quot;confidence&quot;</span>,</span>
<span id="cb181-2"><a href="#cb181-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 5.39207 4.007517 6.776624</code></pre>
<p>So we estimate that patients receiving a daily dose of 3mg will show on average a reduction in their diastolic blood pressure of <span class="math inline">\(5.4\)</span> mmHg. The <span class="math inline">\(95\%\)</span> confidence interval goes from 4 to <span class="math inline">\(6.8\)</span> mmHg. In other words, with a probability of <span class="math inline">\(95\%\)</span> we expect that the diastolic blood pressure reduces on average with some value between 4 and <span class="math inline">\(6.8\)</span> mmHg.</p>
<p>The R code shown below illustrates how the lower and upper bounds of <span class="math inline">\(95\%\)</span> confindence intervals of the mean blood pressure reduction for patients can be calculated for a series of values for the daily dose. Based on these calculations we obtain Figure <a href="#fig:RegCI">2.19</a>. The plot shows that the lower and upper bounds of the confidence interval, as a function of <span class="math inline">\(x\)</span>, describe a hyperbole.</p>
<p>We may not overinterpret this graph: although the lower and upper bounds are visually presented as hyperboles, we can only interpret them in a <strong>pointwise</strong> fashion. Only for a given value of the regressor, the corresponding lower and upper bounds can be interpreted as a 95% CI of the conditional mean outcome. The hyperboles are <em>not</em> a confidence envelope that tells us that with a probability of 95% we may expect the true regression line to be in between the hyperbole lines.</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>xs<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="fl">0.2</span>)</span>
<span id="cb183-2"><a href="#cb183-2" aria-hidden="true" tabindex="-1"></a>m.CI<span class="ot">&lt;-</span><span class="fu">predict</span>(m,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">dose=</span>xs),</span>
<span id="cb183-3"><a href="#cb183-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">interval=</span><span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb183-4"><a href="#cb183-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure,<span class="at">xlab=</span><span class="st">&quot;dose (mg / day)&quot;</span>,</span>
<span id="cb183-5"><a href="#cb183-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb183-6"><a href="#cb183-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xs,m.CI[,<span class="dv">2</span>],<span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb183-7"><a href="#cb183-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xs,m.CI[,<span class="dv">3</span>],<span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb183-8"><a href="#cb183-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">coef</span>(<span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure)),<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegCI"></span>
<img src="DASM2_files/figure-html/RegCI-1.png" alt="Scatter plot of the blood pressure reduction data with the estimated regression line (red line). The graph also shows the under and upper bounds (blue lines) of the 95\% confidence intervals of the conditional means  $\E{Y\mid x}$ as a function of $x$ (dose)." width="672" />
<p class="caption">
Figure 2.19: Scatter plot of the blood pressure reduction data with the estimated regression line (red line). The graph also shows the under and upper bounds (blue lines) of the 95% confidence intervals of the conditional means <span class="math inline">\(\E{Y\mid x}\)</span> as a function of <span class="math inline">\(x\)</span> (dose).
</p>
</div>
</div>
<div id="S:PI" class="section level2" number="2.12">
<h2><span class="header-section-number">2.12</span> Predictions and Prediction Intervals (PI)</h2>
</div>
<div id="example-blood-pressure-2" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>From previous analyses we know that the fitted regression model is given by
<span class="math display">\[
  \hat{m}(x) = 0.033 - 1.79 x.
\]</span></p>
<p>We now want to use this fitted regression model to make a prediction of the diastolic blood pressure reduction for <em>a</em> patient that receives <span class="math inline">\(x=3\)</span> mg/day. So we are now concerned with the <strong>prediction of an individual outcome</strong> for a given value of the regressor.</p>
<p>Prediction only makes sense if the to-be-predicted outcome is not yet observed. This <em>future</em> outcome, conditional on some <span class="math inline">\(x\)</span>, is denoted as <span class="math inline">\(Y^*\)</span>. Since <span class="math inline">\(Y^*\)</span> is not yet observed, it is not part of the sample observations used for fitting the regression model.</p>
<p>We will denote the prediction as <span class="math inline">\(\hat{Y}\)</span> or <span class="math inline">\(\hat{Y}(x)\)</span>, or as <span class="math inline">\(\hat{y}\)</span> or <span class="math inline">\(\hat{y}(x)\)</span>.</p>
<p>So far we know that <span class="math inline">\(\hat{m}(x)\)</span> is an estimate of the mean outcome (blood pressure reduction) at regressor value <span class="math inline">\(x\)</span> (daily dose), i.e. <span class="math inline">\(\hat{m}(x)\)</span> is an estimate of a conditional mean.
I will now argue that this is also a good prediction for an individual outcome at regressor value <span class="math inline">\(x\)</span>:</p>
<ul>
<li><p>we know that <span class="math inline">\(\hat{m}(x)\)</span> is an estimate of the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span>, which is the point on the regression line at <span class="math inline">\(x\)</span>.</p></li>
<li><p>regression model <a href="#eq:Mod4">(2.7)</a> states that for a given <span class="math inline">\(x\)</span>, the outcomes <span class="math inline">\(Y\)</span> are normally distributed with mean <span class="math inline">\(\E{Y\mid x}\)</span></p></li>
<li><p>since this normal distribution is symmetric about its mean <span class="math inline">\(\E{Y\mid x}\)</span>, it is equally likely to observe an outcome smaller than <span class="math inline">\(\E{Y\mid x}\)</span> than it is to observe an outcome larger than <span class="math inline">\(\E{Y\mid x}\)</span></p></li>
<li><p>we do not have information that allows us to suspect that an individual (to-be-predicted) outcome is smaller or larger than the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span></p></li>
</ul>
<p>Because of these reasons, the point on the estimated regression line, i.e. the estimate <span class="math inline">\(\hat{m}(x)\)</span> of <span class="math inline">\(\E{Y\mid x}\)</span>, is our best prediction of an outcome at regressor value <span class="math inline">\(x\)</span>.</p>
<p>Back to the blood pressure example. So we predict that the diastolic blood pressure reduction of a patient that is on the 3mg/day regime is given by
<span class="math display">\[
  \hat{y}(3) = 0.033 + 1.79 \times 3 = 5.4 \text{ mmHg}.
\]</span></p>
<p>Note that <span class="math inline">\(\hat{y}(3)\)</span> is numerically identical to <span class="math inline">\(\hat{m}(3)\)</span>, but because of the difference in interpretation we use different notation.</p>
<p>The prediction is comparable to a point estimate in the sense that it is only a single numerical result, and that it does not contain any information about how well it succeeds in predicting the outcome. Later in this section we will construct prediction intervals that can serve this purpose, but first we dig a little deeper into the concept of prediction.</p>
<p>So we aim at predicting an individual outcome for a given value of <span class="math inline">\(x\)</span>. We will assume in this section that this outcome behaves according the same statistical model as the sample observations. Thus we assume that model <a href="#eq:Mod4">(2.7)</a> applies,
<span class="math display">\[
  Y^* = m(x) + \eps^* = \beta_0 + \beta_1 x + \eps^*
\]</span>
with <span class="math inline">\(\eps^* \sim N(0,\sigma^2)\)</span>.</p>
<p>Moreover, we will assume that this outcome is independent from the <span class="math inline">\(n\)</span> sample observations, i.e. <span class="math inline">\(Y^* \ind (Y_1,\ldots, Y_n)\)</span>, or, equivalently, <span class="math inline">\(\eps^* \ind (Y_1,\ldots, Y_n)\)</span>.
Hence, the parameter estimator <span class="math inline">\(\hat{\mb\beta}\)</span> is independent of <span class="math inline">\(Y^*\)</span> (or <span class="math inline">\(\eps^*\)</span>).</p>
<p>Now that we know the prediction <span class="math inline">\(\hat{Y}(x)\)</span> and the to-be-predicted outcome <span class="math inline">\(Y^*\)</span>, we can define a measure to quantify the qualilty of the prediction. The <strong>prediction error</strong> is given by
<span class="math display">\[
  \hat{Y}(x)-Y^*,
\]</span>
but this can never be observed, because, by definition, <span class="math inline">\(Y^*\)</span> is not observed (yet).
Moreover, since both <span class="math inline">\(\hat{Y}(x)\)</span> and <span class="math inline">\(Y^*\)</span> show sampling variability (i.e. they are both random variables), it makes more sense to investigate the quality of the predictions <em>on average</em> over many repeated experiments. In each repeated experiment, we both get new sample data <span class="math inline">\((X_1,Y_1), \ldots, (X_n,Y_n)\)</span>, as well as a new to-be-predicted outcome <span class="math inline">\(Y^*\)</span>. Here we define the <strong>expected conditional test error</strong> (sometimes also referred to as the <strong>mean squared error</strong>) as
<span class="math display">\[
   \text{Err}(x) = \Ef{\mb{Y} Y^*}{(\hat{Y}(x)-Y^*)^2} 
\]</span>
in which the expectation is with respect to the the distribution of the sample data and the to-be-predicted outcome. In a later chapter we will come back to this measure.</p>
<p>Both the mean squared error and prediction intervals are based on the <strong>sampling distribution of the prediction error</strong>, which, under model <a href="#eq:Mod4">(2.7)</a>, is given by
<span class="math display">\[
  \hat{Y}(x) - Y^* \sim N(0,\sigma^2_m(x)+\sigma^2).
\]</span>
This is a direct consequence of the sampling distribution of <span class="math inline">\(\hat{Y}(x)=\hat{m}(x)\)</span>, as given by Theorem <a href="#thm:DistrMHat">2.9</a>, and the independence between the sample observations and <span class="math inline">\(Y^*\)</span>. When the normality assumption does not hold, the result is approximately valid for large sample sizes.</p>
<p>The <strong>prediction interval</strong> (PI) is given in the next theorem.</p>

<div class="theorem">
<span id="thm:PIMod4" class="theorem"><strong>Theorem 2.11  (Prediciton interval for a given <span class="math inline">\(x\)</span> and with normal error terms)  </strong></span>Assume that model <a href="#eq:Mod4">(2.7)</a> holds. Then, for a given <span class="math inline">\(x\)</span> and a confidence level <span class="math inline">\(1-\alpha\)</span>,<br />
<span class="math display" id="eq:tmp23676268725">\[\begin{equation}
   \prob{\hat{Y}(x) - t_{n-2;1-\alpha/2}\sqrt{\hat\sigma_m^2(x)+\MSE} \leq Y^* \leq \hat{Y}(x) + t_{n-2;1-\alpha/2}\sqrt{\hat\sigma_m^2(x)+\MSE}} = 1-\alpha.
    \tag{2.11}
 \end{equation}\]</span>
</div>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> For the standardised prediction error we have
<span class="math display">\[
   \frac{\hat{Y}(x)-Y^*}{\sqrt{\sigma_m^2(x)+\sigma^2}}\sim N(0,1).
 \]</span>
From theorem <a href="#thm:DistrMHat">2.9</a> we know that <span class="math inline">\(\sigma_m^2(x)\)</span> is proportional to <span class="math inline">\(\sigma^2\)</span>. Hence, we can write <span class="math inline">\(\sigma_m^2(x)=\sigma_m^{\prime 2}(x)\sigma^2\)</span>, with <span class="math inline">\(\sigma_m^{\prime 2}(x)\)</span> a proportionality factor. The only unknown in <span class="math inline">\(\sigma^2_m(x)\)</span> is the residual variance <span class="math inline">\(\sigma^2\)</span> that can be estimated by MSE. Upon using MSE we become the estimator <span class="math inline">\(\hat\sigma_m^2(x)=\sigma_m^{\prime 2}(x) \MSE\)</span>. Hence, the variance <span class="math inline">\(\sigma_m^2(x)+\sigma^2\)</span> is estimated by <span class="math inline">\(MSE(\sigma_m^{\prime 2}(x)+1)\)</span>.
We further know that <span class="math inline">\((n-2)\MSE / \sigma^2 \sim \chi^2_{n-2}\)</span>. Hence,
<span class="math display">\[
  \frac{\hat{Y}(x)-Y^*}{\sqrt{MSE(\sigma_m^{\prime 2}(x)+1)}}\sim t_{n-2},
\]</span>
and thus
<span class="math display">\[
  \prob{-t_{n-2;1-\alpha/2} \leq \frac{\hat{Y}(x)-Y^*}{\sqrt{MSE(\sigma_m^\prime(x)^2+1)}} \leq t_{n-2;1-\alpha/2}}=1-\alpha.
\]</span>
Form this probability statement the prediction interval follows immediately.
</div>
<p>Note that the probability expression in <a href="#eq:tmp23676268725">(2.11)</a> has another structure than for confidence intervals. In the prediction interval there is a random variable (<span class="math inline">\(Y^*\)</span>) in between the two inequality signs, whereas in confidence intervals it is a population parameter.</p>
</div>
<div id="example-blood-pressure-3" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>The next R code calculates the predicted blood pressure reduction for a patient on the 3 mg/day regime, as well as a 95% prediction interval.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure)</span>
<span id="cb184-2"><a href="#cb184-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">dose=</span><span class="dv">3</span>),</span>
<span id="cb184-3"><a href="#cb184-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">interval=</span><span class="st">&quot;prediction&quot;</span>,<span class="at">level=</span><span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>##       fit       lwr      upr
## 1 5.39207 -3.033452 13.81759</code></pre>
<p>Conclusion: we predict that a patient with a daily dose of 3mg will show a reduction in diastolic blood pressure of <span class="math inline">\(5.4\)</span> mmHg after a period of 2 months. With a probability of 95% we expect for a patient on the 3 mg/day regime for a period of 2 months that his/her diastolic blood pressure will reduce with up to 13.8 mmHg or increase with up to 3 mmHg.</p>
<p>The next chunck of R code makes a graph with lower and upper limits of 95% prediction intervals of the blood pressure reduction for a sequence of daily doses; see Figure <a href="#fig:RegPI">2.20</a>. The under and upper bounds describe a hyperbole. Note that these bounds are furhter away from the fitted regression line than the bounds of confidence intervals (for the same confidence level <span class="math inline">\(1-\alpha\)</span>). Again only a pointwise interpretation is correct.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a>xs<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)</span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a>m.PI<span class="ot">&lt;-</span><span class="fu">predict</span>(m,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">dose=</span>xs),<span class="at">interval=</span><span class="st">&quot;prediction&quot;</span>)</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure,<span class="at">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>,</span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xs,m.PI[,<span class="dv">2</span>],<span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(xs,m.PI[,<span class="dv">3</span>],<span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">coef</span>(<span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose,<span class="at">data=</span>BloodPressure)),<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegPI"></span>
<img src="DASM2_files/figure-html/RegPI-1.png" alt="Scatter plot of the blood pressure data and the fitted regression line (red line). The graph shows the lower and upper bounst of 95\% prediction intervals as a function of the dose." width="672" />
<p class="caption">
Figure 2.20: Scatter plot of the blood pressure data and the fitted regression line (red line). The graph shows the lower and upper bounst of 95% prediction intervals as a function of the dose.
</p>
</div>
<div id="simulation-study" class="section level3" number="2.12.1">
<h3><span class="header-section-number">2.12.1</span> Simulation Study</h3>
<p>To better understand the difference between confidence intervals and prediction intervals, their interpretations are here illustrated with a simulation study.</p>
<p>The following R code simulates repeated sampling from a linear regression model. For each sample, <span class="math inline">\(\hat{m}(x)=\hat{y}(x)\)</span> is computed for <span class="math inline">\(x=3\)</span>, together with its 95% confidence interal and prediction interval.
For each repeated experiment we check whether the true conditional mean <span class="math inline">\(\E{Y\mid x}\)</span> is within the CI and whether a new independently sampled outcome <span class="math inline">\(Y^*\)</span> is within the PI.</p>
<p>The settings in this simulation study are inspired by the blood reduction example.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="#cb187-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">126778</span>)</span>
<span id="cb187-2"><a href="#cb187-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 <span class="co"># number of repeated samples</span></span>
<span id="cb187-3"><a href="#cb187-3" aria-hidden="true" tabindex="-1"></a>dose<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">10</span>) <span class="co"># 4 doses</span></span>
<span id="cb187-4"><a href="#cb187-4" aria-hidden="true" tabindex="-1"></a>dose<span class="ot">&lt;-</span><span class="fu">rep</span>(dose,<span class="dv">10</span>) <span class="co"># 10 replicates for each dose</span></span>
<span id="cb187-5"><a href="#cb187-5" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="dv">3</span> <span class="co"># dose for which the conditional mean must be estimated and an outcome must be predicted</span></span>
<span id="cb187-6"><a href="#cb187-6" aria-hidden="true" tabindex="-1"></a>mx<span class="ot">&lt;-</span><span class="fl">0.03+1.79</span><span class="sc">*</span>x <span class="co"># true conditional mean </span></span>
<span id="cb187-7"><a href="#cb187-7" aria-hidden="true" tabindex="-1"></a>m.hat<span class="ot">&lt;-</span><span class="fu">c</span>() </span>
<span id="cb187-8"><a href="#cb187-8" aria-hidden="true" tabindex="-1"></a>cnt.CI<span class="ot">&lt;-</span><span class="dv">0</span>  </span>
<span id="cb187-9"><a href="#cb187-9" aria-hidden="true" tabindex="-1"></a>cnt.PI<span class="ot">&lt;-</span><span class="dv">0</span>  </span>
<span id="cb187-10"><a href="#cb187-10" aria-hidden="true" tabindex="-1"></a>CI.all<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N,<span class="at">ncol=</span><span class="dv">2</span>) </span>
<span id="cb187-11"><a href="#cb187-11" aria-hidden="true" tabindex="-1"></a>PI.all<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N,<span class="at">ncol=</span><span class="dv">2</span>) </span>
<span id="cb187-12"><a href="#cb187-12" aria-hidden="true" tabindex="-1"></a>YStars<span class="ot">&lt;-</span><span class="fu">c</span>() </span>
<span id="cb187-13"><a href="#cb187-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb187-14"><a href="#cb187-14" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span><span class="fl">0.03+1.79</span><span class="sc">*</span>dose<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">40</span>,<span class="at">sd=</span><span class="dv">4</span>) <span class="co"># random sample van uitkomsten</span></span>
<span id="cb187-15"><a href="#cb187-15" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>dose) <span class="co"># model fit</span></span>
<span id="cb187-16"><a href="#cb187-16" aria-hidden="true" tabindex="-1"></a>    CI<span class="ot">&lt;-</span><span class="fu">predict</span>(m,<span class="fu">data.frame</span>(<span class="at">dose=</span>x),<span class="at">interval=</span><span class="st">&quot;confidence&quot;</span>)</span>
<span id="cb187-17"><a href="#cb187-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>((mx<span class="sc">&gt;</span>CI[<span class="dv">2</span>])<span class="sc">&amp;</span>(mx<span class="sc">&lt;</span>CI[<span class="dv">3</span>])) cnt.CI<span class="ot">&lt;-</span>cnt.CI<span class="sc">+</span><span class="dv">1</span> <span class="co"># if mx is within CI then counter is increased with one</span></span>
<span id="cb187-18"><a href="#cb187-18" aria-hidden="true" tabindex="-1"></a>    yStar<span class="ot">&lt;-</span><span class="fl">0.03+1.79</span><span class="sc">*</span>x<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">1</span>,<span class="at">sd=</span><span class="dv">4</span>) <span class="co"># a new outcome (to be predicted)</span></span>
<span id="cb187-19"><a href="#cb187-19" aria-hidden="true" tabindex="-1"></a>    PI<span class="ot">&lt;-</span><span class="fu">predict</span>(m,<span class="fu">data.frame</span>(<span class="at">dose=</span>x),<span class="at">interval=</span><span class="st">&quot;predict&quot;</span>)</span>
<span id="cb187-20"><a href="#cb187-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span>((yStar<span class="sc">&gt;</span>PI[<span class="dv">2</span>])<span class="sc">&amp;</span>(yStar<span class="sc">&lt;</span>PI[<span class="dv">3</span>])) cnt.PI<span class="ot">&lt;-</span>cnt.PI<span class="sc">+</span><span class="dv">1</span> <span class="co"># if yStar is within PI then counter is increased with one</span></span>
<span id="cb187-21"><a href="#cb187-21" aria-hidden="true" tabindex="-1"></a>    CI.all[i,]<span class="ot">&lt;-</span>CI[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb187-22"><a href="#cb187-22" aria-hidden="true" tabindex="-1"></a>    PI.all[i,]<span class="ot">&lt;-</span>PI[<span class="dv">2</span><span class="sc">:</span><span class="dv">3</span>]</span>
<span id="cb187-23"><a href="#cb187-23" aria-hidden="true" tabindex="-1"></a>    YStars<span class="ot">&lt;-</span><span class="fu">c</span>(YStars,yStar)</span>
<span id="cb187-24"><a href="#cb187-24" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb187-25"><a href="#cb187-25" aria-hidden="true" tabindex="-1"></a>CI.coverage<span class="ot">&lt;-</span>cnt.CI<span class="sc">/</span>N</span>
<span id="cb187-26"><a href="#cb187-26" aria-hidden="true" tabindex="-1"></a>PI.coverage<span class="ot">&lt;-</span>cnt.PI<span class="sc">/</span>N</span>
<span id="cb187-27"><a href="#cb187-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb187-28"><a href="#cb187-28" aria-hidden="true" tabindex="-1"></a>CI.coverage</span></code></pre></div>
<pre><code>## [1] 0.95</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="#cb189-1" aria-hidden="true" tabindex="-1"></a>PI.coverage</span></code></pre></div>
<pre><code>## [1] 0.9493</code></pre>
<p>Based on <span class="math inline">\(N=\)</span> 10^{4} repeated experimetns, we find an empirical coverage of 0.95 for the CI and 0.9493 for the PI. Both are very close to the nominal lavel of <span class="math inline">\(95\%\)</span>.</p>
<p>Figure <a href="#fig:BIPIReg">2.21</a> visualises the results of the first 50 repeated samples.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="#cb191-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb191-2"><a href="#cb191-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">11</span>,<span class="dv">21</span>),<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">50</span>),<span class="at">xlab=</span><span class="st">&quot;repeated experiment&quot;</span>,</span>
<span id="cb191-3"><a href="#cb191-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb191-4"><a href="#cb191-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb191-5"><a href="#cb191-5" aria-hidden="true" tabindex="-1"></a>    colour<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">-</span><span class="fu">as.numeric</span>((mx<span class="sc">&gt;</span>CI.all[i,<span class="dv">1</span>])<span class="sc">&amp;</span>(mx<span class="sc">&lt;</span>CI.all[i,<span class="dv">2</span>]))</span>
<span id="cb191-6"><a href="#cb191-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">c</span>(i,i),<span class="fu">c</span>(CI.all[i,<span class="dv">1</span>],CI.all[i,<span class="dv">2</span>]),<span class="at">col=</span>colour)</span>
<span id="cb191-7"><a href="#cb191-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb191-8"><a href="#cb191-8" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(YStars[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>],<span class="at">col=</span><span class="dv">4</span>,<span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb191-9"><a href="#cb191-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>mx,<span class="at">col=</span><span class="dv">3</span>)</span>
<span id="cb191-10"><a href="#cb191-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-11"><a href="#cb191-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">11</span>,<span class="dv">21</span>),<span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">50</span>),<span class="at">xlab=</span><span class="st">&quot;repeated experiment&quot;</span>,</span>
<span id="cb191-12"><a href="#cb191-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;diastolic blood pressure reduction (mmHg)&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb191-13"><a href="#cb191-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {</span>
<span id="cb191-14"><a href="#cb191-14" aria-hidden="true" tabindex="-1"></a>    colour<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">-</span><span class="fu">as.numeric</span>((YStars[i]<span class="sc">&gt;</span>PI.all[i,<span class="dv">1</span>])<span class="sc">&amp;</span>(YStars[i]<span class="sc">&lt;</span>PI.all[i,<span class="dv">2</span>]))</span>
<span id="cb191-15"><a href="#cb191-15" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lines</span>(<span class="fu">c</span>(i,i),<span class="fu">c</span>(PI.all[i,<span class="dv">1</span>],PI.all[i,<span class="dv">2</span>]),<span class="at">col=</span>colour)</span>
<span id="cb191-16"><a href="#cb191-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb191-17"><a href="#cb191-17" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(YStars[<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>],<span class="at">col=</span><span class="dv">4</span>,<span class="at">cex=</span><span class="fl">0.5</span>)</span>
<span id="cb191-18"><a href="#cb191-18" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span>mx,<span class="at">col=</span><span class="dv">3</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:BIPIReg"></span>
<img src="DASM2_files/figure-html/BIPIReg-1.png" alt="95\% confidencen intervals (left) en prediction intervals (right) of 50 repeated experiments. Black lines cover the true conditional mean (left) or the new to-be-predicted outcome (right), the red lines do not. The green horizontal reference line corresponds to $m(3)=0.03+1.79*3=5.4$. The blue points are the to-be-predicted outcomes." width="672" />
<p class="caption">
Figure 2.21: 95% confidencen intervals (left) en prediction intervals (right) of 50 repeated experiments. Black lines cover the true conditional mean (left) or the new to-be-predicted outcome (right), the red lines do not. The green horizontal reference line corresponds to <span class="math inline">\(m(3)=0.03+1.79*3=5.4\)</span>. The blue points are the to-be-predicted outcomes.
</p>
</div>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
</div>
</div>
<div id="exercise-galtons-data" class="section level2 unnumbered">
<h2>Exercise: Galton’s data</h2>
<p>Consider again the Galton’s height example. Give predictions of a son’s heights with a father of 160 cm and for a son’s height with a father of 175 cm, as wel the corresponding 90% prediction intervals.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm, <span class="at">data=</span>Galton.sons)</span>
<span id="cb193-2"><a href="#cb193-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">father.cm=</span><span class="fu">c</span>(<span class="dv">160</span>,<span class="dv">175</span>)),</span>
<span id="cb193-3"><a href="#cb193-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>,</span>
<span id="cb193-4"><a href="#cb193-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">level =</span> <span class="fl">0.90</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 171.0432 161.4991 180.5872
## 2 178.6580 169.2686 188.0474</code></pre>
<p>We predict the heigths of a son of a father of 160 cm and 175 cm as 171 cm and 179 cm, respectively.</p>
<p>With a probability of 90% we excpect that height of a son of a father of 160 cm to be in between 162 and 181 cm.</p>
With a probability of 90% we excpect that height of a son of a father of 175 cm to be in between 169 and 188 cm.
</details>
</div>
<div id="decomposition-of-the-total-sum-of-squares" class="section level2" number="2.13">
<h2><span class="header-section-number">2.13</span> Decomposition of the Total Sum of Squares</h2>
<p>In this section we discuss the construction of <strong>sum of squares</strong>. The convention is to present these sum of squares in a table which is known as the <strong>analysis of variance table</strong> or the <strong>anova table</strong>. <em>Anova</em> stands for <strong>analysis of variance</strong>.</p>

<div class="definition">
<span id="def:unnamed-chunk-59" class="definition"><strong>Definition 2.2  (Total sum of squares)  </strong></span> The total sum of squares is given by
<span class="math display">\[
   \SSTot = \sum_{i=1}^n (Y_i-\bar{Y})^2 .
 \]</span>
</div>
<p>SSTot measures the total variabilty of the outcome in the sample. The statistic
<span class="math display">\[
  \frac{\SSTot}{n-1} 
\]</span>
is the sample variance of the <strong>marginal distribution</strong> of the outcome.
The marginal distribution of <span class="math inline">\(Y\)</span> has mean <span class="math inline">\(\E{Y}\)</span>, which is estimated by the sample mean <span class="math inline">\(\bar{Y}\)</span>. The statistic <span class="math inline">\(\frac{\SSTot}{n-1}\)</span> is thus an estimator of the variance of the marginal distribution.</p>
<p>In this chapter the focus is mainly on the conditional distribution of the outcome <span class="math inline">\(Y\)</span> given the regressor <span class="math inline">\(x\)</span>, and particularly on the conditional mean <span class="math inline">\(\E{Y\mid x}\)</span>.
We already know that MSE is an estimator of the variance of <span class="math inline">\(Y\)</span> given <span class="math inline">\(\mb{x}\)</span> (i.e. the variance of the error term <span class="math inline">\(\eps\)</span>).</p>
<p>Figure <a href="#fig:RegMod3Marginal">2.22</a> gives an illustration based on two conditional distributions and the marginal distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:RegMod3Marginal"></span>
<img src="DASM2_files/figure-html/RegMod3Marginal-1.png" alt="Illustration of the normal linear regression model. The black line represents the regression line $m(x;\beta)$. The red points are the outcomes from the conditional distribution of $Y$ given $x=2$ and the blue plots come from the conditional distribution of $Y$ given $x=5$. The red and the bleu lines show the corresponding density functions of these conditional distributions. The black curve at the right side of the graph represents the density function of the marginal distribution of $Y$." width="672" />
<p class="caption">
Figure 2.22: Illustration of the normal linear regression model. The black line represents the regression line <span class="math inline">\(m(x;\beta)\)</span>. The red points are the outcomes from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=2\)</span> and the blue plots come from the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x=5\)</span>. The red and the bleu lines show the corresponding density functions of these conditional distributions. The black curve at the right side of the graph represents the density function of the marginal distribution of <span class="math inline">\(Y\)</span>.
</p>
</div>

<div class="definition">
<span id="def:unnamed-chunk-60" class="definition"><strong>Definition 2.3  (Sum of squares of the regression)  </strong></span>The sum of squares of the regression is defined as
<span class="math display">\[
  \SSR = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2 = \sum_{i=1}^n (\hat{m}(x_i) - \bar{Y})^2.
\]</span>
</div>
<p>SSR is a measure for the deviation between the fitted regression line (<span class="math inline">\(\hat{Y}_i = \hat{m}(x_i)=\hat\beta_0+\hat\beta_1 x_i\)</span>) and a regression line with intercept only. Note that for the latter regression model (<span class="math inline">\(\E{Y \mid x}=\beta_0\)</span>) the intercept is estimated as <span class="math inline">\(\hat\beta_0=\bar{Y}\)</span> and hence the fitted regression line is a flat line at <span class="math inline">\(\hat{m}(x)=\bar{Y}\)</span>.
In other words, SSE measures the size of the regressor effect in the sense that <span class="math inline">\(\SSR \approx 0\)</span> indicates that the regressor has hardly an effect on the conditional outcome mean, and <span class="math inline">\(\SSR &gt;0\)</span> indicates an effect of the regressor.</p>
<p>Finally, we repeat the definition of the sum of squared errors (SEE) as given in Equation <a href="#eq:SSEReg1">(2.3)</a>:
<span class="math display">\[
  \SSE = \sum_{i=1}^n (\hat{Y}_i -Y_i)^2 = \sum_{i=1}^n (\hat{m}(x_i) -Y_i)^2.
\]</span>
We know already that SSE is a measure for the deviation between the sample outcomes and the predictions (points on the fitted regression model) at the observed regressor values <span class="math inline">\(x_i\)</span>. The smaller SSE, the better the fit of the regression line.
</p>

<div class="theorem">
<span id="thm:unnamed-chunk-61" class="theorem"><strong>Theorem 2.12  (Decomposition of the Total sum of Squares)  </strong></span> <span class="math display">\[
   \SSTot = \SSR + \SSE
 \]</span>
</div>

<div class="proof">
 <span class="proof"><em>Proof. </em></span> <span class="math display">\[\begin{eqnarray*}
  \SSTot 
    &amp;=&amp;  \sum_{i=1}^n (Y_i-\bar{Y})^2 \\
    &amp;=&amp;  \sum_{i=1}^n (Y_i-\hat{Y}_i+\hat{Y}_i-\bar{Y})^2 \\
    &amp;=&amp;  \sum_{i=1}^n (Y_i-\hat{Y}_i)^2+\sum_{i=1}^n(\hat{Y}_i-\bar{Y})^2  + 2 \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat{Y}_i-\bar{Y})\\
    &amp;=&amp;  \SSE+\SSR  + 2 \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat{Y}_i-\bar{Y}).
  \end{eqnarray*}\]</span>
Upon using <span class="math inline">\(\hat{Y}_i=\hat\beta_0+\hat\beta_1x_i\)</span> and the parameter estimates as given in Theorem <a href="#thm:LSEReg1">2.1</a>, we rewrite the last term as
<span class="math display">\[\begin{eqnarray*}
    \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat{Y}_i-\bar{Y}) 
       &amp;=&amp; \sum_{i=1}^n (Y_i-\hat{Y})(\hat\beta_0+\hat\beta_1 x_i - \bar{Y}) \\
       &amp;=&amp; \sum_{i=1}^n (Y_i-\hat{Y}_i)(\bar{Y}-\hat\beta_1\bar{x}+\hat\beta_1x_i-\bar{Y}) \\
       &amp;=&amp; \sum_{i=1}^n (Y_i-\hat{Y}_i)(\hat\beta_1(x_i-\bar{x})) \\
       &amp;=&amp; \hat\beta_1\sum_{i=1}^n (Y_i-\hat{Y}_i)(x_i-\bar{x}) \\
       &amp;=&amp; \hat\beta_1\sum_{i=1}^n (Y_i-\hat{Y}_i)x_i -\hat\beta_1\bar{x}\sum_{i=1}^n (Y_i-\hat{Y}_i) .
  \end{eqnarray*}\]</span>
The first term is eqal to zero because the parameter estimates in <span class="math inline">\(\hat{Y}_i\)</span> are the solutions of the estimating equations <a href="#eq:LSEEE1">(2.5)</a>. Similarly, the second term is also equal to zero (estimating equation <a href="#eq:LSEEE0">(2.4)</a>).
</div>
<p>The meaning of the decomposition of SSTot is the following. The total variability in the outcome data (SSTot) is partly explained by the regression relationship (SSR). The variability that is not explained by the fitted regression model, is the residual variability (SSE).</p>

<div class="definition">
<span id="def:unnamed-chunk-63" class="definition"><strong>Definition 2.4  (Coefficient of Determination)  </strong></span>The coefficient of determination is defined as
<span class="math display">\[
   R^2 = 1-\frac{\SSE}{\SSTot}.
 \]</span>
</div>
<p>The coefficient of determination can also be written as
<span class="math display">\[
   R^2 = 1-\frac{\SSE}{\SSTot}=1-\frac{\SSTot-\SSR}{\SSTot}=\frac{\SSR}{\SSTot}.
\]</span>
So it is the fraction of the total variability in the sample outcomes that is explained by the fitted regression line.
We can also say that it is the proportion of the total variability in the observed outcomes that is predictable by the fitted regression line.</p>
</div>
<div id="example-galtons-height" class="section level2 unnumbered">
<h2>Example (Galton’s height)</h2>
<p>We look again at the output of the analysis of Galton’s data.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(son.cm<span class="sc">~</span>father.cm,<span class="at">data=</span>Galton.sons)</span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = son.cm ~ father.cm, data = Galton.sons)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.9406  -3.5300   0.2605   3.4064  20.5805 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 89.81819   11.73609   7.653 1.37e-12 ***
## father.cm    0.50766    0.06683   7.596 1.91e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.661 on 171 degrees of freedom
## Multiple R-squared:  0.2523, Adjusted R-squared:  0.2479 
## F-statistic:  57.7 on 1 and 171 DF,  p-value: 1.907e-12</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: son.cm
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## father.cm   1 1849.1 1849.13  57.701 1.907e-12 ***
## Residuals 171 5480.0   32.05                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>In the output we read <span class="math inline">\(R^2=0.2523\)</span>. Hence, only 25% of the variability in the observed heights of sons can be explained by the linear regression relationship with their father’s height.</p>
<p>The anova table gives <span class="math inline">\(\SSR=1849.1\)</span> and <span class="math inline">\(\SSE=5480\)</span>. Hence, <span class="math inline">\(\SSTot=\SSR+\SSE=7329.1\)</span>. With these numbers we also obtain <span class="math inline">\(R^2=\SSR / \SSTot=0.2523\)</span>.</p>
<p>This data analysis is also an example where the <span class="math inline">\(R^2\)</span> is small, but the effect of the regressor on the mean outcome is highly significant (<span class="math inline">\(p&lt;0.001\)</span>). This may be explained by the sample size: the <span class="math inline">\(p\)</span>-value is sensitive to the sample size; the <span class="math inline">\(p\)</span>-value measures evidence in the data against the null in favor of the alternative hypothesis, and with increasing sample size the evidence can accumulate. The coefficient of determination, however, is not sensitive to the sample size.</p>
<p>And, finally, we may not forget that the <span class="math inline">\(p\)</span>-value refers to hypotheses in terms of a <span class="math inline">\(\beta\)</span>-parameter that has an interpretation related to the (conditional) <em>mean</em> outcome, whereas prediction refer to individual outcomes.</p>
</div>
<div id="exercise-r2-and-prediction" class="section level2 unnumbered">
<h2>Exercise: <span class="math inline">\(R^2\)</span> and prediction</h2>
<p>A large <span class="math inline">\(R^2\)</span> is sometimes considered as an indication that the model can potentially be used as a good prediction model. However, this is generally not true.</p>
<p>You are asked to empirically demonstrate this statement. Simulate two datasets, both with the same residual variance <span class="math inline">\(\sigma^2\)</span>, but one with a small non-zero value for <span class="math inline">\(\beta_1\)</span> and another with a large value for <span class="math inline">\(\beta_1\)</span>. Discuss your findings.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2627</span>)</span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>  <span class="co"># regressor values</span></span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>sigma<span class="ot">&lt;-</span><span class="dv">1</span> <span class="co"># residual standard deviation</span></span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a>beta0<span class="ot">&lt;-</span><span class="dv">1</span> <span class="co"># intercept parameter (the same for the two simulated data sets)</span></span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a>eps<span class="ot">&lt;-</span><span class="fu">rnorm</span>(<span class="dv">11</span>,<span class="at">sd=</span>sigma) <span class="co"># simulation of the error terms (these will be used for the two simulated datasets)</span></span>
<span id="cb199-6"><a href="#cb199-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-7"><a href="#cb199-7" aria-hidden="true" tabindex="-1"></a><span class="co"># simulation of first data set</span></span>
<span id="cb199-8"><a href="#cb199-8" aria-hidden="true" tabindex="-1"></a>beta1<span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb199-9"><a href="#cb199-9" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x<span class="sc">+</span>eps</span>
<span id="cb199-10"><a href="#cb199-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,</span>
<span id="cb199-11"><a href="#cb199-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>, <span class="at">cex.axis=</span><span class="fl">1.5</span>)</span>
<span id="cb199-12"><a href="#cb199-12" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb199-13"><a href="#cb199-13" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">coef</span>(m),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-65-1.png" width="672" /></p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.44430 -0.75415  0.04751  0.71378  1.26962 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.22797    0.58101   0.392    0.704    
## x            1.15065    0.09821  11.716 9.44e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.03 on 9 degrees of freedom
## Multiple R-squared:  0.9385, Adjusted R-squared:  0.9316 
## F-statistic: 137.3 on 1 and 9 DF,  p-value: 9.438e-07</code></pre>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## x          1 145.640 145.640  137.27 9.438e-07 ***
## Residuals  9   9.549   1.061                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulation of first data set</span></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a>beta1<span class="ot">&lt;-</span><span class="fl">0.2</span></span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x<span class="sc">+</span>eps</span>
<span id="cb204-4"><a href="#cb204-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y,</span>
<span id="cb204-5"><a href="#cb204-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>, <span class="at">cex.axis=</span><span class="fl">1.5</span>)</span>
<span id="cb204-6"><a href="#cb204-6" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb204-7"><a href="#cb204-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">coef</span>(m),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.44430 -0.75415  0.04751  0.71378  1.26962 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  0.22797    0.58101   0.392  0.70392   
## x            0.35065    0.09821   3.570  0.00602 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.03 on 9 degrees of freedom
## Multiple R-squared:  0.5862, Adjusted R-squared:  0.5402 
## F-statistic: 12.75 on 1 and 9 DF,  p-value: 0.00602</code></pre>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##           Df  Sum Sq Mean Sq F value  Pr(&gt;F)   
## x          1 13.5252  13.525  12.748 0.00602 **
## Residuals  9  9.5486   1.061                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>These two analyses illustrate that <span class="math inline">\(R^2\)</span> is not fully informative about the predictability of outcomes. Both settings give exactly the same MSE (<span class="math inline">\(\MSE=1.061\)</span>), because I used twice the same vector of simulated error terms. The MSE estimates the variability about the true regression line and it is the variability of to-be-predicted outcomes about this regression line.</p>
<p>The <span class="math inline">\(R^2\)</span> is larger for the larger value of <span class="math inline">\(\beta_1\)</span>, because the larger <span class="math inline">\(\beta_1\)</span> causes the range of sample outcomes to be larger, which in terms results in a larger SSTot. Thus, <span class="math inline">\(R^2=1-\SSE / \SSTot\)</span> is larger for the setting with the larger <span class="math inline">\(\beta_1\)</span> (larger SSTot).</p>
</details>
<hr />

</div>
</div>
<div id="Ch:Reg2" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Multiple Linear Regression Analysis</h1>
<div id="example-lead-concentration" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>In the 1970s several studies focused on the environmental and health risks of lead smelters. The dataset considered here involves children who lived near a lead smelter in El Paso, Texas, USA. Children who were exposed to the lead smelter were included in this study, as well as a control group of children. outcomes of interest are</p>
<ul>
<li><p>finger-wrist tapping score (variable name: <em>MAXFWT</em>). It is the number of finger taps in a period of 10 second. It is considered a proxy for the integrity of the neuromuscular system. The higher the score the better. It can be measured for the left and the right hand. We will use the maximum of the two as outcome variable.</p></li>
<li><p>full-scale IQ score (variable name: <em>Iqf</em>). The higher the score the better.</p></li>
</ul>
<p>One of the research questions was how the MAXWT is related to the lead concentrations in the blood (<em>Ld72</em> and <em>Ld73</em> for measurements in 1972 and 1973 ) and to the total number of years the child lived in the proxy of the lead smelter (<em>Totyrs</em>). However, we should perhaps account for other variables in the dataset, such as the age (<em>Age</em>) and the gender (<em>Sex</em>) of the children.</p>
<p>This dataset was downloaded from <a href="https://www2.stat.duke.edu/courses/Fall00/sta102/HW/lab1.html" class="uri">https://www2.stat.duke.edu/courses/Fall00/sta102/HW/lab1.html</a>.</p>
<p>Before we introduce the multiple linear regression model, we will start with some data exploration of the dataset.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/lead.RData&quot;</span>)</span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a><span class="fu">skim</span>(lead)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-67">Table 3.1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">lead</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">102</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">11</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">11</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="10%" />
<col width="7%" />
<col width="10%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="32%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Id</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">205.30</td>
<td align="right">118.57</td>
<td align="right">101.00</td>
<td align="right">126.25</td>
<td align="right">151.50</td>
<td align="right">213.75</td>
<td align="right">505.00</td>
<td align="left">&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2582&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="even">
<td align="left">Area</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.64</td>
<td align="right">0.73</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="right">3.00</td>
<td align="left">&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2586&gt;&lt;U+2581&gt;&lt;U+2582&gt;</td>
</tr>
<tr class="odd">
<td align="left">Age</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">9.05</td>
<td align="right">3.52</td>
<td align="right">3.75</td>
<td align="right">6.38</td>
<td align="right">8.67</td>
<td align="right">12.06</td>
<td align="right">15.92</td>
<td align="left">&lt;U+2587&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2586&gt;&lt;U+2585&gt;</td>
</tr>
<tr class="even">
<td align="left">Sex</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.38</td>
<td align="right">0.49</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="right">2.00</td>
<td align="left">&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2585&gt;</td>
</tr>
<tr class="odd">
<td align="left">Iqf</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">91.91</td>
<td align="right">14.36</td>
<td align="right">50.00</td>
<td align="right">82.25</td>
<td align="right">91.50</td>
<td align="right">100.75</td>
<td align="right">141.00</td>
<td align="left">&lt;U+2581&gt;&lt;U+2586&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="even">
<td align="left">Lead_type</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">1.24</td>
<td align="right">0.43</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">1.00</td>
<td align="right">2.00</td>
<td align="left">&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;</td>
</tr>
<tr class="odd">
<td align="left">Ld72</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">34.17</td>
<td align="right">17.28</td>
<td align="right">1.00</td>
<td align="right">24.00</td>
<td align="right">31.00</td>
<td align="right">38.00</td>
<td align="right">99.00</td>
<td align="left">&lt;U+2582&gt;&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2581&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="even">
<td align="left">Ld73</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">31.58</td>
<td align="right">10.77</td>
<td align="right">15.00</td>
<td align="right">24.00</td>
<td align="right">28.00</td>
<td align="right">38.00</td>
<td align="right">58.00</td>
<td align="left">&lt;U+2585&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2582&gt;</td>
</tr>
<tr class="odd">
<td align="left">Totyrs</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">6.73</td>
<td align="right">3.37</td>
<td align="right">1.00</td>
<td align="right">4.00</td>
<td align="right">6.00</td>
<td align="right">9.00</td>
<td align="right">15.00</td>
<td align="left">&lt;U+2583&gt;&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2583&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="even">
<td align="left">MAXFWT</td>
<td align="right">19</td>
<td align="right">0.81</td>
<td align="right">52.05</td>
<td align="right">12.90</td>
<td align="right">13.00</td>
<td align="right">47.00</td>
<td align="right">52.00</td>
<td align="right">59.00</td>
<td align="right">84.00</td>
<td align="left">&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2587&gt;&lt;U+2585&gt;&lt;U+2581&gt;</td>
</tr>
<tr class="odd">
<td align="left">Exposed</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">0.24</td>
<td align="right">0.43</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">0.00</td>
<td align="right">1.00</td>
<td align="left">&lt;U+2587&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2581&gt;&lt;U+2582&gt;</td>
</tr>
</tbody>
</table>
<p>We look at scatter plots to explore the relation between the FWT and the blood lead concentrations (in 1973) and the total number of years the children lived in the neighborhood of the lead smelter. For these two settings, we will fit simple linear regression models and plot the fitted regression lines.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>m.Ld73<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73,<span class="at">data=</span>lead)</span>
<span id="cb210-2"><a href="#cb210-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.Ld73)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.308  -5.808   0.993   7.584  30.661 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  64.3660     4.0020  16.083   &lt;2e-16 ***
## Ld73         -0.3938     0.1206  -3.266   0.0016 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.2 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1164, Adjusted R-squared:  0.1055 
## F-statistic: 10.67 on 1 and 81 DF,  p-value: 0.0016</code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m.Ld73)</span></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) 56.4032516 72.3287437
## Ld73        -0.6337485 -0.1539078</code></pre>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a>m.Totyrs<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Totyrs,<span class="at">data=</span>lead)</span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.Totyrs)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -37.341  -5.070  -0.584   7.552  29.902 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  42.8836     3.2174  13.329  &lt; 2e-16 ***
## Totyrs        1.2429     0.3964   3.136  0.00239 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.25 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1082, Adjusted R-squared:  0.09723 
## F-statistic: 9.832 on 1 and 81 DF,  p-value: 0.00239</code></pre>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m.Totyrs)</span></code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 36.4820525 49.285110
## Totyrs       0.4542191  2.031607</code></pre>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(lead,</span>
<span id="cb218-2"><a href="#cb218-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>Ld73, <span class="at">y=</span>MAXFWT)) <span class="sc">+</span></span>
<span id="cb218-3"><a href="#cb218-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb218-4"><a href="#cb218-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;blood lead concentration (microgram/100ml)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>) <span class="sc">+</span></span>
<span id="cb218-5"><a href="#cb218-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb218-6"><a href="#cb218-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span>m.Ld73<span class="sc">$</span>coefficients[<span class="dv">1</span>],<span class="at">slope=</span>m.Ld73<span class="sc">$</span>coefficients[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## Warning: Removed 19 rows containing missing values (geom_point).</code></pre>
<p><img src="DASM2_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(lead,</span>
<span id="cb220-2"><a href="#cb220-2" aria-hidden="true" tabindex="-1"></a>       <span class="fu">aes</span>(<span class="at">x=</span>Totyrs, <span class="at">y=</span>MAXFWT)) <span class="sc">+</span></span>
<span id="cb220-3"><a href="#cb220-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>) <span class="sc">+</span></span>
<span id="cb220-4"><a href="#cb220-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Number of years living close to smelter&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>) <span class="sc">+</span></span>
<span id="cb220-5"><a href="#cb220-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb220-6"><a href="#cb220-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept=</span>m.Totyrs<span class="sc">$</span>coefficients[<span class="dv">1</span>],<span class="at">slope=</span>m.Totyrs<span class="sc">$</span>coefficients[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## Warning: Removed 19 rows containing missing values (geom_point).</code></pre>
<p><img src="DASM2_files/figure-html/unnamed-chunk-69-2.png" width="672" /></p>
<p>From these two regression analyses, we conclude at the 5% level of significance that there is a significant negative effect of the blood lead concentration on the expected FWT (<span class="math inline">\(p=0.0016\)</span>) and that there is a significant positive effect of the total number of years the child lived close to the lead smelter and the expected blood lead concentration (<span class="math inline">\(p=0.0024\)</span>). Particularly, this latter result is counter-intuitive. It suggests that the longer children are exposed, the better are their FWT score on average. This effect is estimated as an average increase in FWT of 1.2 taps/10seconds for each extra year living near the lead smelter. The 95% confidence of this effect goes from 0.5 to 2 taps/10seconds for each extra year. An explanation will be give later.</p>
</div>
<div id="S:AddMeervoudigModel" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> The Additive Multiple Linear Regression Model</h2>
<div id="the-statistical-model" class="section level3 unnumbered">
<h3>The Statistical Model</h3>
<p>We will use the following notation:</p>
<ul>
<li><p><span class="math inline">\(n\)</span>: total number of sample observations</p></li>
<li><p><span class="math inline">\(p-1\)</span>: number of regressors</p></li>
<li><p><span class="math inline">\(Y_i\)</span>: outcome of observation <span class="math inline">\(i=1,\ldots, n\)</span></p></li>
<li><p><span class="math inline">\(x_{ij}\)</span>: value of regressor <span class="math inline">\(j=1,\ldots, p-1\)</span> for observation <span class="math inline">\(i=1,\ldots, n\)</span>.</p></li>
</ul>
<p>We use the notation <span class="math inline">\(x_j\)</span> for referring to regressor <span class="math inline">\(j\)</span>, and <span class="math inline">\(x_{ij}\)</span> for referring to the value of regressor <span class="math inline">\(j\)</span> for observation <span class="math inline">\(i\)</span>.</p>
<p>We now introduce the additive <strong>normal multiple linear regression model</strong>,
<span class="math display" id="eq:Mod5">\[\begin{equation}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{ip-1} + \eps_ i
 \tag{3.1}
\end{equation}\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>This model can be equivalently represented as
<span class="math display">\[
  Y_i \mid x_{i1}, x_{i2}, \ldots, x_{ip-1} \iid N(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{ip-1},\sigma^2) ,
\]</span>
which better stresses that it is a model for the conditional distribution of the outcome, given the regressors.</p>
<p>Research questions can often be translated into a statistical problem by making use of the regression coefficients (i.e. the <span class="math inline">\(\beta\)</span>-parameters). These have an interpretation in terms of the conditional mean of the outcome, i.e. </p>
<p><span class="math display">\[
  m(x_1,x_2,\ldots, x_{p-1}) = \E{Y \mid x_1,x_2,\ldots, x_{p-1}} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \cdots + \beta_{p-1} x_{p-1}.
\]</span></p>
<p>The interpretation of the <span class="math inline">\(\beta\)</span>-parameters is similar as for the simple regression model, but still there is a very important difference. For parameter <span class="math inline">\(\beta_1\)</span> this follows from the following calculation:</p>
<p><span class="math display">\[\begin{eqnarray*}
&amp; &amp; 
 \E{Y \mid x_1+1, x_2, \ldots, x_{p-1}} - \E{Y \mid x_1, x_2, \ldots, x_{p-1}} \\
&amp;=&amp;  m(x_1+1, x_2, \ldots, x_{p-1}) - m(x_1, x_2, \ldots, x_{p-1}) \\
&amp;=&amp; \left(\beta_0 + \beta_1 (x_{1}+1) + \beta_2 x_{2} + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp; &amp; - \left(\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp;=&amp; \beta_1.
\end{eqnarray*}\]</span></p>
<p>Hence, the parameter <span class="math inline">\(\beta_1\)</span> quantifies the increase in expected outcome when the regressor <span class="math inline">\(x_1\)</span> increases with one unit, while the other regressors in the model remain constant. The expression <em>while the other regressors in the model remain constant</em> is very important for a correct interpretation of the parameter <span class="math inline">\(\beta_1\)</span>.</p>
<p>More generally, for parameter <span class="math inline">\(\beta_j\)</span>,</p>
<p><span class="math display">\[\begin{eqnarray*}
&amp; &amp; 
 \E{Y \mid x_1, \ldots, x_j+1,\ldots, x_{p-1}} - \E{Y \mid x_1, \ldots, x_j,\ldots, x_{p-1}} \\
&amp;=&amp;  m(x_1, \ldots, x_j+1,\ldots, x_{p-1}) - m(x_1, \ldots,  x_j, \ldots, x_{p-1}) \\
&amp;=&amp; \left(\beta_0 + \beta_1 x_{1} + \cdots + \beta_j (x_{j}+1) + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp; &amp; - \left(\beta_0 + \beta_1 x_{1} + \cdots + \beta_j x_{j} + \cdots + \beta_{p-1} x_{p-1}\right) \\
&amp;=&amp; \beta_j.
\end{eqnarray*}\]</span>
The parameter <span class="math inline">\(\beta_j\)</span> thus quantifies the expected increase of the outcome when the regressor <span class="math inline">\(x_j\)</span> increases with one unit while the other regressors in the model remain constant.</p>
<p>Although model <a href="#eq:Mod5">(3.1)</a> is not much more complicated that the simple linear model, the notation is more demanding (because of the multple regressors). A more efficient and compact notation is provided by the matrix notation. We will use the following notation.</p>
<p>With the matrix notation, model <a href="#eq:Mod5">(3.1)</a> can be written as
<span class="math display" id="eq:Mod6">\[\begin{equation}
 \tag{3.2}
 \mb{Y} = \mb{X}\mb{\beta} + \mb{\eps}
\end{equation}\]</span>
with <span class="math inline">\(\mb{\eps} \sim \text{MVN}(\mb{0},\mb{I}_n\sigma^2)\)</span>.
With this notation we also write <span class="math inline">\(m(\mb{x})\)</span> instead of <span class="math inline">\(m(x_1, \ldots, x_{p-1})\)</span>.</p>
<p>The parameters of models <a href="#eq:Mod5">(3.1)</a> and <a href="#eq:Mod6">(3.2)</a> can be estimated with the least squares estimation procedure. The method is identical to the method described in Section <a href="#S:LSE1">2.2</a>.</p>
<p>All the results that were provided in Chapter <a href="#Ch:Reg1">2</a> and that made use of the matrix notation, are still valid here. In the previous chapter the vector <span class="math inline">\(\mb\beta\)</span> had two elements (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>). Now the vector <span class="math inline">\(\mb\beta\)</span> has <span class="math inline">\(p\)</span> elements (<span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_{p-1}\)</span>). The design matrix had 2 columns, whereas it now has <span class="math inline">\(p\)</span> columns. All theory related to the sampling distributions, confidence and prediction intervals and hypothesis tests remain valid for the additive multiple linear regression model. This will now be illustrated for the lead concentration example.</p>
</div>
</div>
<div id="example-lead-concentration-1" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We will focus here on the research question that aims to assess the effects of the blood lead concentration and the total number of years living near the smelter on the FWT score. We thus consider the model</p>
<p><span class="math display">\[
   Y_i = \beta_0 + \beta_1 \text{Ld73}_i + \beta_2 \text{Totyrs}_i +\eps_i
 \]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>The next chunck of R code demonstrates how the parameters can be estimated with the least squares method, making use of the matrix notation.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="co"># indicator for the rows without missing values for MAXFWT</span></span>
<span id="cb222-2"><a href="#cb222-2" aria-hidden="true" tabindex="-1"></a>indNA<span class="ot">&lt;-</span><span class="sc">!</span><span class="fu">is.na</span>(lead<span class="sc">$</span>MAXFWT)</span>
<span id="cb222-3"><a href="#cb222-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb222-4"><a href="#cb222-4" aria-hidden="true" tabindex="-1"></a>Y<span class="ot">&lt;-</span>lead<span class="sc">$</span>MAXFWT[indNA]</span>
<span id="cb222-5"><a href="#cb222-5" aria-hidden="true" tabindex="-1"></a>XReg<span class="ot">&lt;-</span><span class="fu">as.matrix</span>(lead[indNA,<span class="fu">c</span>(<span class="st">&quot;Ld73&quot;</span>,<span class="st">&quot;Totyrs&quot;</span>)])</span>
<span id="cb222-6"><a href="#cb222-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(XReg)</span></code></pre></div>
<pre><code>##   Ld73 Totyrs
## 1   18     11
## 2   28      6
## 3   29      5
## 4   30      5
## 5   34     11
## 6   25      6</code></pre>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">cbind</span>(<span class="dv">1</span>,XReg)</span>
<span id="cb224-2"><a href="#cb224-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(X)</span></code></pre></div>
<pre><code>##     Ld73 Totyrs
## 1 1   18     11
## 2 1   28      6
## 3 1   29      5
## 4 1   30      5
## 5 1   34     11
## 6 1   25      6</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a>beta.hat<span class="ot">&lt;-</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">%*%</span><span class="fu">t</span>(X)<span class="sc">%*%</span>Y</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a>beta.hat</span></code></pre></div>
<pre><code>##              [,1]
##        55.3138948
## Ld73   -0.4115833
## Totyrs  1.3029689</code></pre>
<p>From these calculations we find
<span class="math display">\[\begin{eqnarray*}
  \hat\beta_0 &amp;=&amp; 55.31 \\
  \hat\beta_1 &amp;=&amp; -0.41 \\
  \hat\beta_2 &amp;=&amp; 1.30 .
 \end{eqnarray*}\]</span></p>
<p>The same estimates can be obtained with the <em>lm</em> function.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a>m.LdTot<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">+</span>Totyrs,<span class="at">data=</span>lead)</span>
<span id="cb228-2"><a href="#cb228-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.LdTot)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.665  -4.731  -0.464   7.645  26.638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  55.3139     4.5428  12.176  &lt; 2e-16 ***
## Ld73         -0.4116     0.1130  -3.642 0.000478 ***
## Totyrs        1.3030     0.3698   3.524 0.000707 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.42 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2351, Adjusted R-squared:  0.216 
## F-statistic: 12.29 on 2 and 80 DF,  p-value: 2.21e-05</code></pre>
<p>Interpretation of the regression coefficients:</p>
<ul>
<li><p>We estimate that childeren that all lived for the same time near the smelter, their FWT decreases on average with <span class="math inline">\(4\)</span> taps in a 10 second interval for an increase of their blood lead levels with 10 microgram per 100ml. This effect is often referred to as the effect of blood lead concentrations, <em>corrected for</em> (or <em>controlling for</em>) years living near the smelter. Note that for expressing the effect of blood lead levels we do not need to specify the exact value of the <em>Totyrs</em>. The effect of blood lead levels is the same for each fixed value of <em>Totyrs</em>.</p></li>
<li><p>We estimate that children that all have the same blood lead contrentrations, their FWT increases on average with <span class="math inline">\(1.3\)</span> taps in a 10 second interval for an increase of their time living near the smelter with 1 year. This is the estimated effect of time, <em>corrected for</em> (or <em>controlling for</em>) blood lead levels. Note that for expressing the effect of <em>Totyrs</em> we do not need to specify the exact value of the <em>Ld73</em>. The effect of <em>Totyrs</em> is the same for each fixed value of <em>Ld73</em>.</p></li>
</ul>
<p>With only two regressors, the fitted regression model can be visualised in a three-dimensional graph. It is a flat plane. See Figure <a href="#fig:LeadFit3D">3.1</a>.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">range</span>(lead<span class="sc">$</span>Ld73)[<span class="dv">1</span>],<span class="fu">range</span>(lead<span class="sc">$</span>Ld73)[<span class="dv">2</span>], <span class="at">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb230-2"><a href="#cb230-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">range</span>(lead<span class="sc">$</span>Totyrs)[<span class="dv">1</span>],<span class="fu">range</span>(lead<span class="sc">$</span>Totyrs)[<span class="dv">2</span>], <span class="at">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb230-3"><a href="#cb230-3" aria-hidden="true" tabindex="-1"></a>MAXFWT <span class="ot">&lt;-</span> <span class="fu">outer</span>(x, y, <span class="cf">function</span>(a, b){<span class="fu">predict</span>(m.LdTot,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">Ld73=</span>a,<span class="at">Totyrs=</span>b))})</span>
<span id="cb230-4"><a href="#cb230-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-5"><a href="#cb230-5" aria-hidden="true" tabindex="-1"></a>axx<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;blood lead concentration (microgram/100ml)&quot;</span>)</span>
<span id="cb230-6"><a href="#cb230-6" aria-hidden="true" tabindex="-1"></a>axy<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;Number of years living near smelter&quot;</span>)</span>
<span id="cb230-7"><a href="#cb230-7" aria-hidden="true" tabindex="-1"></a>axz<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>)</span>
<span id="cb230-8"><a href="#cb230-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-9"><a href="#cb230-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb230-10"><a href="#cb230-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x=</span>lead<span class="sc">$</span>Ld73,</span>
<span id="cb230-11"><a href="#cb230-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>lead<span class="sc">$</span>Totyrs,</span>
<span id="cb230-12"><a href="#cb230-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span>lead<span class="sc">$</span>MAXFWT) <span class="sc">%&gt;%</span></span>
<span id="cb230-13"><a href="#cb230-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x=</span><span class="sc">~</span>x,</span>
<span id="cb230-14"><a href="#cb230-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span><span class="sc">~</span>y,</span>
<span id="cb230-15"><a href="#cb230-15" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span><span class="sc">~</span>MAXFWT,</span>
<span id="cb230-16"><a href="#cb230-16" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale=</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb230-17"><a href="#cb230-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis=</span>axx,<span class="at">yaxis=</span>axy,<span class="at">zaxis=</span>axz))</span></code></pre></div>
<pre><code>## Warning: Ignoring 19 observations</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:LeadFit3D"></span>
<div id="htmlwidget-9bd837d861d524e31f44" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-9bd837d861d524e31f44">{"x":{"visdat":{"3511681cba29":["function () ","plotlyVisDat"]},"cur_data":"3511681cba29","attrs":{"3511681cba29":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,31,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,27,26,20,27,28,31,28,37,34,34,23,25,25,23,43,49,47,45,45],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,5,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,4,3,4,4,4,4,4,4,3,4,4,5,4,6,4,4,4,3,3],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,null,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,null,null,null,null,null,null,null,null,null,48,null,null,null,null,50,null,null,null,null,null],"type":"scatter3d","mode":"markers","inherit":true},"3511681cba29.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"blood lead concentration (microgram/100ml)"},"yaxis":{"title":"Number of years living near smelter"},"zaxis":{"title":"FWT (taps per 10 seconds)"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,23],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,6],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,48,50],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"MAXFWT","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333332","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[50.4431143620533,52.4699548026394,54.4967952432254,56.5236356838115,58.5504761243976,60.5773165649837,62.6041570055698,64.6309974461559,66.6578378867419,68.684678327328],[48.4766608958821,50.5035013364682,52.5303417770542,54.5571822176403,56.5840226582264,58.6108630988125,60.6377035393986,62.6645439799847,64.6913844205708,66.7182248611568],[46.5102074297109,48.537047870297,50.563888310883,52.5907287514691,54.6175691920552,56.6444096326413,58.6712500732274,60.6980905138135,62.7249309543996,64.7517713949856],[44.5437539635397,46.5705944041258,48.5974348447118,50.6242752852979,52.651115725884,54.6779561664701,56.7047966070562,58.7316370476423,60.7584774882284,62.7853179288144],[42.5773004973685,44.6041409379546,46.6309813785406,48.6578218191267,50.6846622597128,52.7115027002989,54.738343140885,56.7651835814711,58.7920240220572,60.8188644626432],[40.6108470311973,42.6376874717834,44.6645279123695,46.6913683529555,48.7182087935416,50.7450492341277,52.7718896747138,54.7987301152999,56.825570555886,58.852410996472],[38.6443935650261,40.6712340056122,42.6980744461983,44.7249148867843,46.7517553273704,48.7785957679565,50.8054362085426,52.8322766491287,54.8591170897148,56.8859575303008],[36.6779400988549,38.704780539441,40.7316209800271,42.7584614206131,44.7853018611992,46.8121423017853,48.8389827423714,50.8658231829575,52.8926636235436,54.9195040641296],[34.7114866326837,36.7383270732698,38.7651675138559,40.7920079544419,42.818848395028,44.8456888356141,46.8725292762002,48.8993697167863,50.9262101573724,52.9530505979585],[32.7450331665125,34.7718736070986,36.7987140476847,38.8255544882707,40.8523949288568,42.8792353694429,44.906075810029,46.9329162506151,48.9597566912012,50.9865971317872]],"type":"surface","x":[15,19.7777777777778,24.5555555555556,29.3333333333333,34.1111111111111,38.8888888888889,43.6666666666667,48.4444444444444,53.2222222222222,58],"y":[1,2.55555555555556,4.11111111111111,5.66666666666667,7.22222222222222,8.77777777777778,10.3333333333333,11.8888888888889,13.4444444444444,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.1: Three-dimensional scatter plot and fitted regression model for the lead concentration example.
</p>
</div>
<p>The standard error of the parameter estimates are also part of the output. The 95% confidence intervals of the parameter esimates can be obtained as before, as illustrated in the next R code.</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m.LdTot)</span></code></pre></div>
<pre><code>##                  2.5 %    97.5 %
## (Intercept) 46.2734484 64.354341
## Ld73        -0.6364626 -0.186704
## Totyrs       0.5670955  2.038842</code></pre>
<p>Thus, for children with the same blood lead levels, we expect with a probability of 95% that their FWT increases on average with <span class="math inline">\(0.6\)</span> up to <span class="math inline">\(2\)</span> taps per 10 seconds for each extra year of living in the neighborhood of the smelter. This is of course an unexpected and weird conclusion, because one would expect that the accumulated exposure to the lead-poluled environment would result in a negative effect on the neuromuscular system, rather than the positive effect that we observe here. Since the data come from an observational study, we have here only established associations and no causations.</p>
<p>Many research questions can be translated in hypotheses such as (<span class="math inline">\(j=1,\ldots, p-1\)</span>)
<span class="math display">\[
  H_0: \beta_j = 0 \text{ versus }
  H_1: \beta_j \neq 0
\]</span>
(or one-sided alternatives).</p>
<p>The null hypothesis expresses that regressor <span class="math inline">\(j\)</span> has no linear effect on the mean outcome, given that all other regressors in the model remain constant, i.e. 
<span class="math display">\[
 \E{Y \mid x_1, \ldots, x_j+1,\ldots, x_{p-1}} = \E{Y \mid x_1, \ldots, x_j,\ldots, x_{p-1}}
\]</span>
for all <span class="math inline">\(x_1, x_2, \ldots, x_{p-1}\)</span>.</p>
<p>If <span class="math inline">\(H_0:\beta_j=0\)</span> is true, and assuming that model <a href="#eq:Mod6">(3.2)</a> holds, we find
<span class="math display">\[
  T= \frac{\hat\beta_j}{\hat\sigma_{\beta_j}} \HSim t_{n-p}.
\]</span>
This result is sufficient for finding <span class="math inline">\(p\)</span>-values and critical values for the test, just like in the previous chapter.</p>
<p>First we test the hypotheses
<span class="math display">\[
   H_0: \beta_1=0 \text{ versus } H_1:\beta_1\neq 0 .
 \]</span>
This null hypothesis implies that for children living for the same time near the smelter, their blood lead levels have no linear effect on the average FWT.</p>
<p>Next we will test
<span class="math display">\[
   H_0: \beta_2=0 \text{ versus } H_1:\beta_2\neq 0 .
 \]</span>
Here the null hypothesis implies that for children with the same blood lead levels, the time living near the smelter has no linear effect on the average FWT. The <span class="math inline">\(p\)</span>-values can be read from the <em>summary</em> output.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.LdTot)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.665  -4.731  -0.464   7.645  26.638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  55.3139     4.5428  12.176  &lt; 2e-16 ***
## Ld73         -0.4116     0.1130  -3.642 0.000478 ***
## Totyrs        1.3030     0.3698   3.524 0.000707 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.42 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2351, Adjusted R-squared:  0.216 
## F-statistic: 12.29 on 2 and 80 DF,  p-value: 2.21e-05</code></pre>
<p>From the R output we read that the <span class="math inline">\(p\)</span>-values for both hypothesis tests are smaller than the nominal significance level of <span class="math inline">\(\alpha=0.05\)</span>. Thus, at the 5% level of significance, we conclude that the blood level concentration has a negative effect on the mean FWT, while controlling for the number of years living near the lead smelter (<span class="math inline">\(p=0.0005\)</span>). We also conclude that the time living near the smelter has a significant positive effect on the mean FWT while controlling for blood lead levels (<span class="math inline">\(p=0.0007\)</span>).</p>
<p>We now illustrate how to make predictions based on the fitted multiple linear regression model. Suppose that we want to predict the FWT score of a child with a blood lead concentration of 35 microgram / 100ml and who lived 4 years near the smelter.</p>
<p>The numerical value of this prediction is calculated as
<span class="math display">\[
   \hat{y} = \hat\beta_0 +\hat\beta_1 \text{Ld73} + \hat\beta_2\text{Totyrs} = 55.31-0.41\times 35 + 1.30\times 4 = 46.12.
 \]</span>
The 95% PI is computed with the following R code.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="#cb236-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m.LdTot,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">Ld73=</span><span class="dv">35</span>,<span class="at">Totyrs=</span><span class="dv">4</span>), <span class="at">interval=</span><span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 46.12036 23.10707 69.13364</code></pre>
<p>So we conclude that with a chance of 95% a child with a blood lead level of 35 microgram / 100ml and who lived for 4 years near the smelter will have a FWT of somewhere between 23 and 69 taps per 10 seconds. Note that this is a rather wide interval, knowing that the range of FWT values in the dataset goes from 13 to 84 taps per 10 seconds.</p>
</div>
<div id="exercise-lead-concentration" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Consider again the lead concentration dataset, but now we want to assess the effect of exposure on the mean FWT. Exposure is a binary variable (<em>Exposed</em>) coded as 1 when the subject was exposed, and 0 when not. Since we expect that also age has an effect on the FWT (these are children and with age their neuromuscular system still becomes better), we will correct the effect of exposure for age.</p>
<p>Fit a regression model with <em>Age</em> and <em>Exposed</em> in the model. Interpret the model parameter estimates. Assess the effect of exposure, corrected for age, and also assess the effect of age, corrected for exposure. Make an informative graph.</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="#cb238-1" aria-hidden="true" tabindex="-1"></a>lead<span class="sc">$</span>Gender<span class="ot">&lt;-</span><span class="fu">ifelse</span>(lead<span class="sc">$</span>Sex<span class="sc">==</span><span class="dv">1</span>,<span class="st">&quot;Boy&quot;</span>,<span class="st">&quot;Girl&quot;</span>)</span>
<span id="cb238-2"><a href="#cb238-2" aria-hidden="true" tabindex="-1"></a>lead<span class="sc">$</span>Exposure<span class="ot">&lt;-</span><span class="fu">ifelse</span>(lead<span class="sc">$</span>Exposed<span class="sc">==</span><span class="dv">1</span>,<span class="st">&quot;Exposed&quot;</span>,<span class="st">&quot;Not Exposed&quot;</span>)</span>
<span id="cb238-3"><a href="#cb238-3" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Exposure,<span class="at">data=</span>lead)</span>
<span id="cb238-4"><a href="#cb238-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Age + Exposure, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.505  -4.250   2.049   6.098  16.773 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          19.7742     3.8571   5.127 2.01e-06 ***
## Age                   2.6202     0.3464   7.564 5.76e-11 ***
## ExposureNot Exposed   7.5158     2.4776   3.033  0.00326 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.367 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4852, Adjusted R-squared:  0.4723 
## F-statistic:  37.7 on 2 and 80 DF,  p-value: 2.923e-12</code></pre>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="#cb240-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m)</span></code></pre></div>
<pre><code>##                         2.5 %    97.5 %
## (Intercept)         12.098393 27.450083
## Age                  1.930831  3.309656
## ExposureNot Exposed  2.585143 12.446383</code></pre>
<p>Based on the this output we come to the following conclusions:</p>
<ul>
<li><p>It is estimated that for children of the same age, on average the FWT is <span class="math inline">\(7.5\)</span> taps per 10 seconds lower in the exposed group as compared to the non-exposed group. With a probability of 95%, this effect ranges between <span class="math inline">\(2.6\)</span> and <span class="math inline">\(12.4\)</span> taps per 10 seconds. This is a significant result at the 5% level of significance (two-sided p-value = <span class="math inline">\(0.0033\)</span>).</p></li>
<li><p>For children within the same exposure group, we estimate that the mean FWT score increases with <span class="math inline">\(2.6\)</span> taps per 10 seconds for each extra year of each. This comes with a 95% confidence interval ranging from <span class="math inline">\(1.9\)</span> to <span class="math inline">\(3.1\)</span> taps per 10 seconds. This is a significant result at the 5% level of significance (two-sided p-value <span class="math inline">\(&lt;0.001\)</span>).</p></li>
</ul>
<p>The next graph gives a graphical appreciation of the results. The graph illustrates the additivity of the model fit:</p>
<ul>
<li><p>within an exposure group, the effect of age on the mean FWT is the same</p></li>
<li><p>for any given age, the effect of exposure on the mean FWT is the same</p></li>
</ul>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="#cb242-1" aria-hidden="true" tabindex="-1"></a>lead<span class="sc">$</span>fitted[indNA]<span class="ot">&lt;-</span><span class="fu">predict</span>(m, <span class="at">newdata =</span> lead[indNA,])</span>
<span id="cb242-2"><a href="#cb242-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb242-3"><a href="#cb242-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(lead[indNA,], <span class="fu">aes</span>(<span class="at">x=</span>Age, <span class="at">y=</span>MAXFWT, <span class="at">color=</span>Exposure)) <span class="sc">+</span></span>
<span id="cb242-4"><a href="#cb242-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb242-5"><a href="#cb242-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>fitted), <span class="at">size=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb242-6"><a href="#cb242-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb242-7"><a href="#cb242-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Age (years)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>) </span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
</details>
</div>
<div id="the-non-additive-multiple-linear-regression-model" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> The Non-Additive Multiple Linear Regression Model</h2>
<div id="interaction" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Interaction</h3>
<p>We extend model <a href="#eq:Mod5">(3.1)</a> by added an <strong>interaction term</strong>. To introduce this new concept, we start with a model with only two regressors. Consider the model</p>
<p><span class="math display" id="eq:tmp28797869287">\[\begin{equation}
\tag{3.3}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i1}x_{i2} + \eps_ i
\end{equation}\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.
The only difference with model <a href="#eq:Mod5">(3.1)</a> for <span class="math inline">\(p-1=2\)</span> regressors is that the term <span class="math inline">\(\beta_3x_{i1}x_{i2}\)</span> is added. This extra term is the <strong>interaction term</strong>; it quantifies the <strong>interaction effect</strong> of the regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> on the mean outcome (note: it is <em>not</em> the interaction between two regressors). In model <a href="#eq:tmp28797869287">(3.3)</a> we call the terms <span class="math inline">\(\beta_1x_{i1}\)</span> and <span class="math inline">\(\beta_2x_{i2}\)</span> the <strong>main effects</strong> of the regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, respectively.</p>
<p>To understand the implications of the addition of the interaction term, we proceed as we studied the effect of <span class="math inline">\(x_1\)</span> in Section <a href="#S:AddMeervoudigModel">3.1</a>: we calculate the difference in expected outcome when <span class="math inline">\(x_1\)</span> increases with one unit when the regressor <span class="math inline">\(x_2\)</span> is kept constant:</p>
<p><span class="math display">\[\begin{eqnarray*}
  &amp; &amp; \E{Y \mid x_1+1,x_2} - \E{Y\mid x_1,x_2} \\
   &amp;=&amp; [\beta_0 + \beta_1 (x_{1}+1) + \beta_2 x_{2} + \beta_3 (x_{1}+1)x_{2}]- [\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1}x_{2}] \\
   &amp;=&amp; \beta_1 + \beta_3 x_{2}.
\end{eqnarray*}\]</span></p>
<p>This expression shows that the effect of <span class="math inline">\(x_1\)</span> depends on the value of <span class="math inline">\(x_2\)</span>! In the additive model, <span class="math inline">\(\beta_3=0\)</span> and the effect of <span class="math inline">\(x_1\)</span> is quantified by <span class="math inline">\(\beta_1\)</span> alone, and this effect is the same for all values of <span class="math inline">\(x_2\)</span>.</p>
<p>Similar, for the effect of <span class="math inline">\(x_2\)</span>, at a constant value of <span class="math inline">\(x_1\)</span>:</p>
<p><span class="math display">\[\begin{eqnarray*}
  &amp; &amp; \E{Y \mid x_1,x_2+1} - \E{Y\mid x_1,x_2} \\
   &amp;=&amp; [\beta_0 + \beta_1 x_{1} + \beta_2 (x_{2}+1) + \beta_3 x_{1}(x_{2}+1)]- [\beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_{1}x_{2}] \\
   &amp;=&amp; \beta_2 + \beta_3 x_{1}.
\end{eqnarray*}\]</span></p>
<p>The presence of an interaction effect has a very important implication: the main effects, as quantified by <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, have no longer a meaningful interpration. For example. the parameter <span class="math inline">\(\beta_1\)</span> only has an interpretation of the effect of <span class="math inline">\(x_1\)</span> if <span class="math inline">\(x_2=0\)</span>. As a consequence, testing null hypotheses <span class="math inline">\(H_0:\beta_1=0\)</span> and <span class="math inline">\(H_0:\beta_2=0\)</span> in model <a href="#eq:tmp28797869287">(3.3)</a> is meaningless and should not be done, unless there is no interaction. These arguments result in an example of <strong>hierarchical model building</strong>: in model <a href="#eq:tmp28797869287">(3.3)</a> we test the hypotheses in the following order:</p>
<ol style="list-style-type: decimal">
<li><p>test <span class="math inline">\(H_0:\beta_3=0\)</span> (test for absence of interaction effect)</p></li>
<li><p>test <span class="math inline">\(H_0:\beta_1=0\)</span> and/or <span class="math inline">\(H_0:\beta_2=0\)</span> (test of absence of main effects) only if there is no evidence or indication for the presence of an interaction effect.</p></li>
</ol>
</div>
<div id="parameter-estimators" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Parameter Estimators</h3>
<p>We can be very brief about estimating the parameters of model <a href="#eq:tmp28797869287">(3.3)</a>. Note that we can define a third regressor as <span class="math inline">\(x_{i3}=x_{i1}x_{i2}\)</span> (<span class="math inline">\(i=1,\ldots, n\)</span>), with which we can rewrite the model as</p>
<p><span class="math display">\[\begin{equation*}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \eps_ i
\end{equation*}\]</span></p>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. This model is of the form of model <a href="#eq:Mod5">(3.1)</a> for <span class="math inline">\(p-1=3\)</span> regressors. Hence, all theory for the (additive) regression model still applies. It is only the interpration of the parameters that changes.</p>
</div>
<div id="example-lead-concentration-2" class="section level3 unnumbered">
<h3>Example (Lead concentration)</h3>
<p>Earlier we have analysed the Lead Concentration data with an additive model. We now know that this analysis only makes sense in the absence of an interacion effect. We therefore now analyse the data with the model
<span class="math display">\[
   Y_i = \beta_0 + \beta_1 \text{Ld73}_i + \beta_2 \text{Totyrs}_i + \beta_3 \text{Ld73}_i\times \text{Totyrs}_i +\eps_i
 \]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. We will test the hypotheses
<span class="math display">\[
   H_0:\beta_3=0 \text{ versus } H_1:\beta_3\neq 0 
 \]</span>
at the <span class="math inline">\(5\%\)</span> level of significance.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="#cb243-1" aria-hidden="true" tabindex="-1"></a>m.LdTotInt<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">+</span>Totyrs<span class="sc">+</span>Ld73<span class="sc">:</span>Totyrs,<span class="at">data=</span>lead)</span>
<span id="cb243-2"><a href="#cb243-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.LdTotInt)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs + Ld73:Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -41.050  -5.041  -0.778   7.145  26.003 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 65.44259   10.11101   6.472 7.48e-09 ***
## Ld73        -0.76982    0.33894  -2.271   0.0259 *  
## Totyrs      -0.02079    1.23740  -0.017   0.9866    
## Ld73:Totyrs  0.04665    0.04162   1.121   0.2658    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.4 on 79 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2471, Adjusted R-squared:  0.2185 
## F-statistic: 8.641 on 3 and 79 DF,  p-value: 4.998e-05</code></pre>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="#cb245-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m.LdTotInt)</span></code></pre></div>
<pre><code>##                   2.5 %      97.5 %
## (Intercept) 45.31712607 85.56806075
## Ld73        -1.44446176 -0.09517508
## Totyrs      -2.48376371  2.44218377
## Ld73:Totyrs -0.03619254  0.12949028</code></pre>
<p>From the R output we read on the line of <em>Ld73:Totyrs</em> the parameter estimate of the interaction effect: <span class="math inline">\(\hat\beta_3=0.047\)</span>. This tells us that we estimate that the effect of the time living near the smelter (<span class="math inline">\(\beta_2\)</span>), increases with an additional <span class="math inline">\(10\times 0.047\approx 0.5\)</span> taps per 10 seconds for each increase in blood lead concentration of 10 micrograms / 100ml (this sentence refers to the effect of <em>Totyrs</em> being <span class="math inline">\(\beta_2 + \beta_3\)</span><em>Ld73</em>). Thus, the larger the lead concentration in the blood, the stronger the positive effect of <em>Totyrs</em>! Also the 95% confidence interval of <span class="math inline">\(\beta_3\)</span> suggests only small interaction effects, mostly positive, but potentially also negative effects are plausible. The <span class="math inline">\(p\)</span>-value for testing <span class="math inline">\(H_0:\beta_3=0\)</span> versus <span class="math inline">\(H_1: \beta_3\neq 0\)</span> equals <span class="math inline">\(p=0.2658\)</span>. Thus, at the <span class="math inline">\(5\%\)</span> level of significance there is hardly any evidence of an interaction effect, and if it were present the estimate and its confidence interval indicate only a very small effect. Hence, it seems OK to remove the interaction term from the model, and continue with the analysis based on the additive model, as we have done before.</p>
<p>Some notes:</p>
<ul>
<li><p>when the interaction effect is not significant, we adapt the convention to first remove the interaction term from the model and refitting the model before looking at the main effects.</p></li>
<li><p>when looking at the estimates of the main effects in the model with interaction, we see e.g. the estimate <span class="math inline">\(\hat\beta_2\)</span> is now negative. It was positve in the additive model. However, this parameter is hard to interpret. Moreover, although the estimate is negative, the effect of <em>Totyrs</em> is still positive! This can be seen as follows. With the interaction term, the effect of <em>Totyrs</em> is given by <span class="math inline">\(\hat\beta_2 + \hat\beta_3\)</span><em>Ld73</em>, and the range of blood lead concentrations is 15 to 58 microgram / 100ml. The estimated effect of <em>Totyrs</em> thus ranges from <span class="math inline">\(0.7\)</span> to <span class="math inline">\(2.7\)</span> taps per 10 seconds per increase of <em>Totyrs</em> with one year.</p></li>
</ul>
<p>Figure <a href="#fig:Interactie">3.2</a> gives a three-dimensional illustration of model <a href="#eq:tmp28797869287">(3.3)</a>.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">range</span>(lead<span class="sc">$</span>Ld73)[<span class="dv">1</span>],<span class="fu">range</span>(lead<span class="sc">$</span>Ld73)[<span class="dv">2</span>], <span class="at">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb247-2"><a href="#cb247-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">range</span>(lead<span class="sc">$</span>Totyrs)[<span class="dv">1</span>],<span class="fu">range</span>(lead<span class="sc">$</span>Totyrs)[<span class="dv">2</span>], <span class="at">length.out =</span> <span class="dv">10</span>)</span>
<span id="cb247-3"><a href="#cb247-3" aria-hidden="true" tabindex="-1"></a>MAXFWT <span class="ot">&lt;-</span> <span class="fu">outer</span>(x, y, <span class="cf">function</span>(a, b){<span class="fu">predict</span>(m.LdTotInt,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">Ld73=</span>a,<span class="at">Totyrs=</span>b))})</span>
<span id="cb247-4"><a href="#cb247-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb247-5"><a href="#cb247-5" aria-hidden="true" tabindex="-1"></a>axx<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;blood lead concentration (microgram/100ml)&quot;</span>)</span>
<span id="cb247-6"><a href="#cb247-6" aria-hidden="true" tabindex="-1"></a>axy<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;Number of years living near smelter&quot;</span>)</span>
<span id="cb247-7"><a href="#cb247-7" aria-hidden="true" tabindex="-1"></a>axz<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;FWT (taps per 10 seconds)&quot;</span>)</span>
<span id="cb247-8"><a href="#cb247-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb247-9"><a href="#cb247-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb247-10"><a href="#cb247-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x=</span>lead<span class="sc">$</span>Ld73,</span>
<span id="cb247-11"><a href="#cb247-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>lead<span class="sc">$</span>Totyrs,</span>
<span id="cb247-12"><a href="#cb247-12" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span>lead<span class="sc">$</span>MAXFWT) <span class="sc">%&gt;%</span></span>
<span id="cb247-13"><a href="#cb247-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x=</span><span class="sc">~</span>x,</span>
<span id="cb247-14"><a href="#cb247-14" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span><span class="sc">~</span>y,</span>
<span id="cb247-15"><a href="#cb247-15" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span><span class="sc">~</span>MAXFWT,</span>
<span id="cb247-16"><a href="#cb247-16" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale=</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb247-17"><a href="#cb247-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis=</span>axx,<span class="at">yaxis=</span>axy,<span class="at">zaxis=</span>axz))</span></code></pre></div>
<pre><code>## Warning: Ignoring 19 observations</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Interactie"></span>
<div id="htmlwidget-cedc12a069507f758d43" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-cedc12a069507f758d43">{"x":{"visdat":{"35116266be47":["function () ","plotlyVisDat"]},"cur_data":"35116266be47","attrs":{"35116266be47":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,31,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,27,26,20,27,28,31,28,37,34,34,23,25,25,23,43,49,47,45,45],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,5,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,4,3,4,4,4,4,4,4,3,4,4,5,4,6,4,4,4,3,3],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,null,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,null,null,null,null,null,null,null,null,null,48,null,null,null,null,50,null,null,null,null,null],"type":"scatter3d","mode":"markers","inherit":true},"35116266be47.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"blood lead concentration (microgram/100ml)"},"yaxis":{"title":"Number of years living near smelter"},"zaxis":{"title":"FWT (taps per 10 seconds)"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[18,28,29,30,34,25,24,15,16,24,19,27,29,32,25,23,28,19,22,22,27,38,25,24,24,20,18,18,26,27,24,22,31,34,37,25,20,16,21,34,27,22,26,30,33,26,23,23,23,24,31,39,32,23,16,28,33,34,38,21,31,33,53,49,40,40,47,45,43,58,48,50,40,58,57,51,47,48,43,54,52,34,23],"y":[11,6,5,5,11,6,6,15,7,7,12,10,12,12,10,10,15,9,8,11,7,10,2,2,2,9,1,1,5,5,5,6,6,12,6,2,11,9,9,10,6,3,3,5,15,6,14,6,5,5,3,3,8,8,8,7,2,6,6,6,8,15,10,11,9,6,4,12,6,7,6,6,5,6,7,10,12,8,8,6,8,3,6],"z":[72,61,49,48,51,49,50,58,50,51,59,65,57,53,74,50,84,46,52,64,59,55,46,52,63,52,42,57,23,65,38,59,26,53,50,56,49,76,68,60,46,57,45,46,64,40,62,13,79,61,46,50,48,65,62,56,54,72,57,50,65,56,54,57,48,41,34,54,38,49,58,14,40,13,51,44,52,42,55,44,48,48,50],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"MAXFWT","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[54.5742601434174,55.6303937856728,56.6865274279282,57.7426610701836,58.798794712439,59.8549283546943,60.9110619969497,61.9671956392051,63.0233292814605,64.0794629237159],[51.1191167222872,52.5219493614186,53.92478200055,55.3276146396814,56.7304472788128,58.1332799179442,59.5361125570755,60.9389451962069,62.3417778353383,63.7446104744697],[47.663973301157,49.4135049371644,51.1630365731718,52.9125682091792,54.6620998451866,56.411631481194,58.1611631172013,59.9106947532087,61.6602263892161,63.4097580252235],[44.2088298800268,46.3050605129102,48.4012911457936,50.497521778677,52.5937524115604,54.6899830444438,56.7862136773271,58.8824443102105,60.9786749430939,63.0749055759773],[40.7536864588966,43.196616088656,45.6395457184154,48.0824753481748,50.5254049779342,52.9683346076936,55.4112642374529,57.8541938672123,60.2971234969717,62.7400531267311],[37.2985430377664,40.0881716644018,42.8778002910372,45.6674289176726,48.457057544308,51.2466861709434,54.0363147975787,56.8259434242141,59.6155720508495,62.4052006774849],[33.8433996166362,36.9797272401476,40.116054863659,43.2523824871704,46.3887101106818,49.5250377341932,52.6613653577046,55.7976929812159,58.9340206047273,62.0703482282387],[30.388256195506,33.8712828158934,37.3543094362808,40.8373360566682,44.3203626770556,47.803389297443,51.2864159178304,54.7694425382177,58.2524691586051,61.7354957789925],[26.9331127743758,30.7628383916392,34.5925640089026,38.422289626166,42.2520152434294,46.0817408606928,49.9114664779562,53.7411920952196,57.5709177124829,61.4006433297463],[23.4779693532456,27.654393967385,31.8308185815244,36.0072431956638,40.1836678098032,44.3600924239426,48.536517038082,52.7129416522214,56.8893662663607,61.0657908805001]],"type":"surface","x":[15,19.7777777777778,24.5555555555556,29.3333333333333,34.1111111111111,38.8888888888889,43.6666666666667,48.4444444444444,53.2222222222222,58],"y":[1,2.55555555555556,4.11111111111111,5.66666666666667,7.22222222222222,8.77777777777778,10.3333333333333,11.8888888888889,13.4444444444444,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.2: Three-dimensional scatter plot and fitted regression model for the lead concentration example
</p>
</div>
</div>
</div>
<div id="example-blood-pressure-4" class="section level2 unnumbered">
<h2>Example (Blood Pressure)</h2>
<p>We consider again the Blood Pressure example and build a model with both dose and gender as regressors. We start with the model that also contains the interaction effect. The <em>gender</em> variable in <em>BloodPressure</em> dataset is coded as follows: 0=man, 1=women.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="#cb249-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose<span class="sc">*</span>gender,<span class="at">data=</span>BloodPressure)</span>
<span id="cb249-2"><a href="#cb249-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ dose * gender, data = BloodPressure)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.3817 -2.3941  0.3182  2.2763  7.1774 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   2.2618     1.2552   1.802   0.0799 .  
## dose          1.4561     0.2151   6.769 6.62e-08 ***
## gender       -4.8307     1.8440  -2.620   0.0128 *  
## dose:gender   0.7390     0.3271   2.259   0.0300 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.856 on 36 degrees of freedom
## Multiple R-squared:  0.7816, Adjusted R-squared:  0.7634 
## F-statistic: 42.95 on 3 and 36 DF,  p-value: 5.55e-12</code></pre>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="#cb251-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m)</span></code></pre></div>
<pre><code>##                   2.5 %    97.5 %
## (Intercept) -0.28384132  4.807536
## dose         1.01978533  1.892366
## gender      -8.57039187 -1.090976
## dose:gender  0.07561727  1.402342</code></pre>
<p>This analysis shows that at the 5% level of significance we reject the null hypothesis of no-interaction (<span class="math inline">\(p=0.03\)</span>). The interaction effect size is estimated as <span class="math inline">\(0.74\)</span>. This means that the effect of dose is estimated to be <span class="math inline">\(0.74\)</span> mmHg / mg/day larger among women as compared to men. In combination with the estimated main effects, this means that we estimate the effect of the daily dose for woman to be <span class="math inline">\(1.46+0.74=2.2\)</span> mmHg / mg, whereas it is only <span class="math inline">\(1.46\)</span> mmHg / mg for men.</p>
<p>This is visualised in the following graph.</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="#cb253-1" aria-hidden="true" tabindex="-1"></a>BloodPressure<span class="sc">$</span>fitted<span class="ot">&lt;-</span><span class="fu">predict</span>(m)</span>
<span id="cb253-2"><a href="#cb253-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb253-3"><a href="#cb253-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(BloodPressure, <span class="fu">aes</span>(<span class="at">x=</span>dose, <span class="at">y=</span>bp.reduction, <span class="at">color=</span><span class="fu">as.factor</span>(gender))) <span class="sc">+</span></span>
<span id="cb253-4"><a href="#cb253-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb253-5"><a href="#cb253-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y=</span>fitted), <span class="at">size=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb253-6"><a href="#cb253-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">15</span>)) <span class="sc">+</span></span>
<span id="cb253-7"><a href="#cb253-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">xlab</span>(<span class="st">&quot;Dose (mg / day)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Diastolic blood pressure reduction (mmHg)&quot;</span>) </span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<p>Interaction is sometimes also referred to as <strong>effect modification</strong>. In this example, we could say that gender is an effect modifyer. It modifies the effect of the dose of the treatment.</p>
</div>
<div id="exercise-blood-pressure-2" class="section level2 unnumbered">
<h2>Exercise: Blood Pressure</h2>
<p>In the previous example, we have given dose effect estimates for men and women separately. What are the standard errors and 95% confidence intervals for these two effects?</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<p>For men, the answer is easy, because the effect for men is directly estimated by a single parameter. So the estimated effect is <span class="math inline">\(1.46\)</span> mmHg / mg/day, with SE<span class="math inline">\(=0.22\)</span> mmHg / mg/day. The 95% confidence interval can also be read from the output, it is <span class="math inline">\(1.2\)</span> to <span class="math inline">\(1.9\)</span> mmHg / mg/day.</p>
<p>For women the effect of dose is given by the sum of two parameter estimates. If we say that <span class="math inline">\(\beta_1\)</span> is the mean effect of dose and <span class="math inline">\(\beta_3\)</span> is the interaction effect, then the estimated dose effect for women is given by <span class="math inline">\(\hat\beta_1+\hat\beta_3\)</span>. This can be written as
<span class="math display">\[
  \mb{c}^t \hat{\mb\beta}
\]</span>
with <span class="math inline">\(\hat{\mb\beta}^t=(\hat\beta_0, \hat\beta_1, \hat\beta_2, \hat\beta_3)\)</span> and with <span class="math inline">\(\mb{c}^t=(0,1,0,1)\)</span>.</p>
<p>As part of Theorem <a href="#thm:BLUE">2.3</a> we have seen that
<span class="math display">\[
  \var{\mb{c}^t\hat{\mb\beta}} = \mb{c}^t \mb\Sigma_\beta \mb{c}
\]</span>
with
<span class="math display">\[
  \mb\Sigma_\beta= \var{\hat{\mb\beta}} = (\mb{X}^t\mb{X})^{-1}\sigma^2.
\]</span>
This covariance matrix can be estimated by replacing <span class="math inline">\(\sigma^2\)</span> with MSE.</p>
<p>In R, there we find the estimated covariance matrix as follows. I also show the standard errors of the parameter estimates so that you can verify that these indeed agree with the <em>summary</em> output.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="#cb254-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)<span class="sc">$</span>cov<span class="sc">*</span><span class="fu">summary</span>(m)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>##             (Intercept)        dose     gender dose:gender
## (Intercept)   1.5755583 -0.20404426 -1.5755583  0.20404426
## dose         -0.2040443  0.04627808  0.2040443 -0.04627808
## gender       -1.5755583  0.20404426  3.4001588 -0.45024729
## dose:gender   0.2040443 -0.04627808 -0.4502473  0.10698568</code></pre>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="#cb256-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">summary</span>(m)<span class="sc">$</span>cov<span class="sc">*</span><span class="fu">summary</span>(m)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span>)) <span class="co"># standard errors</span></span></code></pre></div>
<pre><code>## (Intercept)        dose      gender dose:gender 
##   1.2552124   0.2151234   1.8439520   0.3270867</code></pre>
<p>The estimated variance of <span class="math inline">\(\hat\beta_1+\hat\beta_3\)</span> can thus be computed in R as follows.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="#cb258-1" aria-hidden="true" tabindex="-1"></a>Sigma.hat<span class="ot">&lt;-</span><span class="fu">summary</span>(m)<span class="sc">$</span>cov<span class="sc">*</span><span class="fu">summary</span>(m)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb258-2"><a href="#cb258-2" aria-hidden="true" tabindex="-1"></a>cvector<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">1</span>)</span>
<span id="cb258-3"><a href="#cb258-3" aria-hidden="true" tabindex="-1"></a>var.effect<span class="ot">&lt;-</span><span class="fu">t</span>(cvector)<span class="sc">%*%</span>Sigma.hat<span class="sc">%*%</span>cvector</span>
<span id="cb258-4"><a href="#cb258-4" aria-hidden="true" tabindex="-1"></a>var.effect</span></code></pre></div>
<pre><code>##           [,1]
## [1,] 0.0607076</code></pre>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(var.effect)</span></code></pre></div>
<pre><code>##           [,1]
## [1,] 0.2463891</code></pre>
<p>The standard error is thus given by SE = 0.25. With this SE we calculate the 95% confidence interval for the effect of dose for women as
<span class="math display">\[
  2.2 \pm SE \times t_{36,0.975}.
\]</span>
Thus, with a probability of 95% we expect the dose effect for woman to be in the interval from 1.7 to 2.7 mmHG / mg/day.</p>
</details>
</div>
<div id="example-lead-concentration-3" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We consider again the Lead Concentration example. This time we fit a model that also includes the age of the children. For simplicity we will assume that there are no interaction effects (see the next exercise in which you are asked to check this assumption).</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="#cb262-1" aria-hidden="true" tabindex="-1"></a>m.LdTotAge<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">+</span>Totyrs<span class="sc">+</span>Age, <span class="at">data=</span>lead)</span>
<span id="cb262-2"><a href="#cb262-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.LdTotAge)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs + Age, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.956  -4.460   1.028   5.633  16.187 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.91688    5.18388   6.543 5.52e-09 ***
## Ld73        -0.26067    0.09734  -2.678  0.00901 ** 
## Totyrs      -0.06031    0.38183  -0.158  0.87489    
## Age          2.64503    0.43826   6.035 4.86e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.506 on 79 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4765, Adjusted R-squared:  0.4566 
## F-statistic: 23.97 on 3 and 79 DF,  p-value: 3.961e-11</code></pre>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="#cb264-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m.LdTotAge)</span></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 23.5986229 44.23512971
## Ld73        -0.4544291 -0.06691981
## Totyrs      -0.8203220  0.69969630
## Age          1.7726961  3.51737026</code></pre>
<p>Some conclusions:</p>
<ul>
<li><p>This output shows a large effect size of age: on average <span class="math inline">\(2.65\)</span> (SE <span class="math inline">\(0.44\)</span>) taps per 10 seconds per extra year of age, controlling for blood lead levels and for time living near the smelter. This effect is significant at the 5% level of significance (<span class="math inline">\(p&lt;0.001\)</span>).</p></li>
<li><p>The effect size of years living near the smelter is very small: an average reduction of <span class="math inline">\(0.8\)</span> (SE <span class="math inline">\(0.7\)</span>) taps per 10 seconds per extra year living near the smelter, controlling for age and blood lead levels. This effect is not significant (<span class="math inline">\(p=0.87\)</span>) at all; there seems to be no additional effect of this regressor when controlling for age and blood lead levels.</p></li>
<li><p>The effect of blood lead levels is negative and significant (<span class="math inline">\(p=0.009\)</span>), with an estimated effect size of <span class="math inline">\(-0.26\)</span>, i.e. with each increase of 10 microgram/100ml we exect the mean FWT to decrease with <span class="math inline">\(2.6\)</span> taps per 10 seconds, while controlling for age and years living in the neighborhood of the smelter.</p></li>
</ul>
<p>For convenience, I also show the results of our previous analysis with only <em>Ld73</em> and <em>Totyrs</em> as regressors.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.LdTot)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Totyrs, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -40.665  -4.731  -0.464   7.645  26.638 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  55.3139     4.5428  12.176  &lt; 2e-16 ***
## Ld73         -0.4116     0.1130  -3.642 0.000478 ***
## Totyrs        1.3030     0.3698   3.524 0.000707 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.42 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.2351, Adjusted R-squared:  0.216 
## F-statistic: 12.29 on 2 and 80 DF,  p-value: 2.21e-05</code></pre>
<p>Comparing the two model fits, we see that the years living near the smelter was highly significant and with a large effect size when age was not in the model. After adding age to the model, the importance of <em>Totyrs</em> diminished. This may be explained by the positive correlation betwen <em>Totyrs</em> and <em>age</em>, which is equal to 0.66. This positive correlation makes sense: the older the child, the more chance there is that he/she lived for a longer time near the smelter. Thus age takes away some of the effect of <em>Totyrs</em>.</p>
</div>
<div id="exercise-lead-concentration-1" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Referring to the previous example, you are asked to assess whether there are interaction effects present. Note that with three regressors in the model, you should consider the following interacation effects:</p>
<ul>
<li><p>of two regressors (there are three such interaction effects). These are referred to as <strong>two-way interactions</strong></p></li>
<li><p>of three regressors (there is only a single three-way interaction). This is known as a <strong>three-way interaction</strong></p></li>
</ul>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<p>Adhering to the <strong>hierarchical modelling approach</strong>, we should start with the model with all interaction terms. This is the following model (illustrating an alternative indexing system for the parameters):
<span class="math display">\[
  Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_{1:2} x_{i1} x_{i2} + \beta_{1:3} x_{i1} x_{i3} +\beta_{2:3} x_{i2} x_{i3} + \beta_{1:2:3} x_{i1} x_{i2} x_{i3} + \eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="#cb268-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">*</span>Totyrs<span class="sc">*</span>Age, <span class="at">data=</span>lead)</span>
<span id="cb268-2"><a href="#cb268-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 * Totyrs * Age, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -34.042  -4.525   1.101   5.915  18.320 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)     28.808156  31.401213   0.917    0.362
## Ld73            -0.280911   1.038849  -0.270    0.788
## Totyrs           0.288832   4.632556   0.062    0.950
## Age              3.427570   2.899310   1.182    0.241
## Ld73:Totyrs      0.020753   0.153366   0.135    0.893
## Ld73:Age        -0.010893   0.099059  -0.110    0.913
## Totyrs:Age      -0.060295   0.380883  -0.158    0.875
## Ld73:Totyrs:Age -0.000541   0.012916  -0.042    0.967
## 
## Residual standard error: 9.715 on 75 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4809, Adjusted R-squared:  0.4324 
## F-statistic: 9.924 on 7 and 75 DF,  p-value: 1.051e-08</code></pre>
<p>From this output we read a very small estimate for the three-way interaction: <span class="math inline">\(\hat\beta_{1:2:3}=-0.0005\)</span>. This comes with <span class="math inline">\(p=0.967\)</span>, clearly indicating a non-significant effect at the 5% level of significance. There seems to be no evidence at all for a three-way interaction effect.</p>
<p>Before we can look at the two-way interactions, we have to remove the three-way interaction from the model and refit the new model.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="#cb270-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>(Ld73<span class="sc">+</span>Totyrs<span class="sc">+</span>Age)<span class="sc">^</span><span class="dv">2</span>, <span class="at">data=</span>lead)</span>
<span id="cb270-2"><a href="#cb270-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ (Ld73 + Totyrs + Age)^2, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -34.040  -4.544   1.112   5.952  18.304 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 27.64602   14.60797   1.893   0.0622 .
## Ld73        -0.23998    0.35017  -0.685   0.4952  
## Totyrs       0.46335    2.01155   0.230   0.8184  
## Age          3.53353    1.40709   2.511   0.0142 *
## Ld73:Totyrs  0.01462    0.04553   0.321   0.7490  
## Ld73:Age    -0.01467    0.04068  -0.361   0.7193  
## Totyrs:Age  -0.07550    0.11437  -0.660   0.5111  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.651 on 76 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4809, Adjusted R-squared:  0.4399 
## F-statistic: 11.73 on 6 and 76 DF,  p-value: 2.892e-09</code></pre>
<p>None of the two-way interaction effects appear to be important or significant at the 5% level; all <span class="math inline">\(p\)</span>-values are larger than <span class="math inline">\(0.5\)</span>. So the additive model we interpreted in the previous example was a correct model.</p>
</details>
</div>
<div id="the-anova-table" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> The ANOVA Table</h2>
<div id="sstot-ssr-and-sse" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> SSTot, SSR and SSE</h3>
<p>For the simple linear regression model we have discussed the decompositon of SStot into SSR and SSE in some detail. Here we extend this result to the multiple linear regression model.</p>
<p>The total sum of squares is defined as before (note that its defintion does not depend on a model),
<span class="math display">\[
  \SSTot = \sum_{i=1}^n (Y_i - \bar{Y})^2.
\]</span>
It is still a measure for the total variability in the observed sample outcomes. Also the residual sum of squares is defined as before:
<span class="math display">\[
  \SSE = \sum_{i=1}^n (Y_i-\hat{Y}_i)^2.
\]</span></p>
<p>Consider now a multiple linear regression model with <span class="math inline">\(p-1\)</span> regressors. Then the following decompositon still holds
<span class="math display">\[
  \SSTot = \SSR + \SSE ,
\]</span>
with
<span class="math display">\[
  \SSR = \sum_{i=1}^n (\hat{Y}_i-\bar{Y})^2.
\]</span>
The sum of squares of the regression can still be interpreted as the variability in the outcomes that can be explained by the regression model.</p>
<p>For the degrees of freedom (df) and the mean sum of squares it holds that:</p>
<ul>
<li><p>SSTot has <span class="math inline">\(n-1\)</span> df and <span class="math inline">\(\SSTot/(n-1)\)</span> is an estimator of the marginal variance of <span class="math inline">\(Y\)</span> (i.e. variance of the marginal outcome distribution).</p></li>
<li><p>SSE has <span class="math inline">\(n-p\)</span> df and <span class="math inline">\(\MSE=\SSE/(n-p)\)</span> is an estimator of the residual variance <span class="math inline">\(\sigma^2\)</span>, i.e. the variance of <span class="math inline">\(Y\)</span> given the regressors.</p></li>
<li><p>SSR has <span class="math inline">\(p-1\)</span> df and <span class="math inline">\(\MSR=\SSR/(p-1)\)</span> is the mean sum of squares of the regression.</p></li>
</ul>
<p>A consequence of the decomposition of SSTot is that the *coefficient of determination** remains defined as before, i.e. </p>
<p><span class="math display">\[
  R^2 = 1-\frac{\SSE}{\SSTot} = \frac{\SSR}{\SSTot}
\]</span>
is the proportion of the total outcome variability that can be explained by the regression model.</p>
</div>
</div>
<div id="multicollinearity" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Multicollinearity</h2>
</div>
<div id="example-lead-concentration-4" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We consider again the lead concentration example, but suppose that we are this time only interested in assessing the effect of the blood lead concentrations on the mean FWT. We have blood lead levels measured in 1972 and in 1973. It could make sense to include both in the model. We will do so, but first we will look at simple linear regression models with only <em>Ld72</em> and only <em>Ld73</em> in the model.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="#cb272-1" aria-hidden="true" tabindex="-1"></a>m.Ld72<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld72,<span class="at">data=</span>lead)</span>
<span id="cb272-2"><a href="#cb272-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.Ld72)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld72, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -39.821  -5.500   0.160   7.868  30.802 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  58.4812     2.9357  19.921   &lt;2e-16 ***
## Ld72         -0.1887     0.0761  -2.479   0.0152 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.51 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.07053,    Adjusted R-squared:  0.05905 
## F-statistic: 6.146 on 1 and 81 DF,  p-value: 0.01524</code></pre>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a>m.Ld73<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73,<span class="at">data=</span>lead)</span>
<span id="cb274-2"><a href="#cb274-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.Ld73)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.308  -5.808   0.993   7.584  30.661 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  64.3660     4.0020  16.083   &lt;2e-16 ***
## Ld73         -0.3938     0.1206  -3.266   0.0016 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.2 on 81 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1164, Adjusted R-squared:  0.1055 
## F-statistic: 10.67 on 1 and 81 DF,  p-value: 0.0016</code></pre>
<p>And now the model with both blood lead levels.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="#cb276-1" aria-hidden="true" tabindex="-1"></a>m.Ld7273<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">+</span>Ld72,<span class="at">data=</span>lead)</span>
<span id="cb276-2"><a href="#cb276-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.Ld7273)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Ld73 + Ld72, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -42.204  -5.713   1.214   7.558  30.624 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 64.29882    4.04956  15.878   &lt;2e-16 ***
## Ld73        -0.37270    0.18240  -2.043   0.0443 *  
## Ld72        -0.01741    0.11224  -0.155   0.8771    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.27 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.1166, Adjusted R-squared:  0.09455 
## F-statistic: 5.281 on 2 and 80 DF,  p-value: 0.007009</code></pre>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="#cb278-1" aria-hidden="true" tabindex="-1"></a><span class="co">#m&lt;-lm(MAXFWT~Ld73*Ld72,data=lead)</span></span>
<span id="cb278-2"><a href="#cb278-2" aria-hidden="true" tabindex="-1"></a><span class="co">#summary(m)</span></span></code></pre></div>
<p>These analyses show us (at the 5% level of significance):</p>
<ul>
<li><p>a significant negative effect of <em>Ld72</em> (<span class="math inline">\(p=0.0152\)</span>)</p></li>
<li><p>a significant negative effect of <em>Ld73</em> (<span class="math inline">\(p=0.0016\)</span>)</p></li>
<li><p>a significant negative effect of <em>Ld73</em> when controlling for <em>Ld72</em> (<span class="math inline">\(p=0.0443\)</span>). Note that this <span class="math inline">\(p\)</span>-value is larger than in the model with only <em>Ld73</em>.</p></li>
<li><p>a non-significant negative effect of <em>Ld72</em> when controlling for <em>Ld73</em> (<span class="math inline">\(p=0.8771\)</span>).</p></li>
</ul>
<p>The increase in <span class="math inline">\(p\)</span>-values in the model with both <em>Ld72</em> and <em>Ld73</em> can at least be partly attributed to a decrease of the estimated effect sizes as compared to their effect size estimated in the simple linear regression models. Intuitively, this can be understood as follows: we may expect that a child’s blood lead level does not change very much from year to year. Thus, when we start with a model that includes <em>Ld72</em> as a regressor, and we add <em>Ld73</em> to it, then there is not much information in <em>Ld73</em> added to the regression model. In other words, when <em>Ld72</em> and <em>Ld73</em> share a lot of information about the mean FWD, then their total effect will be split into two smaller effect sizes as compared to their effects in their simple linear regression models. I used the term <em>sharing information</em>. This can, at least to some extent, be translated in correlation between the two regressors.</p>
<p>Another observation from the outputs of the three regression model fits, is that the standard errors of the effects are larger in the multiple regression model as compared to the corresponding standard errors in the simple regression models. Increased standard errors may also (partly) explain the increase in <span class="math inline">\(p\)</span>-values.</p>
<p>Let us look at the correlation between <em>Ld72</em> and <em>Ld73</em>. To make a fair assessment, we will limit the computation to those cases for which the outcome <em>MAXFWT</em> was observed.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="#cb279-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead[indNA,<span class="fu">c</span>(<span class="st">&quot;Ld72&quot;</span>,<span class="st">&quot;Ld73&quot;</span>)])</span></code></pre></div>
<pre><code>##           Ld72      Ld73
## Ld72 1.0000000 0.7467369
## Ld73 0.7467369 1.0000000</code></pre>
<p>The Pearson correlation is thus estimated as <span class="math inline">\(0.747\)</span>, which is clearly indicating a moderately strong positive correlation between the blood lead concentrations measured in 1972 and 1973.</p>
<p>Next we present a visual inspection.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="#cb281-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(lead[indNA,<span class="fu">c</span>(<span class="dv">10</span>,<span class="dv">7</span>,<span class="dv">8</span>)],</span>
<span id="cb281-2"><a href="#cb281-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">progress=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>The problem that we just described is the problem of <strong>multicollinearity</strong>: increased standard errors of estimates of regression coefficients due to strong correlation between regressors in the sample data. This increase in standard error (increased imprecision of parameter estimates) causes the power of statistical tests to decrease (larger <span class="math inline">\(p\)</span>-values) and the confidence intervals to become wider.</p>
<p>To better understand the issue, we look at the regression model with centered regressors (we have already demonstrated that centering has no effect on the parameter estimates, except for the intercept). Consider the model
<span class="math display">\[\begin{equation*}
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \eps_ i
\end{equation*}\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>,
in which the sample means of <span class="math inline">\(x_{i1}\)</span> and <span class="math inline">\(x_{i2}\)</span> are equal to zero.</p>
<p>The parameter estimates are given by
<span class="math display">\[
  \hat{\mb\beta}=(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y}
\]</span>
with the <span class="math inline">\(n\times 3\)</span> design matrix
<span class="math display">\[
 \mb{X}=\begin{pmatrix}
   1 &amp; x_{11} &amp; x_{12}  \\
   1 &amp; x_{21} &amp; x_{22}  \\
   \vdots &amp; \vdots &amp; \vdots \\
   1 &amp; x_{n1} &amp; x_{n2} \end{pmatrix}.
 \]</span>
The matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> is thus proportional to the correlation matrix of the columns of <span class="math inline">\(\mb{X}\)</span>. For example, the element on position <span class="math inline">\((2,3)\)</span> is
<span class="math display">\[
   \sum_{i=1}^n x_{i1}x_{i2} = \sum_{i=1}^n (x_{i1}-\bar{x}_1)(x_{i2}-\bar{x}_2)  .
 \]</span></p>
<p>The variance of <span class="math inline">\(\hat{\mb\beta}\)</span> is given by
<span class="math display">\[
   \var{\hat{\mb{\beta}}}=(\mb{X}^t\mb{X})^{-1}\sigma^2.
 \]</span>
The matrix <span class="math inline">\(\mb{X}^t\mb{X}\)</span> thus plays an important role.</p>
<div id="illustrations-via-simulations" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Illustrations via Simulations</h3>
<p>The next chunck of R code illustrates via a simulation study the behaviour of estimators of the <span class="math inline">\(\beta\)</span>-parameters for a (random) design with <span class="math inline">\(n=50\)</span> observations with uncorrelated regressors (<span class="math inline">\(p-1=2\)</span>).</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="#cb282-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">87561</span>)</span>
<span id="cb282-2"><a href="#cb282-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 </span>
<span id="cb282-3"><a href="#cb282-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">50</span> <span class="co"># sample size</span></span>
<span id="cb282-4"><a href="#cb282-4" aria-hidden="true" tabindex="-1"></a>beta0<span class="ot">&lt;-</span><span class="dv">50</span></span>
<span id="cb282-5"><a href="#cb282-5" aria-hidden="true" tabindex="-1"></a>beta1<span class="ot">&lt;-</span><span class="dv">5</span></span>
<span id="cb282-6"><a href="#cb282-6" aria-hidden="true" tabindex="-1"></a>beta2<span class="ot">&lt;-</span><span class="dv">8</span></span>
<span id="cb282-7"><a href="#cb282-7" aria-hidden="true" tabindex="-1"></a>sigma<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># residual standard deviation</span></span>
<span id="cb282-8"><a href="#cb282-8" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="dv">0</span> <span class="co"># correlation between the regressors</span></span>
<span id="cb282-9"><a href="#cb282-9" aria-hidden="true" tabindex="-1"></a>var.beta<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="dv">0</span>,<span class="at">nrow=</span><span class="dv">3</span>,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb282-10"><a href="#cb282-10" aria-hidden="true" tabindex="-1"></a>beta.hat<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb282-11"><a href="#cb282-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb282-12"><a href="#cb282-12" aria-hidden="true" tabindex="-1"></a>    x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n, <span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">2</span>, <span class="at">byrow=</span>T))</span>
<span id="cb282-13"><a href="#cb282-13" aria-hidden="true" tabindex="-1"></a>    x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb282-14"><a href="#cb282-14" aria-hidden="true" tabindex="-1"></a>    x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb282-15"><a href="#cb282-15" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb282-16"><a href="#cb282-16" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2,<span class="at">x=</span>T) </span>
<span id="cb282-17"><a href="#cb282-17" aria-hidden="true" tabindex="-1"></a>    X<span class="ot">&lt;-</span>m<span class="sc">$</span>x</span>
<span id="cb282-18"><a href="#cb282-18" aria-hidden="true" tabindex="-1"></a>    var.beta<span class="ot">&lt;-</span>var.beta<span class="sc">+</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>  </span>
<span id="cb282-19"><a href="#cb282-19" aria-hidden="true" tabindex="-1"></a>    beta.hat[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb282-20"><a href="#cb282-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb282-21"><a href="#cb282-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb282-22"><a href="#cb282-22" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></span></code></pre></div>
<pre><code>## [1] 49.99716  5.00103  8.00034</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(var.beta<span class="sc">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0010 -0.0004
## x1               0.0010  0.1364 -0.0002
## x2              -0.0004 -0.0002  0.1359</code></pre>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]
## [1,]  0.511119882  0.004228924 -0.003136406
## [2,]  0.004228924  0.135666885 -0.001201206
## [3,] -0.003136406 -0.001201206  0.137048412</code></pre>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             [,1]         [,2]         [,3]
## [1,]  1.00000000  0.016059485 -0.011850425
## [2,]  0.01605948  1.000000000 -0.008809345
## [3,] -0.01185043 -0.008809345  1.000000000</code></pre>
<p>This simulation study shows that</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 0.1357</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 0.137</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx 0\)</span></p></li>
</ul>
<p>We repeat the simulation study a few times, with as only difference that now the correlation (<span class="math inline">\(\rho\)</span>) between the two regressors is set to <span class="math inline">\(\rho=0.5\)</span>, <span class="math inline">\(\rho=0.9\)</span> and <span class="math inline">\(\rho=0.99\)</span>.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">87561</span>)</span>
<span id="cb290-2"><a href="#cb290-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 </span>
<span id="cb290-3"><a href="#cb290-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">50</span> </span>
<span id="cb290-4"><a href="#cb290-4" aria-hidden="true" tabindex="-1"></a>beta0<span class="ot">&lt;-</span><span class="dv">50</span></span>
<span id="cb290-5"><a href="#cb290-5" aria-hidden="true" tabindex="-1"></a>beta1<span class="ot">&lt;-</span><span class="dv">5</span></span>
<span id="cb290-6"><a href="#cb290-6" aria-hidden="true" tabindex="-1"></a>beta2<span class="ot">&lt;-</span><span class="dv">8</span></span>
<span id="cb290-7"><a href="#cb290-7" aria-hidden="true" tabindex="-1"></a>sigma<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># residuel standard deviation</span></span>
<span id="cb290-8"><a href="#cb290-8" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.5</span></span>
<span id="cb290-9"><a href="#cb290-9" aria-hidden="true" tabindex="-1"></a>var.beta<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="dv">0</span>,<span class="at">nrow=</span><span class="dv">3</span>,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb290-10"><a href="#cb290-10" aria-hidden="true" tabindex="-1"></a>beta.hat<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb290-11"><a href="#cb290-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb290-12"><a href="#cb290-12" aria-hidden="true" tabindex="-1"></a>    x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n, <span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">2</span>, <span class="at">byrow=</span>T))</span>
<span id="cb290-13"><a href="#cb290-13" aria-hidden="true" tabindex="-1"></a>    x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb290-14"><a href="#cb290-14" aria-hidden="true" tabindex="-1"></a>    x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb290-15"><a href="#cb290-15" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb290-16"><a href="#cb290-16" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2,<span class="at">x=</span>T) </span>
<span id="cb290-17"><a href="#cb290-17" aria-hidden="true" tabindex="-1"></a>    X<span class="ot">&lt;-</span>m<span class="sc">$</span>x</span>
<span id="cb290-18"><a href="#cb290-18" aria-hidden="true" tabindex="-1"></a>    var.beta<span class="ot">&lt;-</span>var.beta<span class="sc">+</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>  </span>
<span id="cb290-19"><a href="#cb290-19" aria-hidden="true" tabindex="-1"></a>    beta.hat[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb290-20"><a href="#cb290-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb290-21"><a href="#cb290-21" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></span></code></pre></div>
<pre><code>## [1] 49.997155  5.001227  7.999166</code></pre>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="#cb292-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(var.beta<span class="sc">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0007 -0.0012
## x1               0.0007  0.1815 -0.0911
## x2              -0.0012 -0.0911  0.1819</code></pre>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##              [,1]         [,2]         [,3]
## [1,]  0.511119882  0.002418119 -0.006039729
## [2,]  0.002418119  0.179962655 -0.089984081
## [3,] -0.006039729 -0.089984081  0.182736722</code></pre>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="#cb296-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></span></code></pre></div>
<pre><code>##              [,1]         [,2]        [,3]
## [1,]  1.000000000  0.007973062 -0.01976256
## [2,]  0.007973062  1.000000000 -0.49620550
## [3,] -0.019762559 -0.496205501  1.00000000</code></pre>
<p>For <span class="math inline">\(\rho=0.5\)</span> we find</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 0.18</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 0.1827</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx\)</span> -0.496</p></li>
</ul>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">87561</span>)</span>
<span id="cb298-2"><a href="#cb298-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 </span>
<span id="cb298-3"><a href="#cb298-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">50</span> </span>
<span id="cb298-4"><a href="#cb298-4" aria-hidden="true" tabindex="-1"></a>beta0<span class="ot">&lt;-</span><span class="dv">50</span></span>
<span id="cb298-5"><a href="#cb298-5" aria-hidden="true" tabindex="-1"></a>beta1<span class="ot">&lt;-</span><span class="dv">5</span></span>
<span id="cb298-6"><a href="#cb298-6" aria-hidden="true" tabindex="-1"></a>beta2<span class="ot">&lt;-</span><span class="dv">8</span></span>
<span id="cb298-7"><a href="#cb298-7" aria-hidden="true" tabindex="-1"></a>sigma<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># residuel standard deviation</span></span>
<span id="cb298-8"><a href="#cb298-8" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.9</span></span>
<span id="cb298-9"><a href="#cb298-9" aria-hidden="true" tabindex="-1"></a>var.beta<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="dv">0</span>,<span class="at">nrow=</span><span class="dv">3</span>,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb298-10"><a href="#cb298-10" aria-hidden="true" tabindex="-1"></a>beta.hat<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb298-11"><a href="#cb298-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb298-12"><a href="#cb298-12" aria-hidden="true" tabindex="-1"></a>    x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n, <span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">2</span>, <span class="at">byrow=</span>T))</span>
<span id="cb298-13"><a href="#cb298-13" aria-hidden="true" tabindex="-1"></a>    x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb298-14"><a href="#cb298-14" aria-hidden="true" tabindex="-1"></a>    x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb298-15"><a href="#cb298-15" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb298-16"><a href="#cb298-16" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2,<span class="at">x=</span>T) </span>
<span id="cb298-17"><a href="#cb298-17" aria-hidden="true" tabindex="-1"></a>    X<span class="ot">&lt;-</span>m<span class="sc">$</span>x</span>
<span id="cb298-18"><a href="#cb298-18" aria-hidden="true" tabindex="-1"></a>    var.beta<span class="ot">&lt;-</span>var.beta<span class="sc">+</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>  </span>
<span id="cb298-19"><a href="#cb298-19" aria-hidden="true" tabindex="-1"></a>    beta.hat[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb298-20"><a href="#cb298-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb298-21"><a href="#cb298-21" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></span></code></pre></div>
<pre><code>## [1] 49.997155  5.002479  7.997870</code></pre>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(var.beta<span class="sc">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0019 -0.0024
## x1               0.0019  0.7173 -0.6462
## x2              -0.0024 -0.6462  0.7181</code></pre>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  0.51111988  0.00784722 -0.01106511
## [2,]  0.00784722  0.71164404 -0.64226905
## [3,] -0.01106511 -0.64226905  0.71715555</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="#cb304-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  1.00000000  0.01301137 -0.01827627
## [2,]  0.01301137  1.00000000 -0.89903977
## [3,] -0.01827627 -0.89903977  1.00000000</code></pre>
<p>For <span class="math inline">\(\rho=0.9\)</span> we find</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 0.7116</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 0.7172</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx\)</span> -0.899</p></li>
</ul>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">87561</span>)</span>
<span id="cb306-2"><a href="#cb306-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000 </span>
<span id="cb306-3"><a href="#cb306-3" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span><span class="dv">50</span> </span>
<span id="cb306-4"><a href="#cb306-4" aria-hidden="true" tabindex="-1"></a>beta0<span class="ot">&lt;-</span><span class="dv">50</span></span>
<span id="cb306-5"><a href="#cb306-5" aria-hidden="true" tabindex="-1"></a>beta1<span class="ot">&lt;-</span><span class="dv">5</span></span>
<span id="cb306-6"><a href="#cb306-6" aria-hidden="true" tabindex="-1"></a>beta2<span class="ot">&lt;-</span><span class="dv">8</span></span>
<span id="cb306-7"><a href="#cb306-7" aria-hidden="true" tabindex="-1"></a>sigma<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># residuel standard deviation</span></span>
<span id="cb306-8"><a href="#cb306-8" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.99</span></span>
<span id="cb306-9"><a href="#cb306-9" aria-hidden="true" tabindex="-1"></a>var.beta<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="dv">0</span>,<span class="at">nrow=</span><span class="dv">3</span>,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb306-10"><a href="#cb306-10" aria-hidden="true" tabindex="-1"></a>beta.hat<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb306-11"><a href="#cb306-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb306-12"><a href="#cb306-12" aria-hidden="true" tabindex="-1"></a>    x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n, <span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>), <span class="at">ncol=</span><span class="dv">2</span>, <span class="at">byrow=</span>T))</span>
<span id="cb306-13"><a href="#cb306-13" aria-hidden="true" tabindex="-1"></a>    x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb306-14"><a href="#cb306-14" aria-hidden="true" tabindex="-1"></a>    x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb306-15"><a href="#cb306-15" aria-hidden="true" tabindex="-1"></a>    y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb306-16"><a href="#cb306-16" aria-hidden="true" tabindex="-1"></a>    m<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2,<span class="at">x=</span>T) </span>
<span id="cb306-17"><a href="#cb306-17" aria-hidden="true" tabindex="-1"></a>    X<span class="ot">&lt;-</span>m<span class="sc">$</span>x</span>
<span id="cb306-18"><a href="#cb306-18" aria-hidden="true" tabindex="-1"></a>    var.beta<span class="ot">&lt;-</span>var.beta<span class="sc">+</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">*</span>sigma<span class="sc">^</span><span class="dv">2</span>  </span>
<span id="cb306-19"><a href="#cb306-19" aria-hidden="true" tabindex="-1"></a>    beta.hat[i,]<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb306-20"><a href="#cb306-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb306-21"><a href="#cb306-21" aria-hidden="true" tabindex="-1"></a><span class="fu">colMeans</span>(beta.hat) <span class="co"># approximation of expectation of par. estimators</span></span></code></pre></div>
<pre><code>## [1] 49.997155  5.007457  7.992884</code></pre>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(var.beta<span class="sc">/</span>N,<span class="dv">4</span>) <span class="co"># approximation of expection of the estimated covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             (Intercept)      x1      x2
## (Intercept)      0.5220  0.0066 -0.0071
## x1               0.0066  6.8522 -6.7852
## x2              -0.0071 -6.7852  6.8548</code></pre>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(beta.hat) <span class="co"># approximation of covariance matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  0.51111988  0.02833087 -0.03147515
## [2,]  0.02833087  6.80926340 -6.74890998
## [3,] -0.03147515 -6.74890998  6.82629365</code></pre>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(beta.hat) <span class="co"># correlation matrix of beta.hat</span></span></code></pre></div>
<pre><code>##             [,1]        [,2]        [,3]
## [1,]  1.00000000  0.01518618 -0.01685054
## [2,]  0.01518618  1.00000000 -0.98989945
## [3,] -0.01685054 -0.98989945  1.00000000</code></pre>
<p>For <span class="math inline">\(\rho=0.99\)</span> we find</p>
<ul>
<li><p><span class="math inline">\(\var{\hat\beta_1}\approx\)</span> 6.8093</p></li>
<li><p><span class="math inline">\(\var{\hat\beta_2}\approx\)</span> 6.8263</p></li>
<li><p><span class="math inline">\(\cor{\hat\beta_1,\hat\beta_2} \approx\)</span> -0.99</p></li>
</ul>
<p>Based on the simulation studies we conclude (for a fixed sample size):</p>
<ul>
<li><p>The smallest variance on parameter estimates is obtained for uncorrelated regressors</p></li>
<li><p>As the correlation between regressors increases, the variances of the parameter estimators increase</p></li>
<li><p>As the correlation between regressors increases, the correlation between the parameter estimators increases</p></li>
<li><p>For extremely large correlations between regressors (e.g. <span class="math inline">\(\rho=0.99\)</span>), we get extremely large variances of the parameter estimators.</p></li>
</ul>
<p>Figures <a href="#fig:MultCol1">3.3</a> and <a href="#fig:MultCol2">3.4</a> show three-dimensional scatter plots of simulated data, once with <span class="math inline">\(\rho=0\)</span> and once with <span class="math inline">\(\rho=0.99\)</span>. For the former, you can easily imagine a flat plane fitting through the cloud of points (fitted additive model), whereas for the latter it is not obvious where to imagine the plane. There appear to by many possible possitions of a plane going through the cloud of points, and it is not clear which plane would fit best. This <em>uncertainty</em> is an expression of the <em>imprecision</em> of the parameter estimates!</p>
<div class="figure"><span style="display:block;" id="fig:MultCol1"></span>
<div id="htmlwidget-b2394f1ddcfd786ac0ae" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-b2394f1ddcfd786ac0ae">{"x":{"visdat":{"35116dc89602":["function () ","plotlyVisDat"]},"cur_data":"35116dc89602","attrs":{"35116dc89602":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[2.3784652744587,0.975306324484654,-1.75932887084459,0.56120795081297,-1.06641441784728,0.515762365043503,3.19497495747883,-0.0878108473371359,4.04642500954806,-1.85011961378265,0.599690530596075,0.894149099029415,2.79238880356175,3.45837574589989,0.261066881570417,2.17179585624807,0.603943696726338,0.238607297490702,-0.171132534114863,-0.215393214593134,2.25487595274371,3.84778571350042,-2.30858746438483,-3.8535832999874,-1.72165658839257,-1.33160899041643,0.280877105734206,0.480295850948022,-1.91150964475358,0.560013947676214,-0.526408699498642,-0.670832805407232,-2.55174043715131,1.02821739535656,2.02373735638644,-0.535123158644984,-1.12619381381626,-2.41102354949056,-0.435917654330441,1.36319456059915,1.05597031690358,-2.3084051170794,-1.6348764839262,1.38590843525492,-1.28551384203373,2.41196471174138,0.176488181312493,-0.669540511046102,-2.48723031007246,1.24369444780231],"y":[1.00125550602661,-0.211805916360858,2.58441855506921,1.55965821947807,2.06433400087722,2.12646237246761,0.0956248386996567,-1.53709013097185,-3.513577146329,2.64095637157618,-2.51077636343789,-1.33714688323111,-0.855988778030817,0.96924687445716,-1.27479351227566,-0.89830420208184,0.400991269512732,1.34683882098077,1.0049387600799,-1.07773519836634,1.22382192327357,-1.46564793821972,2.92240529155838,0.835711709966758,0.407240483890394,2.88490637906331,-1.43466602440795,0.0894101480203718,-2.54977837337156,0.961856831424946,5.65686497040948,1.62624325727816,-0.178464212393518,3.30742666885604,-1.64627568395074,1.35911377230422,-3.3138758284828,-1.38060824124647,-0.759670294333982,0.614691460500778,1.89835632422177,-0.961910332544514,1.6756699006572,-0.72790626707619,1.19391889361652,1.57159288321511,-0.4074751561433,1.13733937868962,0.45941043536032,-2.27549494595085],"z":[70.2116805840652,64.3698789442331,64.2448370069937,66.3907691914361,64.0013297792731,68.4094607059508,64.3355860944441,29.6129487212036,50.1593392518925,66.0518921304644,32.6476453113268,39.5944560247736,55.5027225050266,79.4537885615799,37.6782299735153,49.326333884117,54.1455706250611,56.3748596085635,58.5385522211463,33.9802167447203,69.7250141967217,53.6944099528347,70.7519939508014,33.5364879454405,40.4517541262869,62.2775312778761,42.7651083511825,44.6638314930381,25.068414788667,62.6473335512159,86.032722400514,50.5736756524326,36.673249115175,83.4921666376461,40.8898997083404,53.4586301669799,23.3981103965939,29.1602256098246,37.3074114437587,52.9325988805186,72.9435922593982,29.6897407289283,58.9699990586283,51.0777085172045,50.8156838978685,76.7760231762726,40.1263188993674,56.008971331872,41.6645650162946,34.706709331544],"type":"scatter3d","mode":"markers","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[2.3784652744587,0.975306324484654,-1.75932887084459,0.56120795081297,-1.06641441784728,0.515762365043503,3.19497495747883,-0.0878108473371359,4.04642500954806,-1.85011961378265,0.599690530596075,0.894149099029415,2.79238880356175,3.45837574589989,0.261066881570417,2.17179585624807,0.603943696726338,0.238607297490702,-0.171132534114863,-0.215393214593134,2.25487595274371,3.84778571350042,-2.30858746438483,-3.8535832999874,-1.72165658839257,-1.33160899041643,0.280877105734206,0.480295850948022,-1.91150964475358,0.560013947676214,-0.526408699498642,-0.670832805407232,-2.55174043715131,1.02821739535656,2.02373735638644,-0.535123158644984,-1.12619381381626,-2.41102354949056,-0.435917654330441,1.36319456059915,1.05597031690358,-2.3084051170794,-1.6348764839262,1.38590843525492,-1.28551384203373,2.41196471174138,0.176488181312493,-0.669540511046102,-2.48723031007246,1.24369444780231],"y":[1.00125550602661,-0.211805916360858,2.58441855506921,1.55965821947807,2.06433400087722,2.12646237246761,0.0956248386996567,-1.53709013097185,-3.513577146329,2.64095637157618,-2.51077636343789,-1.33714688323111,-0.855988778030817,0.96924687445716,-1.27479351227566,-0.89830420208184,0.400991269512732,1.34683882098077,1.0049387600799,-1.07773519836634,1.22382192327357,-1.46564793821972,2.92240529155838,0.835711709966758,0.407240483890394,2.88490637906331,-1.43466602440795,0.0894101480203718,-2.54977837337156,0.961856831424946,5.65686497040948,1.62624325727816,-0.178464212393518,3.30742666885604,-1.64627568395074,1.35911377230422,-3.3138758284828,-1.38060824124647,-0.759670294333982,0.614691460500778,1.89835632422177,-0.961910332544514,1.6756699006572,-0.72790626707619,1.19391889361652,1.57159288321511,-0.4074751561433,1.13733937868962,0.45941043536032,-2.27549494595085],"z":[70.2116805840652,64.3698789442331,64.2448370069937,66.3907691914361,64.0013297792731,68.4094607059508,64.3355860944441,29.6129487212036,50.1593392518925,66.0518921304644,32.6476453113268,39.5944560247736,55.5027225050266,79.4537885615799,37.6782299735153,49.326333884117,54.1455706250611,56.3748596085635,58.5385522211463,33.9802167447203,69.7250141967217,53.6944099528347,70.7519939508014,33.5364879454405,40.4517541262869,62.2775312778761,42.7651083511825,44.6638314930381,25.068414788667,62.6473335512159,86.032722400514,50.5736756524326,36.673249115175,83.4921666376461,40.8898997083404,53.4586301669799,23.3981103965939,29.1602256098246,37.3074114437587,52.9325988805186,72.9435922593982,29.6897407289283,58.9699990586283,51.0777085172045,50.8156838978685,76.7760231762726,40.1263188993674,56.008971331872,41.6645650162946,34.706709331544],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.3: Three-dimensional scatter plot of simulated data with <span class="math inline">\(\rho=0\)</span> correlation between the the regressors.
</p>
</div>
<div class="figure"><span style="display:block;" id="fig:MultCol2"></span>
<div id="htmlwidget-fd4c5f9ab2dd9b358131" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-fd4c5f9ab2dd9b358131">{"x":{"visdat":{"35115fee7c8a":["function () ","plotlyVisDat"]},"cur_data":"35115fee7c8a","attrs":{"35115fee7c8a":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-3.09872215610821,-1.64129656238572,-1.82545967761885,1.04013045844297,2.35589256747558,-2.39078762394887,-1.78057454556649,0.55453683191285,3.72595916393503,-1.14620834488975,-0.321805999813256,-0.0670146877780076,-1.93445661148332,-0.311494715669063,-1.20334251985868,-0.345610562150516,-1.63038085799465,0.67158578941615,-3.31040491183251,-1.20415778061489,1.33085670508721,-0.233731606450318,-0.628268881239662,-1.5821323624261,0.866833094642294,2.5796219832329,-0.660324485637313,-0.674396919654675,-1.24127037514036,-2.5115528566554,3.06500538578505,2.30769676977944,-1.33976217595996,0.199096049184456,-1.86776891847088,-1.53183576843479,-1.31894690235573,-2.26175891694285,4.23714582883521,-0.151067445546016,-0.437748742927344,-0.0212279116568852,0.363064776851925,0.861552510626639,1.17455134658848,-2.78523044648607,3.65255366551743,4.394606072703,-1.08898731313567,-0.852906102921425],"y":[-3.42263787207453,-1.3452189076172,-2.18610897421636,0.998719228499312,2.28998614960951,-2.14358567270772,-1.89812303198702,0.478724603050485,3.81529332701646,-0.657500634153499,-0.292874516027754,0.227364822853622,-1.96737241504492,-1.35135794006987,-1.34703429360294,-0.431633537026034,-1.84656082039435,0.470456927460059,-3.86543900020167,-1.28744604176395,1.40227862841431,-0.00670567117326714,-0.499454099685769,-1.84018091220123,0.417053234234442,2.10570908381666,-0.231721201764698,-1.15461660474536,-1.20022081581562,-2.46766668691505,3.05954564180638,2.33389456171531,-1.97378977870937,0.00955425415240762,-1.70211175673326,-1.56875360774967,-1.20616019716984,-1.92634837600345,4.13990651066453,0.0627625341693089,-0.413770623553748,0.205469024041063,0.560552484140795,1.21954915142408,1.20463914574852,-2.76663986809218,3.55075844081584,4.65395963919026,-1.32295660818761,-0.622345371956771],"z":[10.5067119960926,32.077626248713,23.4146623581262,66.1356938986824,86.907220662261,21.5699213440957,20.7917176302361,57.3834309739898,102.342738524578,31.3856595581194,48.7997920044648,58.2248657403736,28.2357551125701,31.6078677258177,34.7362025397372,51.8348453867013,25.7765728508824,56.7914583452152,-0.259082891828114,26.5823391725765,77.6552215409888,53.2768317590813,40.3559416492888,29.0364453042167,50.0997216159591,86.7121339239354,48.7457185154846,39.8969407787656,28.7398355455262,19.1737007313698,91.0804249724727,80.3818411480242,24.0701472031116,47.0614462189598,35.6133009817934,30.9569946476442,24.9878660664894,17.9414340685221,99.6349527179906,52.179276992885,36.6122734318752,50.287528871461,55.3718849215839,62.0846224165965,64.3736954907903,18.8296384153987,98.1352368794012,106.752324929253,35.5940912679471,43.9812424912553],"type":"scatter3d","mode":"markers","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[-3.09872215610821,-1.64129656238572,-1.82545967761885,1.04013045844297,2.35589256747558,-2.39078762394887,-1.78057454556649,0.55453683191285,3.72595916393503,-1.14620834488975,-0.321805999813256,-0.0670146877780076,-1.93445661148332,-0.311494715669063,-1.20334251985868,-0.345610562150516,-1.63038085799465,0.67158578941615,-3.31040491183251,-1.20415778061489,1.33085670508721,-0.233731606450318,-0.628268881239662,-1.5821323624261,0.866833094642294,2.5796219832329,-0.660324485637313,-0.674396919654675,-1.24127037514036,-2.5115528566554,3.06500538578505,2.30769676977944,-1.33976217595996,0.199096049184456,-1.86776891847088,-1.53183576843479,-1.31894690235573,-2.26175891694285,4.23714582883521,-0.151067445546016,-0.437748742927344,-0.0212279116568852,0.363064776851925,0.861552510626639,1.17455134658848,-2.78523044648607,3.65255366551743,4.394606072703,-1.08898731313567,-0.852906102921425],"y":[-3.42263787207453,-1.3452189076172,-2.18610897421636,0.998719228499312,2.28998614960951,-2.14358567270772,-1.89812303198702,0.478724603050485,3.81529332701646,-0.657500634153499,-0.292874516027754,0.227364822853622,-1.96737241504492,-1.35135794006987,-1.34703429360294,-0.431633537026034,-1.84656082039435,0.470456927460059,-3.86543900020167,-1.28744604176395,1.40227862841431,-0.00670567117326714,-0.499454099685769,-1.84018091220123,0.417053234234442,2.10570908381666,-0.231721201764698,-1.15461660474536,-1.20022081581562,-2.46766668691505,3.05954564180638,2.33389456171531,-1.97378977870937,0.00955425415240762,-1.70211175673326,-1.56875360774967,-1.20616019716984,-1.92634837600345,4.13990651066453,0.0627625341693089,-0.413770623553748,0.205469024041063,0.560552484140795,1.21954915142408,1.20463914574852,-2.76663986809218,3.55075844081584,4.65395963919026,-1.32295660818761,-0.622345371956771],"z":[10.5067119960926,32.077626248713,23.4146623581262,66.1356938986824,86.907220662261,21.5699213440957,20.7917176302361,57.3834309739898,102.342738524578,31.3856595581194,48.7997920044648,58.2248657403736,28.2357551125701,31.6078677258177,34.7362025397372,51.8348453867013,25.7765728508824,56.7914583452152,-0.259082891828114,26.5823391725765,77.6552215409888,53.2768317590813,40.3559416492888,29.0364453042167,50.0997216159591,86.7121339239354,48.7457185154846,39.8969407787656,28.7398355455262,19.1737007313698,91.0804249724727,80.3818411480242,24.0701472031116,47.0614462189598,35.6133009817934,30.9569946476442,24.9878660664894,17.9414340685221,99.6349527179906,52.179276992885,36.6122734318752,50.287528871461,55.3718849215839,62.0846224165965,64.3736954907903,18.8296384153987,98.1352368794012,106.752324929253,35.5940912679471,43.9812424912553],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 3.4: Three-dimensional scatter plot of simulated data with <span class="math inline">\(\rho=0.99\)</span> correlation between the the regressors.
</p>
</div>
</div>
</div>
<div id="excercise-repeatedly-fitting-model-with-rho0.99" class="section level2 unnumbered">
<h2>Excercise: repeatedly fitting model with <span class="math inline">\(\rho=0.99\)</span></h2>
<p>Repeat the simulation with <span class="math inline">\(\rho=0.99\)</span> a few times. Each time make the three dimensional scatter plot and add the fitted additive regression model. What do you see? What do you conclude?</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<p>I will use the same R code as used for the previous graph (<span class="math inline">\(\rho=0.99\)</span>). The code is repeated a few times, each time the simulated data and the fitted regression model are shown. Also the parameter estimates are listed.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="#cb314-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">85723</span>)</span>
<span id="cb314-2"><a href="#cb314-2" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.99</span></span>
<span id="cb314-3"><a href="#cb314-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n,<span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span>T))</span>
<span id="cb314-4"><a href="#cb314-4" aria-hidden="true" tabindex="-1"></a>x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb314-5"><a href="#cb314-5" aria-hidden="true" tabindex="-1"></a>x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb314-6"><a href="#cb314-6" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb314-7"><a href="#cb314-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-8"><a href="#cb314-8" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2)</span>
<span id="cb314-9"><a href="#cb314-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-10"><a href="#cb314-10" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">sort</span>(x1)</span>
<span id="cb314-11"><a href="#cb314-11" aria-hidden="true" tabindex="-1"></a>yy <span class="ot">&lt;-</span> <span class="fu">sort</span>(x2)</span>
<span id="cb314-12"><a href="#cb314-12" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="fu">predict</span>(m1,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x1=</span>a,<span class="at">x2=</span>b))})</span>
<span id="cb314-13"><a href="#cb314-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-14"><a href="#cb314-14" aria-hidden="true" tabindex="-1"></a>axx<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x1&quot;</span>)</span>
<span id="cb314-15"><a href="#cb314-15" aria-hidden="true" tabindex="-1"></a>axy<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x2&quot;</span>)</span>
<span id="cb314-16"><a href="#cb314-16" aria-hidden="true" tabindex="-1"></a>axz<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb314-17"><a href="#cb314-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb314-18"><a href="#cb314-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb314-19"><a href="#cb314-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x=</span>x1,</span>
<span id="cb314-20"><a href="#cb314-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>x2,</span>
<span id="cb314-21"><a href="#cb314-21" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span>y) <span class="sc">%&gt;%</span></span>
<span id="cb314-22"><a href="#cb314-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x=</span><span class="sc">~</span>xx,</span>
<span id="cb314-23"><a href="#cb314-23" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span><span class="sc">~</span>yy,</span>
<span id="cb314-24"><a href="#cb314-24" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span><span class="sc">~</span>fit,</span>
<span id="cb314-25"><a href="#cb314-25" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale=</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb314-26"><a href="#cb314-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis=</span>axx,<span class="at">yaxis=</span>axy,<span class="at">zaxis=</span>axz))</span></code></pre></div>
<div id="htmlwidget-b96920e55d5e04710e35" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-b96920e55d5e04710e35">{"x":{"visdat":{"35111fb4e060":["function () ","plotlyVisDat"]},"cur_data":"35111fb4e060","attrs":{"35111fb4e060":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[-0.715798288657035,0.0398649395013548,-1.35812256167027,-1.26714446436738,2.14146334483501,3.14026008714997,3.50801069085019,-2.65793240843838,-0.521265480318938,1.39620662171399,-2.63563151534892,-3.05780283806507,2.32900014331444,3.45875612860532,2.33525296768733,-0.320074671287452,1.82219997656081,1.6730674847792,-3.39658880982085,4.08293608997422,1.3830334723996,-2.35996957865719,0.964431636964856,0.230673175344973,2.08777851377213,-1.4234935907596,1.51056474867124,-3.68720859601768,2.44360365602448,1.01420374229176,2.46747656291495,0.697040200103037,0.900883640006907,2.02101372710735,0.390255758037399,-2.61685584917697,1.22249818487302,0.241938410104874,-0.0172474728436934,3.14395686474125,2.84088466221408,1.35237709188441,-0.3535563585683,2.91124911937587,-0.429506140129029,-0.43367109640646,2.48833595026866,-0.290378867625921,-2.8428614617323,2.6696267028272],"y":[-0.639911586089848,0.373544422086501,-1.11246369957469,-1.24748150075266,2.28962092866078,3.1889472685881,3.45192812218369,-2.39140349930287,-0.591409303005398,1.42875233442496,-2.81682774425337,-3.73786254670458,2.09776381156926,3.46541694027223,2.27859123888164,-0.188208567834439,1.59698779150616,1.39404237974094,-3.5043406284154,3.94682849009638,1.25412452725694,-2.650818016905,0.519215321254824,0.422234904721515,2.42848505019461,-1.188304715176,1.60484513149822,-3.63747809054158,2.19206373267812,0.882830410974694,2.13883466717866,0.318676754866431,0.870607571621903,1.51408138304498,0.42384622735005,-2.85350805638705,0.936380133591274,0.141632271989487,-0.13974870250099,3.18951457536047,2.91088078833771,1.23225226883807,-0.218665988841759,3.11015681562905,-0.638906706343024,-0.661759302708913,2.85034841678827,-0.226797541738164,-2.87697855225446,2.77279699686677],"z":[44.5344966558363,53.077507431928,31.6885845924147,32.1652062382715,70.8075329050786,88.7222998589672,97.2515182562081,10.0909981974655,40.1282441028323,70.8105030001821,15.0608138130081,4.0134347043684,81.8576160683861,94.3884384375418,85.6795448882354,51.4505062413846,70.4642028083864,78.4081036782455,1.98828166369427,107.143853115057,67.190818529612,11.9541691168441,61.8541675815536,50.6040105979135,80.0581897973798,34.5663932223683,74.5104452696931,-6.29267326405866,80.0200628254843,53.7517000843191,77.5169061514328,56.9793870621456,57.3459677213818,75.3179045875806,57.353845250553,7.78709475535664,65.7418138762468,49.4735128072745,54.793011836345,89.5375916109722,74.8155146601138,66.6410794664439,39.8607905394907,86.4659185593933,52.3163544957849,48.9130675010038,88.5751495169416,44.2108781546426,8.70530020599637,82.6217698173004],"type":"scatter3d","mode":"markers","inherit":true},"35111fb4e060.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[-0.715798288657035,0.0398649395013548,-1.35812256167027,-1.26714446436738,2.14146334483501,3.14026008714997,3.50801069085019,-2.65793240843838,-0.521265480318938,1.39620662171399,-2.63563151534892,-3.05780283806507,2.32900014331444,3.45875612860532,2.33525296768733,-0.320074671287452,1.82219997656081,1.6730674847792,-3.39658880982085,4.08293608997422,1.3830334723996,-2.35996957865719,0.964431636964856,0.230673175344973,2.08777851377213,-1.4234935907596,1.51056474867124,-3.68720859601768,2.44360365602448,1.01420374229176,2.46747656291495,0.697040200103037,0.900883640006907,2.02101372710735,0.390255758037399,-2.61685584917697,1.22249818487302,0.241938410104874,-0.0172474728436934,3.14395686474125,2.84088466221408,1.35237709188441,-0.3535563585683,2.91124911937587,-0.429506140129029,-0.43367109640646,2.48833595026866,-0.290378867625921,-2.8428614617323,2.6696267028272],"y":[-0.639911586089848,0.373544422086501,-1.11246369957469,-1.24748150075266,2.28962092866078,3.1889472685881,3.45192812218369,-2.39140349930287,-0.591409303005398,1.42875233442496,-2.81682774425337,-3.73786254670458,2.09776381156926,3.46541694027223,2.27859123888164,-0.188208567834439,1.59698779150616,1.39404237974094,-3.5043406284154,3.94682849009638,1.25412452725694,-2.650818016905,0.519215321254824,0.422234904721515,2.42848505019461,-1.188304715176,1.60484513149822,-3.63747809054158,2.19206373267812,0.882830410974694,2.13883466717866,0.318676754866431,0.870607571621903,1.51408138304498,0.42384622735005,-2.85350805638705,0.936380133591274,0.141632271989487,-0.13974870250099,3.18951457536047,2.91088078833771,1.23225226883807,-0.218665988841759,3.11015681562905,-0.638906706343024,-0.661759302708913,2.85034841678827,-0.226797541738164,-2.87697855225446,2.77279699686677],"z":[44.5344966558363,53.077507431928,31.6885845924147,32.1652062382715,70.8075329050786,88.7222998589672,97.2515182562081,10.0909981974655,40.1282441028323,70.8105030001821,15.0608138130081,4.0134347043684,81.8576160683861,94.3884384375418,85.6795448882354,51.4505062413846,70.4642028083864,78.4081036782455,1.98828166369427,107.143853115057,67.190818529612,11.9541691168441,61.8541675815536,50.6040105979135,80.0581897973798,34.5663932223683,74.5104452696931,-6.29267326405866,80.0200628254843,53.7517000843191,77.5169061514328,56.9793870621456,57.3459677213818,75.3179045875806,57.353845250553,7.78709475535664,65.7418138762468,49.4735128072745,54.793011836345,89.5375916109722,74.8155146601138,66.6410794664439,39.8607905394907,86.4659185593933,52.3163544957849,48.9130675010038,88.5751495169416,44.2108781546426,8.70530020599637,82.6217698173004],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[-1.26242711541372,-1.01092436283224,-0.677362380466431,0.894427666027485,0.953230537913549,1.04512922242888,1.46104922617549,2.11098515978849,4.97696204874754,5.12522329386786,5.31523502388444,6.44442773702978,6.49916490528612,6.50168252636013,6.62068230119069,7.53417892295243,7.55455167800215,7.63085955984628,7.75227068251352,8.45724127570479,8.90080770495967,9.03827290486239,9.16026181505126,9.16429881531866,9.403236102475,10.2836126227165,10.3142356679477,10.4483988955312,11.1896755759159,11.244474231084,11.5950237721193,11.6819859321856,11.8957689362818,12.1034822689194,12.1231680122258,13.358124195436,13.4610229272862,13.5943827820103,13.811168396198,13.838802130001,14.1867116583832,15.0493492184664,15.2436461875628,15.395303711515,15.8945689494109,16.0919701861539,16.0933915139198,16.7508412014844,16.7846360239899,17.9907622962555],[1.94965030127886,2.20115305386034,2.53471503622615,4.10650508272007,4.16530795460613,4.25720663912147,4.67312664286808,5.32306257648107,8.18903946544012,8.33730071056045,8.52731244057703,9.65650515372236,9.7112423219787,9.71375994305272,9.83275971788328,10.746256339645,10.7666290946947,10.8429369765389,10.9643480992061,11.6693186923974,12.1128851216523,12.250350321555,12.3723392317438,12.3763762320112,12.6153135191676,13.4956900394091,13.5263130846403,13.6604763122238,14.4017529926085,14.4565516477766,14.8071011888119,14.8940633488782,15.1078463529744,15.315559685612,15.3352454289184,16.5702016121286,16.6731003439788,16.8064601987028,17.0232458128906,17.0508795466936,17.3987890750757,18.261426635159,18.4557236042554,18.6073811282076,19.1066463661035,19.3040476028465,19.3054689306123,19.9629186181769,19.9967134406825,21.202839712948],[5.69408485341216,5.94558760599364,6.27914958835945,7.85093963485337,7.90974250673943,8.00164119125477,8.41756119500138,9.06749712861437,11.9334740175734,12.0817352626937,12.2717469927103,13.4009397058557,13.455676874112,13.458194495186,13.5771942700166,14.4906908917783,14.511063646828,14.5873715286722,14.7087826513394,15.4137532445307,15.8573196737856,15.9947848736883,16.1167737838771,16.1208107841445,16.3597480713009,17.2401245915424,17.2707476367736,17.4049108643571,18.1461875447418,18.2009861999099,18.5515357409452,18.6384979010115,18.8522809051077,19.0599942377453,19.0796799810517,20.3146361642619,20.4175348961121,20.5508947508361,20.7676803650239,20.7953140988269,21.143223627209,22.0058611872923,22.2001581563887,22.3518156803409,22.8510809182368,23.0484821549798,23.0499034827456,23.7073531703102,23.7411479928158,24.9472742650813],[8.0697260644469,8.32122881702837,8.65479079939419,10.2265808458881,10.2853837177742,10.3772824022895,10.7932024060361,11.4431383396491,14.3091152286082,14.4573764737285,14.6473882037451,15.7765809168904,15.8313180851467,15.8338357062207,15.9528354810513,16.866332102813,16.8867048578628,16.9630127397069,17.0844238623741,17.7893944555654,18.2329608848203,18.370426084723,18.4924149949119,18.4964519951793,18.7353892823356,19.6157658025771,19.6463888478083,19.7805520753919,20.5218287557765,20.5766274109447,20.9271769519799,21.0141391120462,21.2279221161424,21.4356354487801,21.4553211920864,22.6902773752966,22.7931761071468,22.9265359618709,23.1433215760586,23.1709553098617,23.5188648382438,24.381502398327,24.5757993674234,24.7274568913756,25.2267221292716,25.4241233660145,25.4255446937804,26.082994381345,26.1167892038505,27.3229154761161],[10.1136558448282,10.3651585974096,10.6987205797755,12.2705106262694,12.3293134981554,12.4212121826708,12.8371321864174,13.4870681200304,16.3530450089894,16.5013062541097,16.6913179841263,17.8205106972717,17.875247865528,17.877765486602,17.9967652614326,18.9102618831943,18.930634638244,19.0069425200882,19.1283536427554,19.8333242359467,20.2768906652016,20.4143558651043,20.5363447752931,20.5403817755606,20.7793190627169,21.6596955829584,21.6903186281896,21.8244818557731,22.5657585361578,22.6205571913259,22.9711067323612,23.0580688924275,23.2718518965237,23.4795652291613,23.4992509724677,24.7342071556779,24.8371058875281,24.9704657422521,25.1872513564399,25.2148850902429,25.562794618625,26.4254321787083,26.6197291478047,26.7713866717569,27.2706519096528,27.4680531463958,27.4694744741617,28.1269241617263,28.1607189842318,29.3668452564973],[10.3601366368009,10.6116393893823,10.9452013717481,12.5169914182421,12.5757942901281,12.6676929746435,13.0836129783901,13.7335489120031,16.5995258009621,16.7477870460824,16.937798776099,18.0669914892444,18.1217286575007,18.1242462785747,18.2432460534053,19.156742675167,19.1771154302167,19.2534233120609,19.3748344347281,20.0798050279194,20.5233714571743,20.660836657077,20.7828255672658,20.7868625675332,21.0257998546896,21.9061763749311,21.9367994201623,22.0709626477458,22.8122393281305,22.8670379832986,23.2175875243339,23.3045496844001,23.5183326884964,23.726046021134,23.7457317644404,24.9806879476506,25.0835866795007,25.2169465342248,25.4337321484126,25.4613658822156,25.8092754105977,26.671912970681,26.8662099397774,27.0178674637296,27.5171327016255,27.7145339383684,27.7159552661343,28.3734049536989,28.4071997762044,29.61332604847],[10.5676548312208,10.8191575838023,11.1527195661681,12.724509612662,12.7833124845481,12.8752111690634,13.29113117281,13.941067106423,16.8070439953821,16.9553052405024,17.145316970519,18.2745096836643,18.3292468519207,18.3317644729947,18.4507642478252,19.364260869587,19.3846336246367,19.4609415064808,19.5823526291481,20.2873232223393,20.7308896515942,20.8683548514969,20.9903437616858,20.9943807619532,21.2333180491095,22.1136945693511,22.1443176145823,22.2784808421658,23.0197575225505,23.0745561777186,23.4251057187539,23.5120678788201,23.7258508829163,23.933564215554,23.9532499588603,25.1882061420706,25.2911048739207,25.4244647286448,25.6412503428325,25.6688840766356,26.0167936050177,26.879431165101,27.0737281341973,27.2253856581495,27.7246508960455,27.9220521327884,27.9234734605543,28.5809231481189,28.6147179706244,29.82084424289],[13.4068923070758,13.6583950596572,13.991957042023,15.563747088517,15.622549960403,15.7144486449184,16.130368648665,16.780304582278,19.646281471237,19.7945427163573,19.9845544463739,21.1137471595193,21.1684843277756,21.1710019488496,21.2900017236802,22.2034983454419,22.2238711004916,22.3001789823358,22.421590105003,23.1265606981943,23.5701271274492,23.7075923273519,23.8295812375407,23.8336182378081,24.0725555249645,24.952932045206,24.9835550904372,25.1177183180207,25.8589949984054,25.9137936535735,26.2643431946088,26.351305354675,26.5650883587713,26.7728016914089,26.7924874347153,28.0274436179255,28.1303423497756,28.2637022044997,28.4804878186875,28.5081215524905,28.8560310808726,29.7186686409559,29.9129656100523,30.0646231340045,30.5638883719004,30.7612896086433,30.7627109364092,31.4201606239738,31.4539554464793,32.6600817187449],[23.757300348335,24.0088031009165,24.3423650832823,25.9141551297762,25.9729580016623,26.0648566861776,26.4807766899242,27.1307126235372,29.9966895124963,30.1449507576166,30.3349624876332,31.4641552007785,31.5188923690348,31.5214099901089,31.6404097649394,32.5539063867012,32.5742791417509,32.650587023595,32.7719981462622,33.4769687394535,33.9205351687084,34.0580003686111,34.1799892788,34.1840262790674,34.4229635662237,35.3033400864652,35.3339631316964,35.46812635928,36.2094030396646,36.2642016948328,36.614751235868,36.7017133959343,36.9154964000305,37.1232097326682,37.1428954759745,38.3778516591847,38.4807503910349,38.614110245759,38.8308958599467,38.8585295937498,39.2064391221319,40.0690766822151,40.2633736513115,40.4150311752637,40.9142964131597,41.1116976499026,41.1131189776685,41.7705686652331,41.8043634877386,43.0104897600042],[24.4798141488977,24.7313169014792,25.064878883845,26.6366689303389,26.695471802225,26.7873704867403,27.203290490487,27.8532264240999,30.719203313059,30.8674645581793,31.0574762881959,32.1866690013412,32.2414061695976,32.2439237906716,32.3629235655022,33.2764201872639,33.2967929423136,33.3731008241577,33.494511946825,34.1994825400162,34.6430489692711,34.7805141691738,34.9025030793627,34.9065400796301,35.1454773667865,36.025853887028,36.0564769322592,36.1906401598427,36.9319168402274,36.9867154953955,37.3372650364308,37.424227196497,37.6380102005933,37.8457235332309,37.8654092765372,39.1003654597475,39.2032641915976,39.3366240463217,39.5534096605094,39.5810433943125,39.9289529226946,40.7915904827779,40.9858874518743,41.1375449758265,41.6368102137224,41.8342114504653,41.8356327782312,42.4930824657958,42.5268772883013,43.7330035605669],[25.4853502673585,25.73685301994,26.0704150023058,27.6422050487997,27.7010079206858,27.7929066052011,28.2088266089477,28.8587625425607,31.7247394315198,31.8730006766401,32.0630124066567,33.192205119802,33.2469422880584,33.2494599091324,33.3684596839629,34.2819563057247,34.3023290607744,34.3786369426185,34.5000480652858,35.205018658477,35.6485850877319,35.7860502876346,35.9080391978235,35.9120761980909,36.1510134852473,37.0313900054888,37.06201305072,37.1961762783035,37.9374529586882,37.9922516138563,38.3428011548916,38.4297633149578,38.643546319054,38.8512596516917,38.870945394998,40.1059015782083,40.2088003100584,40.3421601647825,40.5589457789702,40.5865795127733,40.9344890411554,41.7971266012387,41.9914235703351,42.1430810942872,42.6423463321832,42.8397475689261,42.841168896692,43.4986185842566,43.5324134067621,44.7385396790277],[31.5791081045472,31.8306108571287,32.1641728394945,33.7359628859884,33.7947657578745,33.8866644423898,34.3025844461364,34.9525203797494,37.8184972687085,37.9667585138288,38.1567702438454,39.2859629569907,39.340700125247,39.3432177463211,39.4622175211516,40.3757141429134,40.3960868979631,40.4723947798072,40.5938059024744,41.2987764956657,41.7423429249206,41.8798081248233,42.0017970350122,42.0058340352796,42.2447713224359,43.1251478426774,43.1557708879086,43.2899341154922,44.0312107958768,44.086009451045,44.4365589920802,44.5235211521465,44.7373041562427,44.9450174888804,44.9647032321867,46.1996594153969,46.3025581472471,46.4359180019712,46.6527036161589,46.680337349962,47.0282468783441,47.8908844384273,48.0851814075237,48.2368389314759,48.7361041693719,48.9335054061148,48.9349267338807,49.5923764214453,49.6261712439508,50.8322975162164],[33.7291834612381,33.9806862138196,34.3142481961854,35.8860382426793,35.9448411145654,36.0367397990807,36.4526598028273,37.1025957364403,39.9685726253994,40.1168338705197,40.3068456005363,41.4360383136816,41.490775481938,41.493293103012,41.6122928778425,42.5257894996043,42.546162254654,42.6224701364981,42.7438812591654,43.4488518523566,43.8924182816115,44.0298834815142,44.1518723917031,44.1559093919705,44.3948466791268,45.2752231993684,45.3058462445996,45.4400094721831,46.1812861525677,46.2360848077359,46.5866343487712,46.6735965088374,46.8873795129336,47.0950928455713,47.1147785888776,48.3497347720878,48.452633503938,48.5859933586621,48.8027789728498,48.8304127066529,49.178322235035,50.0409597951183,50.2352567642146,50.3869142881668,50.8861795260628,51.0835807628057,51.0850020905716,51.7424517781362,51.7762466006417,52.9823728729073],[34.6973210614902,34.9488238140717,35.2823857964375,36.8541758429314,36.9129787148175,37.0048773993328,37.4207974030794,38.0707333366924,40.9367102256514,41.0849714707718,41.2749832007884,42.4041759139337,42.45891308219,42.461430703264,42.5804304780946,43.4939270998563,43.5142998549061,43.5906077367502,43.7120188594174,44.4169894526087,44.8605558818636,44.9980210817663,45.1200099919552,45.1240469922226,45.3629842793789,46.2433607996204,46.2739838448516,46.4081470724351,47.1494237528198,47.204222407988,47.5547719490232,47.6417341090895,47.8555171131857,48.0632304458233,48.0829161891297,49.3178723723399,49.4207711041901,49.5541309589142,49.7709165731019,49.7985503069049,50.1464598352871,51.0090973953703,51.2033943644667,51.3550518884189,51.8543171263149,52.0517183630578,52.0531396908237,52.7105893783883,52.7443842008938,53.9505104731594],[34.7433542727017,34.9948570252832,35.328419007649,36.9002090541429,36.959011926029,37.0509106105443,37.4668306142909,38.1167665479039,40.982743436863,41.1310046819833,41.3210164119999,42.4502091251452,42.5049462934016,42.5074639144756,42.6264636893061,43.5399603110679,43.5603330661176,43.6366409479617,43.758052070629,44.4630226638202,44.9065890930751,45.0440542929778,45.1660432031667,45.1700802034341,45.4090174905905,46.289394010832,46.3200170560632,46.4541802836467,47.1954569640314,47.2502556191995,47.6008051602348,47.687767320301,47.9015503243972,48.1092636570349,48.1289494003412,49.3639055835515,49.4668043154016,49.6001641701257,49.8169497843134,49.8445835181165,50.1924930464986,51.0551306065819,51.2494275756782,51.4010850996304,51.9003503375264,52.0977515742693,52.0991729020352,52.7566225895998,52.7904174121053,53.9965436843709],[35.582789816119,35.8342925687005,36.1678545510663,37.7396445975602,37.7984474694463,37.8903461539616,38.3062661577082,38.9562020913212,41.8221789802803,41.9704402254006,42.1604519554172,43.2896446685625,43.3443818368188,43.3468994578929,43.4658992327234,44.3793958544852,44.3997686095349,44.476076491379,44.5974876140463,45.3024582072375,45.7460246364924,45.8834898363951,46.005478746584,46.0095157468514,46.2484530340077,47.1288295542493,47.1594525994805,47.293615827064,48.0348925074486,48.0896911626168,48.4402407036521,48.5272028637183,48.7409858678145,48.9486992004522,48.9683849437585,50.2033411269687,50.3062398588189,50.439599713543,50.6563853277307,50.6840190615338,51.0319285899159,51.8945661499992,52.0888631190955,52.2405206430477,52.7397858809437,52.9371871176866,52.9386084454525,53.5960581330171,53.6298529555226,54.8359792277882],[35.9528464216756,36.2043491742571,36.5379111566229,38.1097012031168,38.1685040750028,38.2604027595182,38.6763227632648,39.3262586968778,42.1922355858368,42.3404968309572,42.5305085609737,43.6597012741191,43.7144384423754,43.7169560634494,43.83595583828,44.7494524600417,44.7698252150914,44.8461330969356,44.9675442196028,45.6725148127941,46.116081242049,46.2535464419517,46.3755353521406,46.379572352408,46.6185096395643,47.4988861598058,47.529509205037,47.6636724326205,48.4049491130052,48.4597477681733,48.8102973092086,48.8972594692749,49.1110424733711,49.3187558060087,49.3384415493151,50.5733977325253,50.6762964643755,50.8096563190996,51.0264419332873,51.0540756670903,51.4019851954725,52.2646227555557,52.4589197246521,52.6105772486043,53.1098424865002,53.3072437232432,53.3086650510091,53.9661147385737,53.9999095610792,55.2060358333447],[36.2810595194828,36.5325622720643,36.8661242544301,38.437914300924,38.4967171728101,38.5886158573254,39.004535861072,39.654471794685,42.5204486836441,42.6687099287644,42.858721658781,43.9879143719263,44.0426515401827,44.0451691612567,44.1641689360873,45.077665557849,45.0980383128987,45.1743461947428,45.2957573174101,46.0007279106013,46.4442943398562,46.5817595397589,46.7037484499478,46.7077854502152,46.9467227373716,47.8270992576131,47.8577223028443,47.9918855304278,48.7331622108125,48.7879608659806,49.1385104070159,49.2254725670821,49.4392555711784,49.646968903816,49.6666546471223,50.9016108303326,51.0045095621827,51.1378694169068,51.3546550310945,51.3822887648976,51.7301982932797,52.592835853363,52.7871328224594,52.9387903464116,53.4380555843075,53.6354568210504,53.6368781488163,54.2943278363809,54.3281226588864,55.534248931152],[39.2998463558875,39.551349108469,39.8849110908348,41.4567011373287,41.5155040092148,41.6074026937301,42.0233226974768,42.6732586310897,45.5392355200488,45.6874967651691,45.8775084951857,47.006701208331,47.0614383765874,47.0639559976614,47.182955772492,48.0964523942537,48.1168251493034,48.1931330311475,48.3145441538148,49.019514747006,49.4630811762609,49.6005463761636,49.7225352863525,49.7265722866199,49.9655095737763,50.8458860940178,50.876509139249,51.0106723668325,51.7519490472172,51.8067477023853,52.1572972434206,52.2442594034868,52.4580424075831,52.6657557402207,52.685441483527,53.9203976667373,54.0232963985874,54.1566562533115,54.3734418674992,54.4010756013023,54.7489851296844,55.6116226897677,55.8059196588641,55.9575771828163,56.4568424207122,56.6542436574551,56.655664985221,57.3131146727856,57.3469094952911,58.5530357675567],[39.9310817315206,40.1825844841021,40.5161464664679,42.0879365129618,42.1467393848479,42.2386380693632,42.6545580731098,43.3044940067228,46.1704708956819,46.3187321408022,46.5087438708188,47.6379365839641,47.6926737522205,47.6951913732945,47.814191148125,48.7276877698868,48.7480605249365,48.8243684067806,48.9457795294479,49.6507501226391,50.094316551894,50.2317817517967,50.3537706619856,50.357807662253,50.5967449494093,51.4771214696509,51.5077445148821,51.6419077424656,52.3831844228502,52.4379830780184,52.7885326190537,52.8754947791199,53.0892777832161,53.2969911158538,53.3166768591601,54.5516330423704,54.6545317742205,54.7878916289446,55.0046772431323,55.0323109769354,55.3802205053175,56.2428580654008,56.4371550344971,56.5888125584493,57.0880777963453,57.2854790330882,57.2869003608541,57.9443500484187,57.9781448709242,59.1842711431898],[42.0399912221399,42.2914939747214,42.6250559570872,44.1968460035811,44.2556488754672,44.3475475599825,44.7634675637291,45.4134034973421,48.2793803863011,48.4276416314215,48.6176533614381,49.7468460745834,49.8015832428397,49.8041008639137,49.9231006387443,50.836597260506,50.8569700155558,50.9332778973999,51.0546890200671,51.7596596132584,52.2032260425133,52.340691242416,52.4626801526049,52.4667171528723,52.7056544400286,53.5860309602701,53.6166540055013,53.7508172330849,54.4920939134695,54.5468925686377,54.8974421096729,54.9844042697392,55.1981872738354,55.405900606473,55.4255863497794,56.6605425329896,56.7634412648398,56.8968011195639,57.1135867337516,57.1412204675547,57.4891299959368,58.35176755602,58.5460645251164,58.6977220490686,59.1969872869646,59.3943885237075,59.3958098514734,60.053259539038,60.0870543615435,61.2931806338091],[42.1645003157551,42.4160030683366,42.7495650507024,44.3213550971963,44.3801579690824,44.4720566535977,44.8879766573443,45.5379125909573,48.4038894799164,48.5521507250367,48.7421624550533,49.8713551681986,49.926092336455,49.928609957529,50.0476097323595,50.9611063541213,50.981479109171,51.0577869910151,51.1791981136824,51.8841687068736,52.3277351361285,52.4652003360312,52.5871892462201,52.5912262464875,52.8301635336439,53.7105400538854,53.7411630991166,53.8753263267001,54.6166030070848,54.6714016622529,55.0219512032882,55.1089133633544,55.3226963674506,55.5304097000883,55.5500954433946,56.7850516266049,56.887950358455,57.0213102131791,57.2380958273668,57.2657295611699,57.613639089552,58.4762766496353,58.6705736187317,58.8222311426838,59.3214963805798,59.5188976173227,59.5203189450886,60.1777686326532,60.2115634551587,61.4176897274243],[43.8037789417298,44.0552816943113,44.3888436766771,45.960633723171,46.0194365950571,46.1113352795724,46.527255283319,47.177191216932,50.0431681058911,50.1914293510114,50.381441081028,51.5106337941733,51.5653709624297,51.5678885835037,51.6868883583342,52.600384980096,52.6207577351457,52.6970656169898,52.8184767396571,53.5234473328483,53.9670137621032,54.1044789620059,54.2264678721948,54.2305048724622,54.4694421596186,55.3498186798601,55.3804417250913,55.5146049526748,56.2558816330595,56.3106802882276,56.6612298292629,56.7481919893291,56.9619749934253,57.169688326063,57.1893740693693,58.4243302525796,58.5272289844297,58.6605888391538,58.8773744533415,58.9050081871446,59.2529177155267,60.11555527561,60.3098522447063,60.4615097686585,60.9607750065545,61.1581762432974,61.1595975710633,61.8170472586279,61.8508420811334,63.056968353399],[47.1945163414342,47.4460190940156,47.7795810763815,49.3513711228754,49.4101739947614,49.5020726792768,49.9179926830234,50.5679286166364,53.4339055055954,53.5821667507157,53.7721784807323,54.9013711938777,54.956108362134,54.958625983208,55.0776257580386,55.9911223798003,56.01149513485,56.0878030166942,56.2092141393614,56.9141847325527,57.3577511618076,57.4952163617103,57.6172052718991,57.6212422721665,57.8601795593229,58.7405560795644,58.7711791247956,58.9053423523791,59.6466190327638,59.7014176879319,60.0519672289672,60.1389293890334,60.3527123931297,60.5604257257673,60.5801114690737,61.8150676522839,61.9179663841341,62.0513262388581,62.2681118530459,62.2957455868489,62.643655115231,63.5062926753143,63.7005896444107,63.8522471683629,64.3515124062588,64.5489136430018,64.5503349707677,65.2077846583322,65.2415794808377,66.4477057531033],[49.4474975249407,49.6990002775222,50.032562259888,51.6043523063819,51.663155178268,51.7550538627833,52.1709738665299,52.8209098001429,55.686886689102,55.8351479342223,56.0251596642389,57.1543523773842,57.2090895456405,57.2116071667146,57.3306069415451,58.2441035633069,58.2644763183566,58.3407842002007,58.4621953228679,59.1671659160592,59.6107323453141,59.7481975452168,59.8701864554057,59.8742234556731,60.1131607428294,60.993537263071,61.0241603083022,61.1583235358857,61.8996002162703,61.9543988714385,62.3049484124738,62.39191057254,62.6056935766362,62.8134069092739,62.8330926525802,64.0680488357904,64.1709475676406,64.3043074223647,64.5210930365524,64.5487267703555,64.8966362987376,65.7592738588209,65.9535708279172,66.1052283518694,66.6044935897654,66.8018948265083,66.8033161542742,67.4607658418388,67.4945606643443,68.7006869366099],[50.1498622488843,50.4013650014658,50.7349269838316,52.3067170303255,52.3655199022116,52.4574185867269,52.8733385904735,53.5232745240865,56.3892514130455,56.5375126581659,56.7275243881825,57.8567171013278,57.9114542695841,57.9139718906581,58.0329716654887,58.9464682872504,58.9668410423002,59.0431489241443,59.1645600468115,59.8695306400028,60.3130970692577,60.4505622691604,60.5725511793493,60.5765881796167,60.815525466773,61.6959019870145,61.7265250322457,61.8606882598292,62.6019649402139,62.6567635953821,63.0073131364173,63.0942752964836,63.3080583005798,63.5157716332174,63.5354573765238,64.770413559734,64.8733122915842,65.0066721463083,65.223457760496,65.251091494299,65.5990010226812,66.4616385827644,66.6559355518608,66.807593075813,67.306858313709,67.5042595504519,67.5056808782178,68.1631305657824,68.1969253882879,69.4030516605535],[50.6999688248815,50.951471577463,51.2850335598288,52.8568236063227,52.9156264782088,53.0075251627241,53.4234451664707,54.0733811000837,56.9393579890428,57.0876192341631,57.2776309641797,58.406823677325,58.4615608455813,58.4640784666554,58.5830782414859,59.4965748632477,59.5169476182974,59.5932555001415,59.7146666228087,60.419637216,60.8632036452549,61.0006688451576,61.1226577553465,61.1266947556139,61.3656320427702,62.2460085630118,62.2766316082429,62.4107948358265,63.1520715162111,63.2068701713793,63.5574197124145,63.6443818724808,63.858164876577,64.0658782092147,64.085563952521,65.3205201357312,65.4234188675814,65.5567787223055,65.7735643364932,65.8011980702963,66.1491075986784,67.0117451587617,67.206042127858,67.3576996518102,67.8569648897062,68.0543661264491,68.055787454215,68.7132371417796,68.7470319642851,69.9531582365507],[53.0021447496298,53.2536475022113,53.5872094845771,55.158999531071,55.2178024029571,55.3097010874724,55.725621091219,56.375557024832,59.2415339137911,59.3897951589114,59.579806888928,60.7089996020733,60.7637367703296,60.7662543914037,60.8852541662342,61.798750787996,61.8191235430457,61.8954314248898,62.016842547557,62.7218131407483,63.1653795700032,63.3028447699059,63.4248336800948,63.4288706803622,63.6678079675185,64.5481844877601,64.5788075329912,64.7129707605748,65.4542474409594,65.5090460961276,65.8595956371628,65.9465577972291,66.1603408013253,66.368054133963,66.3877398772693,67.6226960604795,67.7255947923297,67.8589546470538,68.0757402612415,68.1033739950446,68.4512835234267,69.31392108351,69.5082180526063,69.6598755765585,70.1591408144545,70.3565420511974,70.3579633789633,71.0154130665279,71.0492078890334,72.255334161299],[54.4376323658565,54.689135118438,55.0226971008038,56.5944871472977,56.6532900191838,56.7451887036991,57.1611087074458,57.8110446410587,60.6770215300178,60.8252827751381,61.0152945051547,62.1444872183,62.1992243865564,62.2017420076304,62.320741782461,63.2342384042227,63.2546111592724,63.3309190411165,63.4523301637838,64.157300756975,64.6008671862299,64.7383323861327,64.8603212963215,64.8643582965889,65.1032955837453,65.9836721039868,66.014295149218,66.1484583768015,66.8897350571862,66.9445337123543,67.2950832533896,67.3820454134558,67.5958284175521,67.8035417501897,67.823227493496,69.0581836767063,69.1610824085564,69.2944422632805,69.5112278774682,69.5388616112713,69.8867711396534,70.7494086997367,70.9437056688331,71.0953631927853,71.5946284306812,71.7920296674241,71.79345099519,72.4509006827546,72.4846955052601,73.6908217775257],[54.7764622467088,55.0279649992903,55.3615269816561,56.93331702815,56.992119900036,57.0840185845514,57.499938588298,58.149874521911,61.01585141087,61.1641126559904,61.3541243860069,62.4833170991523,62.5380542674086,62.5405718884826,62.6595716633132,63.5730682850749,63.5934410401246,63.6697489219688,63.791160044636,64.4961306378273,64.9396970670822,65.0771622669849,65.1991511771738,65.2031881774412,65.4421254645975,66.322501984839,66.3531250300702,66.4872882576537,67.2285649380384,67.2833635932065,67.6339131342418,67.7208752943081,67.9346582984043,68.1423716310419,68.1620573743483,69.3970135575585,69.4999122894087,69.6332721441328,69.8500577583205,69.8776914921235,70.2256010205057,71.0882385805889,71.2825355496853,71.4341930736375,71.9334583115334,72.1308595482764,72.1322808760423,72.7897305636069,72.8235253861124,74.0296516583779],[54.9220585805758,55.1735613331573,55.5071233155231,57.078913362017,57.137716233903,57.2296149184184,57.645534922165,58.295470855778,61.161447744737,61.3097089898574,61.4997207198739,62.6289134330193,62.6836506012756,62.6861682223496,62.8051679971802,63.7186646189419,63.7390373739916,63.8153452558358,63.936756378503,64.6417269716943,65.0852934009492,65.2227586008519,65.3447475110408,65.3487845113082,65.5877217984645,66.468098318706,66.4987213639372,66.6328845915207,67.3741612719054,67.4289599270735,67.7795094681088,67.8664716281751,68.0802546322713,68.2879679649089,68.3076537082153,69.5426098914255,69.6455086232757,69.7788684779998,69.9956540921875,70.0232878259905,70.3711973543727,71.2338349144559,71.4281318835523,71.5797894075045,72.0790546454004,72.2764558821434,72.2778772099093,72.9353268974739,72.9691217199794,74.1752479922449],[56.1860026561724,56.4375054087539,56.7710673911197,58.3428574376136,58.4016603094997,58.493558994015,58.9094789977616,59.5594149313746,62.4253918203337,62.573653065454,62.7636647954706,63.8928575086159,63.9475946768722,63.9501122979463,64.0691120727768,64.9826086945386,65.0029814495883,65.0792893314324,65.2007004540996,65.9056710472909,66.3492374765458,66.4867026764485,66.6086915866374,66.6127285869048,66.8516658740611,67.7320423943027,67.7626654395338,67.8968286671174,68.638105347502,68.6929040026702,69.0434535437055,69.1304157037717,69.3441987078679,69.5519120405056,69.5715977838119,70.8065539670221,70.9094526988723,71.0428125535964,71.2595981677841,71.2872319015872,71.6351414299693,72.4977789900526,72.6920759591489,72.8437334831011,73.3429987209971,73.54039995774,73.5418212855059,74.1992709730705,74.233065795576,75.4391920678416],[57.9820653937921,58.2335681463736,58.5671301287394,60.1389201752333,60.1977230471194,60.2896217316347,60.7055417353813,61.3554776689943,64.2214545579534,64.3697158030737,64.5597275330903,65.6889202462356,65.7436574144919,65.746175035566,65.8651748103965,66.7786714321583,66.799044187208,66.8753520690521,66.9967631917194,67.7017337849106,68.1453002141655,68.2827654140682,68.4047543242571,68.4087913245245,68.6477286116808,69.5281051319224,69.5587281771536,69.6928914047371,70.4341680851217,70.4889667402899,70.8395162813252,70.9264784413914,71.1402614454876,71.3479747781253,71.3676605214316,72.6026167046418,72.705515436492,72.8388752912161,73.0556609054038,73.0832946392069,73.431204167589,74.2938417276723,74.4881386967686,74.6397962207208,75.1390614586168,75.3364626953597,75.3378840231256,75.9953337106902,76.0291285331957,77.2352548054613],[59.630353403413,59.8818561559945,60.2154181383603,61.7872081848542,61.8460110567403,61.9379097412556,62.3538297450022,63.0037656786152,65.8697425675743,66.0180038126946,66.2080155427112,67.3372082558565,67.3919454241128,67.3944630451869,67.5134628200174,68.4269594417792,68.4473321968289,68.523640078673,68.6450512013403,69.3500217945315,69.7935882237864,69.9310534236891,70.053042333878,70.0570793341454,70.2960166213017,71.1763931415433,71.2070161867745,71.341179414358,72.0824560947426,72.1372547499108,72.4878042909461,72.5747664510123,72.7885494551085,72.9962627877462,73.0159485310525,74.2509047142627,74.3538034461129,74.487163300837,74.7039489150247,74.7315826488278,75.0794921772099,75.9421297372932,76.1364267063895,76.2880842303417,76.7873494682377,76.9847507049806,76.9861720327465,77.6436217203111,77.6774165428166,78.8835428150822],[61.8277439067078,62.0792466592892,62.4128086416551,63.984598688149,64.043401560035,64.1353002445504,64.551220248297,65.20115618191,68.067133070869,68.2153943159894,68.4054060460059,69.5345987591513,69.5893359274076,69.5918535484816,69.7108533233122,70.6243499450739,70.6447227001236,70.7210305819678,70.842441704635,71.5474122978263,71.9909787270812,72.1284439269839,72.2504328371728,72.2544698374402,72.4934071245965,73.373783644838,73.4044066900692,73.5385699176527,74.2798465980374,74.3346452532055,74.6851947942408,74.7721569543071,74.9859399584033,75.1936532910409,75.2133390343473,76.4482952175575,76.5511939494077,76.6845538041318,76.9013394183195,76.9289731521225,77.2768826805047,78.1395202405879,78.3338172096843,78.4854747336365,78.9847399715324,79.1821412082754,79.1835625360413,79.8410122236059,79.8748070461114,81.0809333183769],[62.5656622235667,62.8171649761482,63.150726958514,64.7225170050079,64.781319876894,64.8732185614093,65.2891385651559,65.9390744987689,68.805051387728,68.9533126328483,69.1433243628649,70.2725170760102,70.3272542442665,70.3297718653405,70.4487716401711,71.3622682619328,71.3826410169826,71.4589488988267,71.5803600214939,72.2853306146852,72.7288970439401,72.8663622438428,72.9883511540317,72.9923881542991,73.2313254414554,74.1117019616969,74.1423250069281,74.2764882345116,75.0177649148963,75.0725635700645,75.4231131110997,75.510075271166,75.7238582752622,75.9315716078998,75.9512573512062,77.1862135344164,77.2891122662666,77.4224721209907,77.6392577351784,77.6668914689814,78.0148009973636,78.8774385574468,79.0717355265432,79.2233930504954,79.7226582883914,79.9200595251343,79.9214808529002,80.5789305404648,80.6127253629703,81.8188516352359],[63.1590142307807,63.4105169833622,63.744078965728,65.315869012222,65.374671884108,65.4665705686234,65.88249057237,66.532426505983,69.398403394942,69.5466646400623,69.7366763700789,70.8658690832242,70.9206062514806,70.9231238725546,71.0421236473852,71.9556202691469,71.9759930241966,72.0523009060407,72.173712028708,72.8786826218993,73.3222490511541,73.4597142510569,73.5817031612457,73.5857401615131,73.8246774486695,74.705053968911,74.7356770141422,74.8698402417257,75.6111169221104,75.6659155772785,76.0164651183138,76.10342727838,76.3172102824763,76.5249236151139,76.5446093584202,77.7795655416305,77.8824642734806,78.0158241282047,78.2326097423924,78.2602434761955,78.6081530045776,79.4707905646609,79.6650875337573,79.8167450577095,80.3160102956054,80.5134115323483,80.5148328601142,81.1722825476788,81.2060773701843,82.4122036424499],[65.2317661348326,65.4832688874141,65.8168308697799,67.3886209162738,67.4474237881599,67.5393224726752,67.9552424764218,68.6051784100348,71.4711552989938,71.6194165441142,71.8094282741308,72.9386209872761,72.9933581555324,72.9958757766064,73.114875551437,74.0283721731987,74.0487449282485,74.1250528100926,74.2464639327598,74.9514345259511,75.395000955206,75.5324661551087,75.6544550652976,75.658492065565,75.8974293527213,76.7778058729628,76.808428918194,76.9425921457775,77.6838688261622,77.7386674813304,78.0892170223656,78.1761791824319,78.3899621865281,78.5976755191657,78.6173612624721,79.8523174456823,79.9552161775325,80.0885760322566,80.3053616464443,80.3329953802473,80.6809049086295,81.5435424687127,81.7378394378091,81.8894969617613,82.3887621996573,82.5861634364002,82.5875847641661,83.2450344517307,83.2788292742362,84.4849555465018],[65.3008755241881,65.5523782767696,65.8859402591354,67.4577303056294,67.5165331775154,67.6084318620308,68.0243518657774,68.6742877993904,71.5402646883494,71.6885259334697,71.8785376634863,73.0077303766316,73.062467544888,73.064985165962,73.1839849407926,74.0974815625543,74.117854317604,74.1941621994481,74.3155733221154,75.0205439153067,75.4641103445615,75.6015755444643,75.7235644546531,75.7276014549205,75.9665387420769,76.8469152623184,76.8775383075496,77.0117015351331,77.7529782155178,77.8077768706859,78.1583264117212,78.2452885717874,78.4590715758837,78.6667849085213,78.6864706518276,79.9214268350379,80.024325566888,80.1576854216121,80.3744710357998,80.4021047696029,80.750014297985,81.6126518580683,81.8069488271647,81.9586063511169,82.4578715890128,82.6552728257557,82.6566941535216,83.3141438410862,83.3479386635917,84.5540649358573],[66.4984223383446,66.7499250909261,67.0834870732919,68.6552771197858,68.7140799916719,68.8059786761872,69.2218986799338,69.8718346135468,72.7378115025059,72.8860727476262,73.0760844776428,74.2052771907881,74.2600143590444,74.2625319801185,74.381531754949,75.2950283767108,75.3154011317605,75.3917090136046,75.5131201362719,76.2180907294631,76.661657158718,76.7991223586207,76.9211112688096,76.925148269077,77.1640855562333,78.0444620764749,78.0750851217061,78.2092483492896,78.9505250296742,79.0053236848424,79.3558732258777,79.4428353859439,79.6566183900401,79.8643317226778,79.8840174659841,81.1189736491943,81.2218723810445,81.3552322357686,81.5720178499563,81.5996515837594,81.9475611121415,82.8101986722248,83.0044956413211,83.1561531652733,83.6554184031693,83.8528196399122,83.8542409676781,84.5116906552427,84.5454854777482,85.7516117500138],[66.7622778249142,67.0137805774957,67.3473425598615,68.9191326063554,68.9779354782414,69.0698341627568,69.4857541665034,70.1356901001164,73.0016669890754,73.1499282341958,73.3399399642123,74.4691326773577,74.523869845614,74.526387466688,74.6453872415186,75.5588838632803,75.57925661833,75.6555645001742,75.7769756228414,76.4819462160327,76.9255126452876,77.0629778451903,77.1849667553792,77.1890037556466,77.4279410428029,78.3083175630444,78.3389406082756,78.4731038358591,79.2143805162438,79.2691791714119,79.6197287124472,79.7066908725135,79.9204738766097,80.1281872092473,80.1478729525537,81.3828291357639,81.4857278676141,81.6190877223382,81.8358733365259,81.8635070703289,82.2114165987111,83.0740541587943,83.2683511278907,83.4200086518429,83.9192738897388,84.1166751264818,84.1180964542476,84.7755461418123,84.8093409643178,86.0154672365833],[66.9928263636826,67.2443291162641,67.5778910986299,69.1496811451239,69.2084840170099,69.3003827015253,69.7163027052719,70.3662386388849,73.2322155278439,73.3804767729642,73.5704885029808,74.6996812161261,74.7544183843825,74.7569360054565,74.8759357802871,75.7894324020488,75.8098051570985,75.8861130389426,76.0075241616099,76.7124947548012,77.156061184056,77.2935263839587,77.4155152941476,77.419552294415,77.6584895815714,78.5388661018129,78.5694891470441,78.7036523746276,79.4449290550123,79.4997277101804,79.8502772512157,79.9372394112819,80.1510224153782,80.3587357480158,80.3784214913221,81.6133776745324,81.7162764063825,81.8496362611066,82.0664218752943,82.0940556090974,82.4419651374795,83.3046026975628,83.4988996666592,83.6505571906114,84.1498224285073,84.3472236652502,84.3486449930161,85.0060946805807,85.0398895030862,86.2460157753518],[68.9965437972716,69.248046549853,69.5816085322189,71.1533985787128,71.2122014505988,71.3041001351142,71.7200201388608,72.3699560724738,75.2359329614328,75.3841942065531,75.5742059365697,76.7033986497151,76.7581358179714,76.7606534390454,76.879653213876,77.7931498356377,77.8135225906874,77.8898304725316,78.0112415951988,78.7162121883901,79.159778617645,79.2972438175477,79.4192327277365,79.4232697280039,79.6622070151603,80.5425835354018,80.573206580633,80.7073698082165,81.4486464886012,81.5034451437693,81.8539946848046,81.9409568448709,82.1547398489671,82.3624531816047,82.382138924911,83.6170951081213,83.7199938399715,83.8533536946955,84.0701393088833,84.0977730426863,84.4456825710684,85.3083201311517,85.5026171002481,85.6542746242003,86.1535398620962,86.3509410988391,86.352362426605,87.0098121141696,87.0436069366751,88.2497332089407],[70.8893737073552,71.1408764599367,71.4744384423025,73.0462284887964,73.1050313606825,73.1969300451978,73.6128500489444,74.2627859825574,77.1287628715165,77.2770241166368,77.4670358466534,78.5962285597987,78.6509657280551,78.6534833491291,78.7724831239596,79.6859797457214,79.7063525007711,79.7826603826152,79.9040715052825,80.6090420984737,81.0526085277286,81.1900737276313,81.3120626378202,81.3160996380876,81.555036925244,82.4354134454855,82.4660364907167,82.6001997183002,83.3414763986848,83.396275053853,83.7468245948883,83.8337867549545,84.0475697590507,84.2552830916884,84.2749688349947,85.509925018205,85.6128237500551,85.7461836047792,85.9629692189669,85.99060295277,86.3385124811521,87.2011500412354,87.3954470103318,87.5471045342839,88.0463697721799,88.2437710089228,88.2451923366887,88.9026420242533,88.9364368467588,90.1425631190244],[71.6670774100003,71.9185801625818,72.2521421449476,73.8239321914415,73.8827350633276,73.9746337478429,74.3905537515895,75.0404896852025,77.9064665741616,78.0547278192819,78.2447395492985,79.3739322624438,79.4286694307002,79.4311870517742,79.5501868266047,80.4636834483665,80.4840562034162,80.5603640852603,80.6817752079276,81.3867458011188,81.8303122303737,81.9677774302764,82.0897663404653,82.0938033407327,82.3327406278891,83.2131171481306,83.2437401933618,83.3779034209453,84.1191801013299,84.1739787564981,84.5245282975334,84.6114904575996,84.8252734616958,85.0329867943335,85.0526725376398,86.2876287208501,86.3905274527002,86.5238873074243,86.740672921612,86.7683066554151,87.1162161837972,87.9788537438805,88.1731507129768,88.324808236929,88.824073474825,89.0214747115679,89.0228960393338,89.6803457268984,89.7141405494039,90.9202668216695],[74.198222888388,74.4497256409695,74.7832876233353,76.3550776698293,76.4138805417153,76.5057792262307,76.9216992299773,77.5716351635903,80.4376120525493,80.5858732976696,80.7758850276862,81.9050777408315,81.9598149090879,81.9623325301619,82.0813323049925,82.9948289267542,83.0152016818039,83.091509563648,83.2129206863153,83.9178912795066,84.3614577087614,84.4989229086642,84.620911818853,84.6249488191204,84.8638861062768,85.7442626265183,85.7748856717495,85.909048899333,86.6503255797177,86.7051242348858,87.0556737759211,87.1426359359873,87.3564189400836,87.5641322727212,87.5838180160275,88.8187741992378,88.9216729310879,89.055032785812,89.2718183999997,89.2994521338028,89.6473616621849,90.5099992222682,90.7042961913646,90.8559537153168,91.3552189532127,91.5526201899556,91.5540415177215,92.2114912052861,92.2452860277916,93.4514123000572],[74.2390815510788,74.4905843036603,74.8241462860261,76.39593633252,76.454739204406,76.5466378889214,76.962557892668,77.612493826281,80.47847071524,80.6267319603604,80.8167436903769,81.9459364035223,82.0006735717786,82.0031911928526,82.1221909676832,83.0356875894449,83.0560603444946,83.1323682263388,83.253779349006,83.9587499421973,84.4023163714522,84.5397815713549,84.6617704815438,84.6658074818112,84.9047447689675,85.785121289209,85.8157443344402,85.9499075620237,86.6911842424084,86.7459828975765,87.0965324386118,87.1834945986781,87.3972776027743,87.6049909354119,87.6246766787183,88.8596328619285,88.9625315937787,89.0958914485028,89.3126770626905,89.3403107964935,89.6882203248757,90.5508578849589,90.7451548540553,90.8968123780075,91.3960776159034,91.5934788526464,91.5949001804123,92.2523498679769,92.2861446904824,93.4922709627479],[77.7184028302033,77.9699055827848,78.3034675651506,79.8752576116445,79.9340604835306,80.0259591680459,80.4418791717925,81.0918151054055,83.9577919943646,84.1060532394849,84.2960649695015,85.4252576826468,85.4799948509032,85.4825124719772,85.6015122468077,86.5150088685695,86.5353816236192,86.6116895054633,86.7331006281306,87.4380712213218,87.8816376505767,88.0191028504794,88.1410917606683,88.1451287609357,88.3840660480921,89.2644425683336,89.2950656135648,89.4292288411483,90.1705055215329,90.2253041767011,90.5758537177364,90.6628158778026,90.8765988818988,91.0843122145365,91.1039979578428,92.3389541410531,92.4418528729032,92.5752127276273,92.791998341815,92.8196320756181,93.1675416040002,94.0301791640835,94.2244761331798,94.376133657132,94.875398895028,95.0728001317709,95.0742214595368,95.7316711471014,95.7654659696069,96.9715922418725],[78.2627892573155,78.514292009897,78.8478539922628,80.4196440387567,80.4784469106428,80.5703455951581,80.9862655989047,81.6362015325177,84.5021784214768,84.6504396665971,84.8404513966137,85.969644109759,86.0243812780153,86.0268988990894,86.1458986739199,87.0593952956817,87.0797680507314,87.1560759325755,87.2774870552427,87.982457648434,88.4260240776889,88.5634892775916,88.6854781877805,88.6895151880479,88.9284524752042,89.8088289954458,89.8394520406769,89.9736152682605,90.7148919486451,90.7696906038133,91.1202401448486,91.2072023049148,91.420985309011,91.6286986416487,91.648384384955,92.8833405681652,92.9862393000154,93.1195991547395,93.3363847689272,93.3640185027303,93.7119280311124,94.5745655911957,94.768862560292,94.9205200842442,95.4197853221402,95.6171865588831,95.618607886649,96.2760575742136,96.3098523967191,97.5159786689847],[84.6171566421932,84.8686593947747,85.2022213771405,86.7740114236344,86.8328142955205,86.9247129800358,87.3406329837824,87.9905689173954,90.8565458063545,91.0048070514748,91.1948187814914,92.3240114946367,92.3787486628931,92.3812662839671,92.5002660587976,93.4137626805594,93.4341354356091,93.5104433174532,93.6318544401205,94.3368250333117,94.7803914625666,94.9178566624693,95.0398455726582,95.0438825729256,95.282819860082,96.1631963803235,96.1938194255547,96.3279826531382,97.0692593335229,97.124057988691,97.4746075297263,97.5615696897925,97.7753526938887,97.9830660265264,98.0027517698327,99.237707953043,99.3406066848931,99.4739665396172,99.6907521538049,99.718385887608,100.06629541599,100.928932976073,101.12322994517,101.274887469122,101.774152707018,101.971553943761,101.972975271527,102.630424959091,102.664219781597,103.870346053862]],"type":"surface","x":[-3.68720859601768,-3.39658880982085,-3.05780283806507,-2.8428614617323,-2.65793240843838,-2.63563151534892,-2.61685584917697,-2.35996957865719,-1.4234935907596,-1.35812256167027,-1.26714446436738,-0.715798288657035,-0.521265480318938,-0.43367109640646,-0.429506140129029,-0.3535563585683,-0.320074671287452,-0.290378867625921,-0.0172474728436934,0.0398649395013548,0.230673175344973,0.241938410104874,0.390255758037399,0.697040200103037,0.900883640006907,0.964431636964856,1.01420374229176,1.22249818487302,1.35237709188441,1.3830334723996,1.39620662171399,1.51056474867124,1.6730674847792,1.82219997656081,2.02101372710735,2.08777851377213,2.14146334483501,2.32900014331444,2.33525296768733,2.44360365602448,2.46747656291495,2.48833595026866,2.6696267028272,2.84088466221408,2.91124911937587,3.14026008714997,3.14395686474125,3.45875612860532,3.50801069085019,4.08293608997422],"y":[-3.73786254670458,-3.63747809054158,-3.5043406284154,-2.87697855225446,-2.85350805638705,-2.81682774425337,-2.650818016905,-2.39140349930287,-1.24748150075266,-1.188304715176,-1.11246369957469,-0.661759302708913,-0.639911586089848,-0.638906706343024,-0.591409303005398,-0.226797541738164,-0.218665988841759,-0.188208567834439,-0.13974870250099,0.141632271989487,0.318676754866431,0.373544422086501,0.422234904721515,0.42384622735005,0.519215321254824,0.870607571621903,0.882830410974694,0.936380133591274,1.23225226883807,1.25412452725694,1.39404237974094,1.42875233442496,1.51408138304498,1.59698779150616,1.60484513149822,2.09776381156926,2.13883466717866,2.19206373267812,2.27859123888164,2.28962092866078,2.42848505019461,2.77279699686677,2.85034841678827,2.91088078833771,3.11015681562905,3.1889472685881,3.18951457536047,3.45192812218369,3.46541694027223,3.94682849009638],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="#cb315-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">873636</span>)</span>
<span id="cb315-2"><a href="#cb315-2" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.99</span></span>
<span id="cb315-3"><a href="#cb315-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n,<span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span>T))</span>
<span id="cb315-4"><a href="#cb315-4" aria-hidden="true" tabindex="-1"></a>x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb315-5"><a href="#cb315-5" aria-hidden="true" tabindex="-1"></a>x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb315-6"><a href="#cb315-6" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb315-7"><a href="#cb315-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb315-8"><a href="#cb315-8" aria-hidden="true" tabindex="-1"></a>m2<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2)</span>
<span id="cb315-9"><a href="#cb315-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb315-10"><a href="#cb315-10" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">sort</span>(x1)</span>
<span id="cb315-11"><a href="#cb315-11" aria-hidden="true" tabindex="-1"></a>yy <span class="ot">&lt;-</span> <span class="fu">sort</span>(x2)</span>
<span id="cb315-12"><a href="#cb315-12" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="fu">predict</span>(m2,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x1=</span>a,<span class="at">x2=</span>b))})</span>
<span id="cb315-13"><a href="#cb315-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb315-14"><a href="#cb315-14" aria-hidden="true" tabindex="-1"></a>axx<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x1&quot;</span>)</span>
<span id="cb315-15"><a href="#cb315-15" aria-hidden="true" tabindex="-1"></a>axy<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x2&quot;</span>)</span>
<span id="cb315-16"><a href="#cb315-16" aria-hidden="true" tabindex="-1"></a>axz<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb315-17"><a href="#cb315-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb315-18"><a href="#cb315-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb315-19"><a href="#cb315-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x=</span>x1,</span>
<span id="cb315-20"><a href="#cb315-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>x2,</span>
<span id="cb315-21"><a href="#cb315-21" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span>y) <span class="sc">%&gt;%</span></span>
<span id="cb315-22"><a href="#cb315-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x=</span><span class="sc">~</span>xx,</span>
<span id="cb315-23"><a href="#cb315-23" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span><span class="sc">~</span>yy,</span>
<span id="cb315-24"><a href="#cb315-24" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span><span class="sc">~</span>fit,</span>
<span id="cb315-25"><a href="#cb315-25" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale=</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb315-26"><a href="#cb315-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis=</span>axx,<span class="at">yaxis=</span>axy,<span class="at">zaxis=</span>axz))</span></code></pre></div>
<div id="htmlwidget-db459357a85aa5faa523" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-db459357a85aa5faa523">{"x":{"visdat":{"35116888a76c":["function () ","plotlyVisDat"]},"cur_data":"35116888a76c","attrs":{"35116888a76c":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[0.853118653957231,-0.853261352273738,0.353507080881254,2.02809044634073,-0.748461580289934,-2.73214618289166,-2.29853927796707,-1.96018442660419,-0.454722470925065,-2.04807598083181,1.19988499344418,-0.49994358955681,-0.423969802044517,2.02928467971549,-0.452452431926527,2.02601727895909,0.460500326950301,-0.77453192186067,0.483705121872882,1.39816972373539,-0.0438096739725239,0.252370993505637,3.5049565576428,-1.49238162732587,0.17418874404757,1.95725589276825,-0.465071952665595,-0.987853308859119,-2.54136756927678,0.285227511214592,1.23982109570761,-1.48889585298465,-1.78161724252153,-2.1021094254153,0.218257037107956,-2.96039892327731,2.44172844471548,0.786071244541534,1.15306569690969,-0.798582679493088,2.10635286058609,-1.03089875877432,0.489524155075263,2.8160310859391,-2.33420760703804,-1.52934930101089,0.546362685903954,3.17470241843918,-2.0956060206783,-2.09856896020911],"y":[0.798140282104294,-0.435095605089509,0.366037622814148,2.01529354164428,-1.03935429943388,-2.96739065955215,-2.44805785324636,-1.68349143086038,-0.130308984218711,-1.72262255180219,0.881903485485141,-0.411981610050177,-0.440167129716625,1.81946134566535,-0.523163933743912,2.28814783885948,0.492833910352589,-0.998117849967946,0.88382555281705,1.6692090586459,-0.105847397513396,0.133116785819328,3.02482963380164,-1.81203543162327,0.0462986786367592,1.88898926684997,-0.181207502094158,-0.897975078198987,-2.24036718573331,0.466556294258567,1.39459445907624,-1.10053939641347,-2.1444613286036,-2.13427727000019,0.334158947616182,-2.13680049051357,2.3642827276904,0.574871193088133,1.33457241765193,-1.02197331724591,1.30737116637587,-1.19182468573189,0.794407315568089,3.15373189365332,-1.80220208993125,-1.24340778810319,0.778994609569731,2.97470866881919,-2.28546018563748,-2.01530932119005],"z":[53.0221153406486,39.7664146403462,52.0479361805679,82.6506213025628,37.5203175730443,16.609106244855,18.6280306060541,22.9691851963071,48.4907714408854,34.2709656315768,63.5199068101962,41.6572047273896,46.0899435645803,76.8089435737867,33.4257632639751,72.0565372585594,51.7210942434003,42.4602927515407,68.7869298404291,69.2295575750801,47.6731452582634,59.3982983155231,94.8410897136315,29.0108667202668,56.4003166374144,80.1945484246352,37.5483388157212,35.378731125902,19.7254580191492,49.3768135281181,67.424362045329,34.9332921260732,29.292206066946,26.1975700701937,55.1548681216452,7.38181320423015,80.4576488867466,64.4246699710494,63.3554120046682,36.4779925952663,65.909197622917,35.2082299720879,57.7384756711734,82.3028309566393,20.3260212546623,23.9903681599536,61.509792140678,89.8358621842948,23.1216472897601,15.4869531296032],"type":"scatter3d","mode":"markers","inherit":true},"35116888a76c.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.853118653957231,-0.853261352273738,0.353507080881254,2.02809044634073,-0.748461580289934,-2.73214618289166,-2.29853927796707,-1.96018442660419,-0.454722470925065,-2.04807598083181,1.19988499344418,-0.49994358955681,-0.423969802044517,2.02928467971549,-0.452452431926527,2.02601727895909,0.460500326950301,-0.77453192186067,0.483705121872882,1.39816972373539,-0.0438096739725239,0.252370993505637,3.5049565576428,-1.49238162732587,0.17418874404757,1.95725589276825,-0.465071952665595,-0.987853308859119,-2.54136756927678,0.285227511214592,1.23982109570761,-1.48889585298465,-1.78161724252153,-2.1021094254153,0.218257037107956,-2.96039892327731,2.44172844471548,0.786071244541534,1.15306569690969,-0.798582679493088,2.10635286058609,-1.03089875877432,0.489524155075263,2.8160310859391,-2.33420760703804,-1.52934930101089,0.546362685903954,3.17470241843918,-2.0956060206783,-2.09856896020911],"y":[0.798140282104294,-0.435095605089509,0.366037622814148,2.01529354164428,-1.03935429943388,-2.96739065955215,-2.44805785324636,-1.68349143086038,-0.130308984218711,-1.72262255180219,0.881903485485141,-0.411981610050177,-0.440167129716625,1.81946134566535,-0.523163933743912,2.28814783885948,0.492833910352589,-0.998117849967946,0.88382555281705,1.6692090586459,-0.105847397513396,0.133116785819328,3.02482963380164,-1.81203543162327,0.0462986786367592,1.88898926684997,-0.181207502094158,-0.897975078198987,-2.24036718573331,0.466556294258567,1.39459445907624,-1.10053939641347,-2.1444613286036,-2.13427727000019,0.334158947616182,-2.13680049051357,2.3642827276904,0.574871193088133,1.33457241765193,-1.02197331724591,1.30737116637587,-1.19182468573189,0.794407315568089,3.15373189365332,-1.80220208993125,-1.24340778810319,0.778994609569731,2.97470866881919,-2.28546018563748,-2.01530932119005],"z":[53.0221153406486,39.7664146403462,52.0479361805679,82.6506213025628,37.5203175730443,16.609106244855,18.6280306060541,22.9691851963071,48.4907714408854,34.2709656315768,63.5199068101962,41.6572047273896,46.0899435645803,76.8089435737867,33.4257632639751,72.0565372585594,51.7210942434003,42.4602927515407,68.7869298404291,69.2295575750801,47.6731452582634,59.3982983155231,94.8410897136315,29.0108667202668,56.4003166374144,80.1945484246352,37.5483388157212,35.378731125902,19.7254580191492,49.3768135281181,67.424362045329,34.9332921260732,29.292206066946,26.1975700701937,55.1548681216452,7.38181320423015,80.4576488867466,64.4246699710494,63.3554120046682,36.4779925952663,65.909197622917,35.2082299720879,57.7384756711734,82.3028309566393,20.3260212546623,23.9903681599536,61.509792140678,89.8358621842948,23.1216472897601,15.4869531296032],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[11.3222442786923,12.9087579894233,13.4054788317467,13.5432337775874,13.8362173045825,13.8596204566999,13.8673286620955,14.2307647470743,14.8517476869455,14.8817876367181,15.1248957685411,15.24443772533,16.5888525680381,16.7464341769032,17.0253022964082,17.2122171110933,17.2653144047985,17.3381906518849,17.6441175613983,18.7891309562629,19.0426785194774,19.0581715583487,19.1287826762954,19.8337762409392,19.9892665074596,20.0639943933605,20.5287865900314,20.7940078811042,21.4081730989624,21.5055595044955,21.8126347527654,21.8929104405561,22.1435267549324,22.7671049209893,22.8141893129032,22.825593180557,23.0814820229426,23.0873537609457,24.3812464534004,24.4643437612818,24.6477055487705,25.4866277622223,25.9456346081806,26.1580359795144,26.5438838621285,27.3774285146281,27.6100135613046,29.4748083773219,29.6279232916393,30.0217077784188],[13.5769884393806,15.1635021501116,15.660222992435,15.7979779382757,16.0909614652708,16.1143646173882,16.1220728227838,16.4855089077626,17.1064918476338,17.1365317974064,17.3796399292294,17.4991818860183,18.8435967287264,19.0011783375915,19.2800464570965,19.4669612717816,19.5200585654868,19.5929348125732,19.8988617220866,21.0438751169512,21.2974226801657,21.312915719037,21.3835268369837,22.0885204016274,22.2440106681479,22.3187385540488,22.7835307507197,23.0487520417925,23.6629172596507,23.7603036651838,24.0673789134537,24.1476546012444,24.3982709156207,25.0218490816776,25.0689334735915,25.0803373412453,25.3362261836309,25.342097921634,26.6359906140887,26.7190879219701,26.9024497094588,27.7413719229106,28.2003787688688,28.4127801402027,28.7986280228168,29.6321726753164,29.8647577219929,31.7295525380102,31.8826674523276,32.2764519391071],[15.4615527355445,17.0480664462756,17.544787288599,17.6825422344397,17.9755257614347,17.9989289135521,18.0066371189477,18.3700732039265,18.9910561437977,19.0210960935703,19.2642042253934,19.3837461821822,20.7281610248903,20.8857426337555,21.1646107532605,21.3515255679455,21.4046228616508,21.4774991087371,21.7834260182505,22.9284394131152,23.1819869763296,23.1974800152009,23.2680911331477,23.9730846977914,24.1285749643118,24.2033028502128,24.6680950468837,24.9333163379565,25.5474815558147,25.6448679613478,25.9519432096176,26.0322188974084,26.2828352117847,26.9064133778415,26.9534977697555,26.9649016374093,27.2207904797949,27.226662217798,28.5205549102527,28.6036522181341,28.7870140056228,29.6259362190746,30.0849430650328,30.2973444363667,30.6831923189808,31.5167369714804,31.7493220181569,33.6141168341741,33.7672317484916,34.1610162352711],[17.5079365572547,19.0944502679857,19.5911711103091,19.7289260561498,20.0219095831449,20.0453127352623,20.0530209406579,20.4164570256367,21.0374399655079,21.0674799152805,21.3105880471035,21.4301300038924,22.7745448466005,22.9321264554656,23.2109945749706,23.3979093896557,23.4510066833609,23.5238829304473,23.8298098399607,24.9748232348253,25.2283707980398,25.2438638369111,25.3144749548578,26.0194685195015,26.174958786022,26.2496866719229,26.7144788685938,26.9797001596666,27.5938653775248,27.6912517830579,27.9983270313278,28.0786027191185,28.3292190334948,28.9527971995517,28.9998815914656,29.0112854591194,29.267174301505,29.2730460395081,30.5669387319628,30.6500360398442,30.8333978273329,31.6723200407847,32.1313268867429,32.3437282580768,32.7295761406909,33.5631207931905,33.795705839867,35.6605006558843,35.8136155702017,36.2074000569812],[17.8602782490056,19.4467919597367,19.9435128020601,20.0812677479008,20.3742512748958,20.3976544270132,20.4053626324088,20.7687987173876,21.3897816572588,21.4198216070314,21.6629297388544,21.7824716956433,23.1268865383514,23.2844681472166,23.5633362667215,23.7502510814066,23.8033483751119,23.8762246221982,24.1821515317116,25.3271649265763,25.5807124897907,25.596205528662,25.6668166466088,26.3718102112525,26.5273004777729,26.6020283636738,27.0668205603448,27.3320418514175,27.9462070692758,28.0435934748088,28.3506687230787,28.4309444108695,28.6815607252458,29.3051388913026,29.3522232832165,29.3636271508704,29.619515993256,29.625387731259,30.9192804237138,31.0023777315952,31.1857395190839,32.0246617325357,32.4836685784939,32.6960699498278,33.0819178324419,33.9154624849415,34.1480475316179,36.0128423476352,36.1659572619526,36.5597417487322],[19.8006670578219,21.387180768553,21.8839016108764,22.0216565567171,22.3146400837122,22.3380432358295,22.3457514412251,22.7091875262039,23.3301704660751,23.3602104158478,23.6033185476708,23.7228605044596,25.0672753471678,25.2248569560329,25.5037250755379,25.6906398902229,25.7437371839282,25.8166134310145,26.122540340528,27.2675537353926,27.5211012986071,27.5365943374783,27.6072054554251,28.3121990200688,28.4676892865892,28.5424171724902,29.0072093691611,29.2724306602339,29.8865958780921,29.9839822836252,30.291057531895,30.3713332196858,30.6219495340621,31.2455277001189,31.2926120920329,31.3040159596867,31.5599048020723,31.5657765400754,32.8596692325301,32.9427665404115,33.1261283279002,33.965050541352,34.4240573873102,34.6364587586441,35.0223066412582,35.8558512937578,36.0884363404343,37.9532311564515,38.106346070769,38.5001305575485],[19.8356407595015,21.4221544702326,21.9188753125559,22.0566302583966,22.3496137853917,22.3730169375091,22.3807251429047,22.7441612278835,23.3651441677547,23.3951841175273,23.6382922493503,23.7578342061392,25.1022490488473,25.2598306577124,25.5386987772174,25.7256135919025,25.7787108856077,25.8515871326941,26.1575140422075,27.3025274370721,27.5560750002866,27.5715680391579,27.6421791571046,28.3471727217484,28.5026629882688,28.5773908741697,29.0421830708406,29.3074043619134,29.9215695797716,30.0189559853047,30.3260312335746,30.4063069213653,30.6569232357416,31.2805014017985,31.3275857937124,31.3389896613662,31.5948785037518,31.6007502417549,32.8946429342096,32.977740242091,33.1611020295797,34.0000242430315,34.4590310889898,34.6714324603236,35.0572803429377,35.8908249954373,36.1234100421138,37.9882048581311,38.1413197724485,38.535104259228],[19.864909501661,21.4514232123921,21.9481440547155,22.0858990005562,22.3788825275512,22.4022856796686,22.4099938850642,22.773429970043,23.3944129099142,23.4244528596868,23.6675609915098,23.7871029482987,25.1315177910068,25.289099399872,25.5679675193769,25.754882334062,25.8079796277673,25.8808558748536,26.186782784367,27.3317961792317,27.5853437424461,27.6008367813174,27.6714478992642,28.3764414639079,28.5319317304283,28.6066596163292,29.0714518130002,29.3366731040729,29.9508383219312,30.0482247274642,30.3552999757341,30.4355756635249,30.6861919779012,31.309770143958,31.3568545358719,31.3682584035258,31.6241472459114,31.6300189839144,32.9239116763692,33.0070089842506,33.1903707717393,34.0292929851911,34.4882998311493,34.7007012024832,35.0865490850973,35.9200937375969,36.1526787842733,38.0174736002906,38.170588514608,38.5643730013876],[20.3344244771224,21.9209381878534,22.4176590301768,22.5554139760175,22.8483975030126,22.87180065513,22.8795088605256,23.2429449455044,23.8639278853756,23.8939678351482,24.1370759669712,24.2566179237601,25.6010327664682,25.7586143753333,26.0374824948383,26.2243973095234,26.2774946032286,26.3503708503149,26.6562977598284,27.801311154693,28.0548587179075,28.0703517567788,28.1409628747255,28.8459564393692,29.0014467058897,29.0761745917906,29.5409667884615,29.8061880795343,30.4203532973925,30.5177397029256,30.8248149511955,30.9050906389862,31.1557069533625,31.7792851194194,31.8263695113333,31.8377733789871,32.0936622213727,32.0995339593758,33.3934266518305,33.4765239597119,33.6598857472006,34.4988079606524,34.9578148066106,35.1702161779445,35.5560640605586,36.3896087130582,36.6221937597347,38.4869885757519,38.6401034900694,39.0338879768489],[21.2026417363488,22.7891554470799,23.2858762894033,23.423631235244,23.716614762239,23.7400179143564,23.747726119752,24.1111622047308,24.732145144602,24.7621850943746,25.0052932261976,25.1248351829865,26.4692500256946,26.6268316345598,26.9056997540647,27.0926145687498,27.1457118624551,27.2185881095414,27.5245150190548,28.6695284139195,28.9230759771339,28.9385690160052,29.009180133952,29.7141736985957,29.8696639651161,29.944391851017,30.409184047688,30.6744053387607,31.288570556619,31.385956962152,31.6930322104219,31.7733078982127,32.023924212589,32.6475023786458,32.6945867705598,32.7059906382136,32.9618794805992,32.9677512186022,34.261643911057,34.3447412189384,34.5281030064271,35.3670252198789,35.8260320658371,36.038433437171,36.4242813197851,37.2578259722847,37.4904110189611,39.3552058349784,39.5083207492958,39.9021052360754],[22.9665781294921,24.5530918402231,25.0498126825465,25.1875676283872,25.4805511553823,25.5039543074997,25.5116625128953,25.8750985978741,26.4960815377453,26.5261214875179,26.7692296193409,26.8887715761298,28.2331864188379,28.390768027703,28.669636147208,28.8565509618931,28.9096482555983,28.9825245026846,29.2884514121981,30.4334648070627,30.6870123702772,30.7025054091485,30.7731165270952,31.4781100917389,31.6336003582594,31.7083282441603,32.1731204408312,32.438341731904,33.0525069497622,33.1498933552953,33.4569686035652,33.5372442913559,33.7878606057322,34.4114387717891,34.458523163703,34.4699270313568,34.7258158737424,34.7316876117455,36.0255803042002,36.1086776120816,36.2920393995703,37.1309616130221,37.5899684589803,37.8023698303142,38.1882177129283,39.0217623654279,39.2543474121044,41.1191422281217,41.2722571424391,41.6660416292186],[25.4585511375223,27.0450648482534,27.5417856905768,27.6795406364175,27.9725241634125,27.9959273155299,28.0036355209255,28.3670716059043,28.9880545457755,29.0180944955481,29.2612026273712,29.38074458416,30.7251594268681,30.8827410357333,31.1616091552383,31.3485239699233,31.4016212636286,31.4744975107149,31.7804244202283,32.925437815093,33.1789853783074,33.1944784171787,33.2650895351255,33.9700830997692,34.1255733662896,34.2003012521905,34.6650934488615,34.9303147399343,35.5444799577925,35.6418663633256,35.9489416115954,36.0292172993862,36.2798336137625,36.9034117798193,36.9504961717333,36.9619000393871,37.2177888817727,37.2236606197758,38.5175533122305,38.6006506201119,38.7840124076006,39.6229346210524,40.0819414670106,40.2943428383445,40.6801907209586,41.5137353734582,41.7463204201347,43.6111152361519,43.7642301504694,44.1580146372489],[25.8237281173679,27.410241828099,27.9069626704224,28.0447176162631,28.3377011432581,28.3611042953755,28.3688125007711,28.7322485857499,29.3532315256211,29.3832714753937,29.6263796072168,29.7459215640056,31.0903364067137,31.2479180155789,31.5267861350839,31.7137009497689,31.7667982434742,31.8396744905605,32.1456014000739,33.2906147949386,33.544162358153,33.5596553970243,33.6302665149711,34.3352600796148,34.4907503461352,34.5654782320361,35.0302704287071,35.2954917197799,35.9096569376381,36.0070433431712,36.314118591441,36.3943942792318,36.6450105936081,37.2685887596649,37.3156731515789,37.3270770192327,37.5829658616183,37.5888375996214,38.8827302920761,38.9658275999575,39.1491893874462,39.988111600898,40.4471184468562,40.6595198181901,41.0453677008042,41.8789123533038,42.1114973999803,43.9762922159975,44.129407130315,44.5231916170945],[25.8581615674464,27.4446752781775,27.9413961205009,28.0791510663416,28.3721345933367,28.395537745454,28.4032459508497,28.7666820358285,29.3876649756996,29.4177049254723,29.6608130572953,29.7803550140842,31.1247698567923,31.2823514656574,31.5612195851624,31.7481343998474,31.8012316935527,31.874107940639,32.1800348501525,33.3250482450171,33.5785958082316,33.5940888471028,33.6646999650496,34.3696935296933,34.5251837962138,34.5999116821147,35.0647038787856,35.3299251698584,35.9440903877166,36.0414767932497,36.3485520415196,36.4288277293103,36.6794440436866,37.3030222097435,37.3501066016574,37.3615104693112,37.6173993116968,37.6232710496999,38.9171637421546,39.000261050036,39.1836228375247,40.0225450509765,40.4815518969347,40.6939532682686,41.0798011508827,41.9133458033823,42.1459308500588,44.010725666076,44.1638405803935,44.557625067173],[30.3823844619369,31.968898172668,32.4656190149914,32.6033739608321,32.8963574878271,32.9197606399445,32.9274688453401,33.2909049303189,33.9118878701901,33.9419278199627,34.1850359517857,34.3045779085746,35.6489927512827,35.8065743601479,36.0854424796528,36.2723572943379,36.3254545880432,36.3983308351295,36.7042577446429,37.8492711395076,38.102818702722,38.1183117415933,38.1889228595401,38.8939164241838,39.0494066907042,39.1241345766051,39.5889267732761,39.8541480643489,40.4683132822071,40.5656996877401,40.87277493601,40.9530506238008,41.2036669381771,41.8272451042339,41.8743294961479,41.8857333638017,42.1416222061873,42.1474939441904,43.4413866366451,43.5244839445265,43.7078457320152,44.546767945467,45.0057747914252,45.2181761627591,45.6040240453732,46.4375686978728,46.6701537445492,48.5349485605665,48.6880634748839,49.0818479616635],[30.8075994086513,32.3941131193824,32.8908339617058,33.0285889075465,33.3215724345416,33.3449755866589,33.3526837920545,33.7161198770333,34.3371028169045,34.3671427666772,34.6102508985002,34.729792855289,36.0742076979972,36.2317893068623,36.5106574263673,36.6975722410523,36.7506695347576,36.8235457818439,37.1294726913573,38.274486086222,38.5280336494364,38.5435266883077,38.6141378062545,39.3191313708982,39.4746216374186,39.5493495233196,40.0141417199905,40.2793630110633,40.8935282289215,40.9909146344546,41.2979898827244,41.3782655705152,41.6288818848915,42.2524600509483,42.2995444428623,42.3109483105161,42.5668371529017,42.5727088909048,43.8666015833595,43.9496988912409,44.1330606787296,44.9719828921814,45.4309897381396,45.6433911094735,46.0292389920876,46.8627836445872,47.0953686912637,48.9601635072809,49.1132784215984,49.5070629083779],[32.1371362530886,33.7236499638197,34.2203708061431,34.3581257519838,34.6511092789789,34.6745124310962,34.6822206364918,35.0456567214706,35.6666396613418,35.6966796111144,35.9397877429375,36.0593296997263,37.4037445424344,37.5613261512996,37.8401942708046,38.0271090854896,38.0802063791949,38.1530826262812,38.4590095357946,39.6040229306593,39.8575704938737,39.873063532745,39.9436746506918,40.6486682153355,40.8041584818559,40.8788863677568,41.3436785644278,41.6088998555006,42.2230650733588,42.3204514788919,42.6275267271617,42.7078024149525,42.9584187293288,43.5819968953856,43.6290812872996,43.6404851549534,43.896373997339,43.9022457353421,45.1961384277968,45.2792357356782,45.4625975231669,46.3015197366187,46.7605265825769,46.9729279539108,47.3587758365249,48.1923204890245,48.424905535701,50.2897003517182,50.4428152660357,50.8365997528152],[32.677267416281,34.2637811270121,34.7605019693355,34.8982569151762,35.1912404421713,35.2146435942886,35.2223517996842,35.585787884663,36.2067708245342,36.2368107743069,36.4799189061299,36.5994608629187,37.9438757056269,38.101457314492,38.380325433997,38.567240248682,38.6203375423873,38.6932137894736,38.999140698987,40.1441540938517,40.3977016570661,40.4131946959374,40.4838058138842,41.1887993785279,41.3442896450483,41.4190175309493,41.8838097276202,42.149031018693,42.7631962365512,42.8605826420843,43.1676578903541,43.2479335781449,43.4985498925212,44.122128058578,44.169212450492,44.1806163181458,44.4365051605314,44.4423768985345,45.7362695909892,45.8193668988706,46.0027286863593,46.8416508998111,47.3006577457693,47.5130591171032,47.8989069997173,48.7324516522169,48.9650366988934,50.8298315149106,50.9829464292281,51.3767309160076],[32.9148475007094,34.5013612114405,34.9980820537639,35.1358369996046,35.4288205265996,35.452223678717,35.4599318841126,35.8233679690914,36.4443509089626,36.4743908587352,36.7174989905582,36.8370409473471,38.1814557900552,38.3390373989204,38.6179055184253,38.8048203331104,38.8579176268156,38.930793873902,39.2367207834154,40.3817341782801,40.6352817414945,40.6507747803658,40.7213858983125,41.4263794629563,41.5818697294767,41.6565976153776,42.1213898120486,42.3866111031213,43.0007763209796,43.0981627265126,43.4052379747825,43.4855136625732,43.7361299769495,44.3597081430064,44.4067925349203,44.4181964025741,44.6740852449598,44.6799569829628,45.9738496754176,46.056946983299,46.2403087707876,47.0792309842394,47.5382378301977,47.7506392015316,48.1364870841457,48.9700317366453,49.2026167833217,51.067411599339,51.2205265136564,51.6143110004359],[33.172377597936,34.7588913086671,35.2556121509905,35.3933670968312,35.6863506238263,35.7097537759436,35.7174619813392,36.080898066318,36.7018810061892,36.7319209559619,36.9750290877849,37.0945710445737,38.4389858872819,38.596567496147,38.875435615652,39.062350430337,39.1154477240423,39.1883239711286,39.494250880642,40.6392642755067,40.8928118387211,40.9083048775924,40.9789159955392,41.6839095601829,41.8393998267033,41.9141277126043,42.3789199092752,42.644141200348,43.2583064182062,43.3556928237393,43.6627680720091,43.7430437597999,43.9936600741762,44.617238240233,44.664322632147,44.6757264998008,44.9316153421864,44.9374870801895,46.2313797726442,46.3144770805256,46.4978388680143,47.3367610814661,47.7957679274243,48.0081692987582,48.3940171813723,49.2275618338719,49.4601468805484,51.3249416965656,51.4780566108831,51.8718410976626],[35.6273075474506,37.2138212581817,37.7105421005051,37.8482970463458,38.1412805733408,38.1646837254582,38.1723919308538,38.5358280158326,39.1568109557038,39.1868509054764,39.4299590372994,39.5495009940883,40.8939158367964,41.0514974456616,41.3303655651665,41.5172803798516,41.5703776735568,41.6432539206432,41.9491808301566,43.0941942250213,43.3477417882357,43.363234827107,43.4338459450537,44.1388395096975,44.2943297762179,44.3690576621188,44.8338498587898,45.0990711498625,45.7132363677208,45.8106227732538,46.1176980215237,46.1979737093144,46.4485900236907,47.0721681897476,47.1192525816615,47.1306564493153,47.386545291701,47.392417029704,48.6863097221587,48.7694070300402,48.9527688175288,49.7916910309806,50.2506978769389,50.4630992482728,50.8489471308868,51.6824917833865,51.9150768300629,53.7798716460802,53.9329865603976,54.3267710471771],[35.9717792918917,37.5582930026228,38.0550138449462,38.1927687907869,38.4857523177819,38.5091554698993,38.5168636752949,38.8802997602737,39.5012827001449,39.5313226499175,39.7744307817405,39.8939727385294,41.2383875812375,41.3959691901027,41.6748373096076,41.8617521242927,41.9148494179979,41.9877256650843,42.2936525745977,43.4386659694624,43.6922135326768,43.7077065715481,43.7783176894948,44.4833112541386,44.638801520659,44.7135294065599,45.1783216032309,45.4435428943036,46.0577081121619,46.1550945176949,46.4621697659648,46.5424454537555,46.7930617681319,47.4166399341887,47.4637243261026,47.4751281937564,47.7310170361421,47.7368887741451,49.0307814665999,49.1138787744813,49.2972405619699,50.1361627754217,50.59516962138,50.8075709927139,51.193418875328,52.0269635278276,52.259548574504,54.1243433905213,54.2774583048387,54.6712427916183],[36.0740143558878,37.6605280666189,38.1572489089423,38.295003854783,38.587987381778,38.6113905338954,38.619098739291,38.9825348242698,39.603517764141,39.6335577139136,39.8766658457366,39.9962078025255,41.3406226452336,41.4982042540988,41.7770723736037,41.9639871882888,42.017084481994,42.0899607290804,42.3958876385938,43.5409010334585,43.7944485966729,43.8099416355442,43.8805527534909,44.5855463181347,44.7410365846551,44.815764470556,45.280556667227,45.5457779582997,46.159943176158,46.257329581691,46.5644048299609,46.6446805177516,46.8952968321279,47.5188749981848,47.5659593900987,47.5773632577525,47.8332521001382,47.8391238381412,49.133016530596,49.2161138384774,49.399475625966,50.2383978394178,50.6974046853761,50.90980605671,51.2956539393241,52.1291985918237,52.3617836385001,54.2265784545174,54.3796933688348,54.7734778556143],[36.0964384335477,37.6829521442788,38.1796729866022,38.3174279324428,38.6104114594379,38.6338146115553,38.6415228169509,39.0049589019297,39.6259418418009,39.6559817915735,39.8990899233965,40.0186318801854,41.3630467228935,41.5206283317586,41.7994964512636,41.9864112659487,42.0395085596539,42.1123848067403,42.4183117162537,43.5633251111183,43.8168726743328,43.8323657132041,43.9029768311508,44.6079703957946,44.763460662315,44.8381885482159,45.3029807448868,45.5682020359596,46.1823672538178,46.2797536593509,46.5868289076208,46.6671045954115,46.9177209097878,47.5412990758447,47.5883834677586,47.5997873354124,47.855676177798,47.8615479158011,49.1554406082558,49.2385379161372,49.4218997036259,50.2608219170777,50.719828763036,50.9322301343698,51.3180780169839,52.1516226694835,52.38420771616,54.2490025321773,54.4021174464947,54.7959019332742],[36.377797786776,37.9643114975071,38.4610323398305,38.5987872856712,38.8917708126663,38.9151739647836,38.9228821701792,39.286318255158,39.9073011950292,39.9373411448019,40.1804492766249,40.2999912334137,41.6444060761219,41.801987684987,42.080855804492,42.267770619177,42.3208679128823,42.3937441599686,42.6996710694821,43.8446844643467,44.0982320275611,44.1137250664324,44.1843361843792,44.8893297490229,45.0448200155433,45.1195479014443,45.5843400981152,45.849561389188,46.4637266070462,46.5611130125793,46.8681882608491,46.9484639486399,47.1990802630162,47.822658429073,47.869742820987,47.8811466886408,48.1370355310264,48.1429072690295,49.4367999614842,49.5198972693656,49.7032590568543,50.5421812703061,51.0011881162643,51.2135894875982,51.5994373702123,52.4329820227119,52.6655670693884,54.5303618854056,54.6834767997231,55.0772612865026],[40.1331254443919,41.719639155123,42.2163599974464,42.3541149432871,42.6470984702821,42.6705016223995,42.6782098277951,43.0416459127739,43.6626288526451,43.6926688024177,43.9357769342407,44.0553188910296,45.3997337337377,45.5573153426029,45.8361834621078,46.0230982767929,46.0761955704981,46.1490718175845,46.4549987270979,47.6000121219626,47.853559685177,47.8690527240483,47.939663841995,48.6446574066388,48.8001476731592,48.8748755590601,49.3396677557311,49.6048890468038,50.2190542646621,50.3164406701951,50.623515918465,50.7037916062557,50.9544079206321,51.5779860866889,51.6250704786028,51.6364743462566,51.8923631886423,51.8982349266453,53.1921276191001,53.2752249269815,53.4585867144701,54.2975089279219,54.7565157738802,54.9689171452141,55.3547650278282,56.1883096803278,56.4208947270042,58.2856895430215,58.4388044573389,58.8325889441184],[42.2865745519015,43.8730882626326,44.369809104956,44.5075640507967,44.8005475777917,44.8239507299091,44.8316589353047,45.1950950202835,45.8160779601547,45.8461179099273,46.0892260417503,46.2087679985392,47.5531828412473,47.7107644501124,47.9896325696174,48.1765473843025,48.2296446780077,48.3025209250941,48.6084478346075,49.7534612294722,50.0070087926866,50.0225018315579,50.0931129495046,50.7981065141484,50.9535967806688,51.0283246665697,51.4931168632407,51.7583381543134,52.3725033721716,52.4698897777047,52.7769650259746,52.8572407137653,53.1078570281416,53.7314351941985,53.7785195861124,53.7899234537662,54.0458122961519,54.0516840341549,55.3455767266096,55.4286740344911,55.6120358219797,56.4509580354315,56.9099648813898,57.1223662527237,57.5082141353377,58.3417587878373,58.5743438345138,60.4391386505311,60.5922535648485,60.986038051628],[42.7218934281679,44.3084071388989,44.8051279812223,44.942882927063,45.2358664540581,45.2592696061755,45.2669778115711,45.6304138965499,46.2513968364211,46.2814367861937,46.5245449180167,46.6440868748056,47.9885017175137,48.1460833263788,48.4249514458838,48.6118662605689,48.6649635542741,48.7378398013605,49.0437667108739,50.1887801057385,50.442327668953,50.4578207078243,50.528431825771,51.2334253904148,51.3889156569352,51.4636435428361,51.928435739507,52.1936570305798,52.807822248438,52.9052086539711,53.212283902241,53.2925595900317,53.543175904408,54.1667540704649,54.2138384623788,54.2252423300326,54.4811311724182,54.4870029104213,55.780895602876,55.8639929107574,56.0473546982461,56.8862769116979,57.3452837576562,57.55768512899,57.9435330116041,58.7770776641037,59.0096627107802,60.8744575267975,61.0275724411149,61.4213569278944],[43.0588805936043,44.6453943043353,45.1421151466587,45.2798700924994,45.5728536194945,45.5962567716119,45.6039649770075,45.9674010619863,46.5883840018575,46.6184239516301,46.8615320834531,46.981074040242,48.3254888829501,48.4830704918152,48.7619386113202,48.9488534260053,49.0019507197105,49.0748269667968,49.3807538763103,50.5257672711749,50.7793148343894,50.7948078732607,50.8654189912074,51.5704125558511,51.7259028223716,51.8006307082725,52.2654229049434,52.5306441960162,53.1448094138744,53.2421958194075,53.5492710676774,53.6295467554681,53.8801630698444,54.5037412359013,54.5508256278152,54.562229495469,54.8181183378546,54.8239900758577,56.1178827683124,56.2009800761938,56.3843418636825,57.2232640771343,57.6822709230925,57.8946722944264,58.2805201770405,59.1140648295401,59.3466498762166,61.2114446922338,61.3645596065513,61.7583440933308],[43.3834464293513,44.9699601400824,45.4666809824058,45.6044359282465,45.8974194552415,45.9208226073589,45.9285308127545,46.2919668977333,46.9129498376045,46.9429897873771,47.1860979192001,47.305639875989,48.6500547186971,48.8076363275622,49.0865044470672,49.2734192617523,49.3265165554575,49.3993928025439,49.7053197120573,50.850333106922,51.1038806701364,51.1193737090077,51.1899848269544,51.8949783915982,52.0504686581186,52.1251965440195,52.5899887406905,52.8552100317632,53.4693752496215,53.5667616551545,53.8738369034244,53.9541125912151,54.2047289055914,54.8283070716483,54.8753914635622,54.886795331216,55.1426841736017,55.1485559116047,56.4424486040594,56.5255459119409,56.7089076994295,57.5478299128813,58.0068367588396,58.2192381301735,58.6050860127875,59.4386306652871,59.6712157119636,61.5360105279809,61.6891254422983,62.0829099290778],[44.0579310411903,45.6444447519214,46.1411655942448,46.2789205400855,46.5719040670806,46.5953072191979,46.6030154245936,46.9664515095723,47.5874344494435,47.6174743992162,47.8605825310392,47.980124487828,49.3245393305362,49.4821209394013,49.7609890589063,49.9479038735913,50.0010011672966,50.0738774143829,50.3798043238964,51.524817718761,51.7783652819754,51.7938583208467,51.8644694387935,52.5694630034372,52.7249532699576,52.7996811558586,53.2644733525295,53.5296946436023,54.1438598614605,54.2412462669936,54.5483215152634,54.6285972030542,54.8792135174305,55.5027916834873,55.5498760754013,55.5612799430551,55.8171687854407,55.8230405234438,57.1169332158985,57.2000305237799,57.3833923112686,58.2223145247204,58.6813213706786,58.8937227420125,59.2795706246266,60.1131152771262,60.3457003238027,62.2104951398199,62.3636100541374,62.7573945409169],[45.1148401341531,46.7013538448842,47.1980746872076,47.3358296330483,47.6288131600434,47.6522163121607,47.6599245175563,48.0233606025351,48.6443435424063,48.674383492179,48.917491624002,49.0370335807908,50.381448423499,50.5390300323641,50.8178981518691,51.0048129665541,51.0579102602594,51.1307865073457,51.4367134168591,52.5817268117238,52.8352743749382,52.8507674138095,52.9213785317563,53.6263720964,53.7818623629204,53.8565902488214,54.3213824454923,54.5866037365651,55.2007689544233,55.2981553599564,55.6052306082262,55.685506296017,55.9361226103933,56.5597007764501,56.6067851683641,56.6181890360179,56.8740778784035,56.8799496164066,58.1738423088613,58.2569396167427,58.4403014042314,59.2792236176832,59.7382304636414,59.9506318349753,60.3364797175894,61.170024370089,61.4026094167655,63.2674042327827,63.4205191471002,63.8143036338797],[45.3440635632487,46.9305772739798,47.4272981163032,47.5650530621439,47.8580365891389,47.8814397412563,47.8891479466519,48.2525840316307,48.8735669715019,48.9036069212745,49.1467150530975,49.2662570098864,50.6106718525945,50.7682534614597,51.0471215809646,51.2340363956497,51.2871336893549,51.3600099364413,51.6659368459547,52.8109502408194,53.0644978040338,53.0799908429051,53.1506019608518,53.8555955254956,54.011085792016,54.0858136779169,54.5506058745879,54.8158271656606,55.4299923835189,55.5273787890519,55.8344540373218,55.9147297251126,56.1653460394889,56.7889242055457,56.8360085974596,56.8474124651134,57.1033013074991,57.1091730455021,58.4030657379569,58.4861630458383,58.6695248333269,59.5084470467787,59.967453892737,60.1798552640709,60.565703146685,61.3992477991846,61.631832845861,63.4966276618783,63.6497425761957,64.0435270629753],[45.4015455944031,46.9880593051342,47.4847801474576,47.6225350932982,47.9155186202933,47.9389217724107,47.9466299778063,48.3100660627851,48.9310490026563,48.9610889524289,49.2041970842519,49.3237390410408,50.6681538837489,50.825735492614,51.104603612119,51.2915184268041,51.3446157205093,51.4174919675957,51.7234188771091,52.8684322719737,53.1219798351882,53.1374728740595,53.2080839920062,53.91307755665,54.0685678231704,54.1432957090713,54.6080879057422,54.873309196815,55.4874744146732,55.5848608202063,55.8919360684762,55.9722117562669,56.2228280706432,56.8464062367001,56.893490628614,56.9048944962678,57.1607833386534,57.1666550766565,58.4605477691112,58.5436450769926,58.7270068644813,59.5659290779331,60.0249359238914,60.2373372952252,60.6231851778393,61.4567298303389,61.6893148770154,63.5541096930327,63.7072246073501,64.1010090941296],[45.96301243713,47.5495261478611,48.0462469901844,48.1840019360251,48.4769854630202,48.5003886151376,48.5080968205332,48.871532905512,49.4925158453832,49.5225557951558,49.7656639269788,49.8852058837677,51.2296207264758,51.3872023353409,51.6660704548459,51.852985269531,51.9060825632362,51.9789588103226,52.284885719836,53.4298991147006,53.6834466779151,53.6989397167864,53.7695508347331,54.4745443993769,54.6300346658973,54.7047625517982,55.1695547484691,55.4347760395419,56.0489412574001,56.1463276629332,56.4534029112031,56.5336785989938,56.7842949133701,57.407873079427,57.4549574713409,57.4663613389947,57.7222501813803,57.7281219193834,59.0220146118381,59.1051119197195,59.2884737072082,60.12739592066,60.5864027666183,60.7988041379521,61.1846520205662,62.0181966730658,62.2507817197423,64.1155765357596,64.268691450077,64.6624759368565],[48.3309203620991,49.9174340728301,50.4141549151535,50.5519098609942,50.8448933879893,50.8682965401067,50.8760047455023,51.2394408304811,51.8604237703523,51.8904637201249,52.1335718519479,52.2531138087368,53.5975286514449,53.75511026031,54.033978379815,54.2208931945001,54.2739904882053,54.3468667352916,54.6527936448051,55.7978070396697,56.0513546028842,56.0668476417555,56.1374587597022,56.8424523243459,56.9979425908664,57.0726704767673,57.5374626734382,57.802683964511,58.4168491823692,58.5142355879023,58.8213108361722,58.9015865239629,59.1522028383392,59.7757810043961,59.82286539631,59.8342692639638,60.0901581063494,60.0960298443525,61.3899225368072,61.4730198446886,61.6563816321773,62.4953038456291,62.9543106915874,63.1667120629212,63.5525599455353,64.3861045980349,64.6186896447114,66.4834844607287,66.6365993750461,67.0303838618256],[48.9932333516995,50.5797470624306,51.076467904754,51.2142228505947,51.5072063775898,51.5306095297071,51.5383177351028,51.9017538200815,52.5227367599527,52.5527767097254,52.7958848415484,52.9154267983373,54.2598416410454,54.4174232499105,54.6962913694155,54.8832061841005,54.9363034778058,55.0091797248921,55.3151066344056,56.4601200292702,56.7136675924846,56.7291606313559,56.7997717493027,57.5047653139464,57.6602555804668,57.7349834663678,58.1997756630387,58.4649969541115,59.0791621719697,59.1765485775028,59.4836238257726,59.5638995135634,59.8145158279397,60.4380939939965,60.4851783859105,60.4965822535643,60.7524710959499,60.758342833953,62.0522355264077,62.1353328342891,62.3186946217778,63.1576168352296,63.6166236811878,63.8290250525217,64.2148729351358,65.0484175876354,65.2810026343119,67.1457974503291,67.2989123646466,67.6926968514261],[51.9561938071583,53.5427075178894,54.0394283602128,54.1771833060535,54.4701668330486,54.4935699851659,54.5012781905615,54.8647142755403,55.4856972154115,55.5157371651841,55.7588452970072,55.878387253796,57.2228020965041,57.3803837053693,57.6592518248743,57.8461666395593,57.8992639332646,57.9721401803509,58.2780670898643,59.423080484729,59.6766280479434,59.6921210868147,59.7627322047615,60.4677257694052,60.6232160359256,60.6979439218265,61.1627361184975,61.4279574095703,62.0421226274285,62.1395090329616,62.4465842812314,62.5268599690222,62.7774762833985,63.4010544494553,63.4481388413693,63.4595427090231,63.7154315514087,63.7213032894118,65.0151959818665,65.0982932897479,65.2816550772366,66.1205772906884,66.5795841366466,66.7919855079805,67.1778333905946,68.0113780430942,68.2439630897707,70.1087579057879,70.2618728201054,70.6556573068849],[52.4186878621709,54.0052015729019,54.5019224152253,54.639677361066,54.9326608880611,54.9560640401785,54.9637722455741,55.3272083305529,55.9481912704241,55.9782312201967,56.2213393520197,56.3408813088086,57.6852961515167,57.8428777603818,58.1217458798868,58.3086606945719,58.3617579882771,58.4346342353634,58.7405611448769,59.8855745397415,60.139122102956,60.1546151418273,60.225226259774,60.9302198244177,61.0857100909382,61.1604379768391,61.62523017351,61.8904514645828,62.504616682441,62.6020030879741,62.909078336244,62.9893540240347,63.239970338411,63.8635485044679,63.9106328963818,63.9220367640356,64.1779256064212,64.1837973444243,65.477690036879,65.5607873447604,65.7441491322491,66.5830713457009,67.0420781916591,67.254479562993,67.6403274456071,68.4738720981067,68.7064571447832,70.5712519608005,70.7243668751179,71.1181513618974],[52.8131878065176,54.3997015172486,54.896422359572,55.0341773054127,55.3271608324078,55.3505639845252,55.3582721899208,55.7217082748996,56.3426912147708,56.3727311645434,56.6158392963664,56.7353812531553,58.0797960958634,58.2373777047285,58.5162458242335,58.7031606389186,58.7562579326238,58.8291341797101,59.1350610892236,60.2800744840882,60.5336220473027,60.549115086174,60.6197262041207,61.3247197687644,61.4802100352849,61.5549379211858,62.0197301178567,62.2849514089295,62.8991166267877,62.9965030323208,63.3035782805907,63.3838539683814,63.6344702827577,64.2580484488146,64.3051328407285,64.3165367083823,64.5724255507679,64.578297288771,65.8721899812257,65.9552872891071,66.1386490765958,66.9775712900476,67.4365781360058,67.6489795073397,68.0348273899538,68.8683720424534,69.1009570891299,70.9657519051472,71.1188668194646,71.5126513062441],[54.3773996700658,55.9639133807969,56.4606342231203,56.598389168961,56.891372695956,56.9147758480734,56.922484053469,57.2859201384478,57.906903078319,57.9369430280916,58.1800511599146,58.2995931167035,59.6440079594116,59.8015895682768,60.0804576877817,60.2673725024668,60.320469796172,60.3933460432584,60.6992729527718,61.8442863476365,62.0978339108509,62.1133269497222,62.1839380676689,62.8889316323127,63.0444218988331,63.119149784734,63.583941981405,63.8491632724777,64.463328490336,64.560714895869,64.8677901441389,64.9480658319296,65.198682146306,65.8222603123628,65.8693447042767,65.8807485719305,66.1366374143162,66.1425091523192,67.436401844774,67.5194991526554,67.702860940144,68.5417831535958,69.0007899995541,69.213191370888,69.5990392535021,70.4325839060017,70.6651689526781,72.5299637686954,72.6830786830128,73.0768631697923],[59.9002086090454,61.4867223197764,61.9834431620998,62.1211981079405,62.4141816349356,62.437584787053,62.4452929924486,62.8087290774274,63.4297120172986,63.4597519670712,63.7028600988942,63.8224020556831,65.1668168983912,65.3243985072563,65.6032666267613,65.7901814414464,65.8432787351516,65.9161549822379,66.2220818917514,67.367095286616,67.6206428498305,67.6361358887018,67.7067470066485,68.4117405712922,68.5672308378127,68.6419587237136,69.1067509203845,69.3719722114573,69.9861374293155,70.0835238348486,70.3905990831185,70.4708747709092,70.7214910852855,71.3450692513424,71.3921536432563,71.4035575109101,71.6594463532957,71.6653180912988,72.9592107837535,73.0423080916349,73.2256698791236,74.0645920925754,74.5235989385336,74.7360003098675,75.1218481924816,75.9553928449812,76.1879778916577,78.0527727076749,78.2058876219924,78.5996721087719],[60.5794527387442,62.1659664494753,62.6626872917987,62.8004422376394,63.0934257646344,63.1168289167518,63.1245371221474,63.4879732071262,64.1089561469974,64.13899609677,64.382104228593,64.5016461853819,65.84606102809,66.0036426369552,66.2825107564601,66.4694255711452,66.5225228648505,66.5953991119368,66.9013260214502,68.0463394163149,68.2998869795293,68.3153800184006,68.3859911363474,69.0909847009911,69.2464749675115,69.3212028534124,69.7859950500834,70.0512163411561,70.6653815590144,70.7627679645474,71.0698432128173,71.150118900608,71.4007352149844,72.0243133810412,72.0713977729552,72.0828016406089,72.3386904829946,72.3445622209976,73.6384549134524,73.7215522213338,73.9049140088225,74.7438362222743,75.2028430682325,75.4152444395664,75.8010923221805,76.6346369746801,76.8672220213565,78.7320168373738,78.8851317516912,79.2789162384708],[60.599932063724,62.1864457744551,62.6831666167785,62.8209215626192,63.1139050896142,63.1373082417316,63.1450164471272,63.508452532106,64.1294354719772,64.1594754217498,64.4025835535728,64.5221255103617,65.8665403530698,66.024121961935,66.3029900814399,66.489904896125,66.5430021898302,66.6158784369166,66.92180534643,68.0668187412947,68.3203663045091,68.3358593433804,68.4064704613271,69.1114640259709,69.2669542924913,69.3416821783922,69.8064743750631,70.0716956661359,70.6858608839942,70.7832472895272,71.0903225377971,71.1705982255878,71.4212145399641,72.044792706021,72.0918770979349,72.1032809655887,72.3591698079744,72.3650415459774,73.6589342384322,73.7420315463136,73.9253933338022,74.764315547254,75.2233223932123,75.4357237645462,75.8215716471603,76.6551162996599,76.8877013463363,78.7524961623536,78.905611076671,79.2993955634505],[60.6117290337129,62.198242744444,62.6949635867674,62.8327185326081,63.1257020596031,63.1491052117205,63.1568134171161,63.5202495020949,64.1412324419661,64.1712723917387,64.4143805235617,64.5339224803506,65.8783373230587,66.0359189319239,66.3147870514288,66.5017018661139,66.5547991598191,66.6276754069055,66.9336023164189,68.0786157112836,68.332163274498,68.3476563133693,68.418267431316,69.1232609959598,69.2787512624802,69.3534791483811,69.8182713450521,70.0834926361248,70.6976578539831,70.7950442595161,71.102119507786,71.1823951955767,71.433011509953,72.0565896760099,72.1036740679238,72.1150779355776,72.3709667779633,72.3768385159663,73.6707312084211,73.7538285163025,73.9371903037911,74.7761125172429,75.2351193632012,75.4475207345351,75.8333686171492,76.6669132696488,76.8994983163252,78.7642931323425,78.9174080466599,79.3111925334394],[61.3730299955295,62.9595437062605,63.4562645485839,63.5940194944246,63.8870030214197,63.9104061735371,63.9181143789327,64.2815504639115,64.9025334037827,64.9325733535553,65.1756814853783,65.2952234421672,66.6396382848753,66.7972198937404,67.0760880132454,67.2630028279304,67.3161001216357,67.388976368722,67.6949032782355,68.8399166731001,69.0934642363146,69.1089572751859,69.1795683931326,69.8845619577763,70.0400522242968,70.1147801101977,70.5795723068686,70.8447935979414,71.4589588157996,71.5563452213327,71.8634204696026,71.9436961573933,72.1943124717696,72.8178906378265,72.8649750297404,72.8763788973942,73.1322677397798,73.1381394777829,74.4320321702376,74.515129478119,74.6984912656077,75.5374134790595,75.9964203250177,76.2088216963516,76.5946695789657,77.4282142314653,77.6607992781418,79.525594094159,79.6787090084765,80.072493495256],[64.6859634511171,66.2724771618482,66.7691980041716,66.9069529500123,67.1999364770073,67.2233396291247,67.2310478345203,67.5944839194991,68.2154668593703,68.2455068091429,68.4886149409659,68.6081568977548,69.9525717404629,70.1101533493281,70.389021468833,70.5759362835181,70.6290335772234,70.7019098243097,71.0078367338231,72.1528501286878,72.4063976919022,72.4218907307735,72.4925018487203,73.197495413364,73.3529856798844,73.4277135657853,73.8925057624563,74.157727053529,74.7718922713873,74.8692786769203,75.1763539251902,75.2566296129809,75.5072459273573,76.1308240934141,76.177908485328,76.1893123529818,76.4452011953675,76.4510729333705,77.7449656258253,77.8280629337067,78.0114247211954,78.8503469346471,79.3093537806054,79.5217551519393,79.9076030345534,80.741147687053,80.9737327337294,82.8385275497467,82.9916424640641,83.3854269508437],[68.3834292217502,69.9699429324813,70.4666637748047,70.6044187206454,70.8974022476405,70.9208053997578,70.9285136051534,71.2919496901322,71.9129326300034,71.9429725797761,72.1860807115991,72.305622668388,73.650037511096,73.8076191199612,74.0864872394662,74.2734020541512,74.3264993478565,74.3993755949428,74.7053025044562,75.8503158993209,76.1038634625353,76.1193565014066,76.1899676193534,76.8949611839971,77.0504514505175,77.1251793364185,77.5899715330894,77.8551928241622,78.4693580420204,78.5667444475535,78.8738196958233,78.9540953836141,79.2047116979904,79.8282898640472,79.8753742559612,79.886778123615,80.1426669660006,80.1485387040037,81.4424313964584,81.5255287043398,81.7088904918285,82.5478127052803,83.0068195512385,83.2192209225724,83.6050688051865,84.4386134576861,84.6711985043626,86.5359933203798,86.6891082346973,87.0828927214768],[71.9264845699335,73.5129982806646,74.009719122988,74.1474740688287,74.4404575958237,74.4638607479411,74.4715689533367,74.8350050383155,75.4559879781867,75.4860279279593,75.7291360597823,75.8486780165712,77.1930928592793,77.3506744681445,77.6295425876494,77.8164574023345,77.8695546960397,77.9424309431261,78.2483578526395,79.3933712475042,79.6469188107186,79.6624118495899,79.7330229675366,80.4380165321804,80.5935067987008,80.6682346846017,81.1330268812727,81.3982481723454,82.0124133902037,82.1097997957367,82.4168750440066,82.4971507317973,82.7477670461736,83.3713452122305,83.4184296041444,83.4298334717982,83.6857223141839,83.6915940521869,84.9854867446417,85.0685840525231,85.2519458400117,86.0908680534635,86.5498748994218,86.7622762707557,87.1481241533698,87.9816688058694,88.2142538525458,90.0790486685631,90.2321635828805,90.62594806966],[75.188826965711,76.7753406764421,77.2720615187655,77.4098164646062,77.7027999916012,77.7262031437186,77.7339113491142,78.097347434093,78.7183303739642,78.7483703237368,78.9914784555598,79.1110204123487,80.4554352550568,80.613016863922,80.8918849834269,81.078799798112,81.1318970918172,81.2047733389036,81.510700248417,82.6557136432817,82.9092612064961,82.9247542453674,82.9953653633141,83.7003589279579,83.8558491944783,83.9305770803792,84.3953692770502,84.6605905681229,85.2747557859812,85.3721421915142,85.6792174397841,85.7594931275748,86.0101094419511,86.633687608008,86.6807719999219,86.6921758675757,86.9480647099614,86.9539364479644,88.2478291404192,88.3309264483006,88.5142882357892,89.353210449241,89.8122172951993,90.0246186665332,90.4104665491473,91.2440112016469,91.4765962483233,93.3413910643406,93.494505978658,93.8882904654375]],"type":"surface","x":[-2.96039892327731,-2.73214618289166,-2.54136756927678,-2.33420760703804,-2.29853927796707,-2.1021094254153,-2.09856896020911,-2.0956060206783,-2.04807598083181,-1.96018442660419,-1.78161724252153,-1.52934930101089,-1.49238162732587,-1.48889585298465,-1.03089875877432,-0.987853308859119,-0.853261352273738,-0.798582679493088,-0.77453192186067,-0.748461580289934,-0.49994358955681,-0.465071952665595,-0.454722470925065,-0.452452431926527,-0.423969802044517,-0.0438096739725239,0.17418874404757,0.218257037107956,0.252370993505637,0.285227511214592,0.353507080881254,0.460500326950301,0.483705121872882,0.489524155075263,0.546362685903954,0.786071244541534,0.853118653957231,1.15306569690969,1.19988499344418,1.23982109570761,1.39816972373539,1.95725589276825,2.02601727895909,2.02809044634073,2.02928467971549,2.10635286058609,2.44172844471548,2.8160310859391,3.17470241843918,3.5049565576428],"y":[-2.96739065955215,-2.44805785324636,-2.28546018563748,-2.24036718573331,-2.1444613286036,-2.13680049051357,-2.13427727000019,-2.01530932119005,-1.81203543162327,-1.80220208993125,-1.72262255180219,-1.68349143086038,-1.24340778810319,-1.19182468573189,-1.10053939641347,-1.03935429943388,-1.02197331724591,-0.998117849967946,-0.897975078198987,-0.523163933743912,-0.440167129716625,-0.435095605089509,-0.411981610050177,-0.181207502094158,-0.130308984218711,-0.105847397513396,0.0462986786367592,0.133116785819328,0.334158947616182,0.366037622814148,0.466556294258567,0.492833910352589,0.574871193088133,0.778994609569731,0.794407315568089,0.798140282104294,0.881903485485141,0.88382555281705,1.30737116637587,1.33457241765193,1.39459445907624,1.6692090586459,1.81946134566535,1.88898926684997,2.01529354164428,2.28814783885948,2.3642827276904,2.97470866881919,3.02482963380164,3.15373189365332],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">983</span>)</span>
<span id="cb316-2"><a href="#cb316-2" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.99</span></span>
<span id="cb316-3"><a href="#cb316-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n,<span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span>T))</span>
<span id="cb316-4"><a href="#cb316-4" aria-hidden="true" tabindex="-1"></a>x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb316-5"><a href="#cb316-5" aria-hidden="true" tabindex="-1"></a>x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb316-6"><a href="#cb316-6" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb316-7"><a href="#cb316-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-8"><a href="#cb316-8" aria-hidden="true" tabindex="-1"></a>m3<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2)</span>
<span id="cb316-9"><a href="#cb316-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-10"><a href="#cb316-10" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">sort</span>(x1)</span>
<span id="cb316-11"><a href="#cb316-11" aria-hidden="true" tabindex="-1"></a>yy <span class="ot">&lt;-</span> <span class="fu">sort</span>(x2)</span>
<span id="cb316-12"><a href="#cb316-12" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="fu">predict</span>(m3,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x1=</span>a,<span class="at">x2=</span>b))})</span>
<span id="cb316-13"><a href="#cb316-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-14"><a href="#cb316-14" aria-hidden="true" tabindex="-1"></a>axx<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x1&quot;</span>)</span>
<span id="cb316-15"><a href="#cb316-15" aria-hidden="true" tabindex="-1"></a>axy<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x2&quot;</span>)</span>
<span id="cb316-16"><a href="#cb316-16" aria-hidden="true" tabindex="-1"></a>axz<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb316-17"><a href="#cb316-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb316-18"><a href="#cb316-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb316-19"><a href="#cb316-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x=</span>x1,</span>
<span id="cb316-20"><a href="#cb316-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>x2,</span>
<span id="cb316-21"><a href="#cb316-21" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span>y) <span class="sc">%&gt;%</span></span>
<span id="cb316-22"><a href="#cb316-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x=</span><span class="sc">~</span>xx,</span>
<span id="cb316-23"><a href="#cb316-23" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span><span class="sc">~</span>yy,</span>
<span id="cb316-24"><a href="#cb316-24" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span><span class="sc">~</span>fit,</span>
<span id="cb316-25"><a href="#cb316-25" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale=</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb316-26"><a href="#cb316-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis=</span>axx,<span class="at">yaxis=</span>axy,<span class="at">zaxis=</span>axz))</span></code></pre></div>
<div id="htmlwidget-7e32ef689c1dc8bebebf" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-7e32ef689c1dc8bebebf">{"x":{"visdat":{"3511209c1e34":["function () ","plotlyVisDat"]},"cur_data":"3511209c1e34","attrs":{"3511209c1e34":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[2.05567570010434,1.64546737333353,-0.98585636159149,0.145436500810527,2.23080449186914,1.4158553175941,0.396154307276448,5.33433948212546,-1.48078830684263,0.512188899192018,0.394486532393321,-1.51158866544863,-0.414059299619805,1.40682626830658,-1.59611574077461,1.97492388577371,0.742678031713295,-1.65689485118863,2.75101125208859,2.18990436397892,-1.44485803668215,1.47691249674086,-0.755733252895172,-0.09401621207894,-1.62443935718435,0.371981710378547,1.18312406031599,-0.851693229515672,-0.371847705946829,3.61714700692691,-3.79160974822952,0.163903990695111,0.125891042360704,0.646035706452884,3.42993996456778,1.26496305113097,-0.0687014707382249,-0.149982316825593,-3.21180106444372,0.427033533055787,2.26435337007621,1.02513796314026,3.70878577582133,-1.1194680259074,-1.34982293283585,-2.93442533408609,-1.21162327066926,1.5489509747353,1.2953802240997,-2.23948570656194],"y":[2.01929167320148,2.63832015677903,-1.09614889076506,-0.103072564567768,2.02879998919901,1.94249706331783,0.666055586690686,4.76108097024044,-1.36538851182604,0.0807013958788016,0.599834176427104,-1.64818561174969,-0.128709935204398,1.59950770057417,-1.54655160856301,2.01204727691861,0.833835248073225,-1.22426879368609,2.86413548426478,2.02224967478038,-1.81476622854792,1.60794728094207,-1.29261197705307,0.043340365586029,-1.68805158423357,0.281833453138403,1.2408689176852,-1.1241332016812,-0.521936896100953,3.71411632837187,-3.96625309707395,0.386159158000361,0.23726124785331,0.429586249039983,3.81642321445712,0.943841538120979,-0.35957397272494,-0.317063149401063,-3.25357877831655,0.306301030285228,1.96882310451407,0.71247874224825,3.27890051936193,-1.1779059314858,-1.39072935224815,-3.11276389825864,-1.22602967025068,1.43401509400912,1.19911678208591,-2.24357337219482],"z":[72.4112283037445,80.1357940223725,37.0673223603711,51.705495470334,78.8752505297219,73.3573259158224,53.7042664462778,117.156336201732,25.5404408635633,52.3328220462206,51.9942984233549,33.177183721453,55.0833167832042,71.2226609082324,26.9287442980817,82.2824905062434,63.3613389960669,40.2902821986686,85.230854135455,70.2558702036732,29.5134623752076,67.3618315631795,38.4577206056567,61.4990741471532,26.2605936101416,52.0177298700554,63.8321581818994,35.7541723426673,46.3865511742847,101.331828322834,-11.4063643942466,52.7532010886446,59.7645641018044,52.747803571642,96.6374716908503,76.2990365379506,50.6798105691903,43.7543635134124,11.6061812520325,55.3972407582171,72.2609901493509,58.8890099584912,90.8505437370928,34.0836932623783,30.4735690629398,5.79829772235815,43.5057423094769,67.8456192535648,69.2379631381695,17.3828611502725],"type":"scatter3d","mode":"markers","inherit":true},"3511209c1e34.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[2.05567570010434,1.64546737333353,-0.98585636159149,0.145436500810527,2.23080449186914,1.4158553175941,0.396154307276448,5.33433948212546,-1.48078830684263,0.512188899192018,0.394486532393321,-1.51158866544863,-0.414059299619805,1.40682626830658,-1.59611574077461,1.97492388577371,0.742678031713295,-1.65689485118863,2.75101125208859,2.18990436397892,-1.44485803668215,1.47691249674086,-0.755733252895172,-0.09401621207894,-1.62443935718435,0.371981710378547,1.18312406031599,-0.851693229515672,-0.371847705946829,3.61714700692691,-3.79160974822952,0.163903990695111,0.125891042360704,0.646035706452884,3.42993996456778,1.26496305113097,-0.0687014707382249,-0.149982316825593,-3.21180106444372,0.427033533055787,2.26435337007621,1.02513796314026,3.70878577582133,-1.1194680259074,-1.34982293283585,-2.93442533408609,-1.21162327066926,1.5489509747353,1.2953802240997,-2.23948570656194],"y":[2.01929167320148,2.63832015677903,-1.09614889076506,-0.103072564567768,2.02879998919901,1.94249706331783,0.666055586690686,4.76108097024044,-1.36538851182604,0.0807013958788016,0.599834176427104,-1.64818561174969,-0.128709935204398,1.59950770057417,-1.54655160856301,2.01204727691861,0.833835248073225,-1.22426879368609,2.86413548426478,2.02224967478038,-1.81476622854792,1.60794728094207,-1.29261197705307,0.043340365586029,-1.68805158423357,0.281833453138403,1.2408689176852,-1.1241332016812,-0.521936896100953,3.71411632837187,-3.96625309707395,0.386159158000361,0.23726124785331,0.429586249039983,3.81642321445712,0.943841538120979,-0.35957397272494,-0.317063149401063,-3.25357877831655,0.306301030285228,1.96882310451407,0.71247874224825,3.27890051936193,-1.1779059314858,-1.39072935224815,-3.11276389825864,-1.22602967025068,1.43401509400912,1.19911678208591,-2.24357337219482],"z":[72.4112283037445,80.1357940223725,37.0673223603711,51.705495470334,78.8752505297219,73.3573259158224,53.7042664462778,117.156336201732,25.5404408635633,52.3328220462206,51.9942984233549,33.177183721453,55.0833167832042,71.2226609082324,26.9287442980817,82.2824905062434,63.3613389960669,40.2902821986686,85.230854135455,70.2558702036732,29.5134623752076,67.3618315631795,38.4577206056567,61.4990741471532,26.2605936101416,52.0177298700554,63.8321581818994,35.7541723426673,46.3865511742847,101.331828322834,-11.4063643942466,52.7532010886446,59.7645641018044,52.747803571642,96.6374716908503,76.2990365379506,50.6798105691903,43.7543635134124,11.6061812520325,55.3972407582171,72.2609901493509,58.8890099584912,90.8505437370928,34.0836932623783,30.4735690629398,5.79829772235815,43.5057423094769,67.8456192535648,69.2379631381695,17.3828611502725],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[-1.01501733498186,5.92575118072584,7.29715388224758,15.7622410597593,19.9384138831665,21.1724937294414,21.5607502996326,22.5505686091704,24.0681288498178,24.3149244796506,25.0236985549046,25.6721467543051,25.6892960135079,26.1408260964481,26.6645212264791,26.9370617390643,32.5293392057974,34.1105993088012,34.524614204999,36.3589949656358,36.6086785170938,38.0346008902696,38.3984617113282,39.9232054305956,40.3572962221059,40.595587100066,41.3733291401257,41.7962676104527,43.454320136231,44.0992535473642,44.5513708301302,45.7332675151053,46.8046239178792,49.290761455767,49.6973874567381,51.5784470996558,53.1901873335845,53.2723808014304,56.530576775854,56.786967322095,57.2079295583462,57.2784830725,57.3072911881958,57.3710850066178,63.3072304204227,65.506456454146,69.5458725257649,73.7844596408181,74.7808311883697,83.980897245039],[0.938776621993586,7.87954513770128,9.25094783922303,17.7160350167347,21.892207840142,23.1262876864168,23.5145442566081,24.5043625661459,26.0219228067932,26.2687184366261,26.97749251188,27.6259407112805,27.6430899704833,28.0946200534235,28.6183151834545,28.8908556960398,34.4831331627728,36.0643932657766,36.4784081619744,38.3127889226113,38.5624724740693,39.988394847245,40.3522556683037,41.8769993875711,42.3110901790813,42.5493810570415,43.3271230971012,43.7500615674282,45.4081140932064,46.0530475043396,46.5051647871057,47.6870614720808,48.7584178748547,51.2445554127424,51.6511814137136,53.5322410566312,55.14398129056,55.2261747584058,58.4843707328295,58.7407612790704,59.1617235153217,59.2322770294754,59.2610851451712,59.3248789635932,65.2610243773981,67.4602504111214,71.4996664827404,75.7382535977936,76.7346251453451,85.9346912020145],[1.87345566516669,8.81422418087439,10.1856268823961,18.6507140599078,22.8268868833151,24.0609667295899,24.4492232997812,25.439041609319,26.9566018499663,27.2033974797992,27.9121715550531,28.5606197544536,28.5777690136564,29.0292990965966,29.5529942266276,29.8255347392129,35.4178122059459,36.9990723089497,37.4130872051475,39.2474679657844,39.4971515172424,40.9230738904181,41.2869347114768,42.8116784307442,43.2457692222544,43.4840601002146,44.2618021402743,44.6847406106013,46.3427931363795,46.9877265475127,47.4398438302788,48.6217405152539,49.6930969180278,52.1792344559155,52.5858604568867,54.4669200998043,56.0786603337331,56.1608538015789,59.4190497760026,59.6754403222435,60.0964025584948,60.1669560726486,60.1957641883443,60.2595580067663,66.1957034205712,68.3949294542945,72.4343455259135,76.6729326409667,77.6693041885182,86.8693702451876],[4.21520887219707,11.1559773879048,12.5273800894265,20.9924672669382,25.1686400903455,26.4027199366203,26.7909765068116,27.7807948163493,29.2983550569967,29.5451506868295,30.2539247620835,30.902372961484,30.9195222206868,31.371052303627,31.894747433658,32.1672879462433,37.7595654129763,39.3408255159801,39.7548404121779,41.5892211728148,41.8389047242728,43.2648270974485,43.6286879185072,45.1534316377746,45.5875224292848,45.8258133072449,46.6035553473047,47.0264938176316,48.6845463434099,49.3294797545431,49.7815970373092,50.9634937222843,52.0348501250582,54.5209876629459,54.9276136639171,56.8086733068347,58.4204135407634,58.5026070086093,61.7608029830329,62.0171935292739,62.4381557655252,62.5087092796789,62.5375173953747,62.6013112137967,68.5374566276016,70.7366826613249,74.7760987329439,79.0146858479971,80.0110573955486,89.211123452218],[6.17837797340016,13.1191464891079,14.4905491906296,22.9556363681413,27.1318091915485,28.3658890378234,28.7541456080146,29.7439639175524,31.2615241581998,31.5083197880326,32.2170938632866,32.8655420626871,32.8826913218899,33.3342214048301,33.8579165348611,34.1304570474463,39.7227345141794,41.3039946171832,41.718009513381,43.5523902740179,43.8020738254758,45.2279961986516,45.5918570197103,47.1166007389777,47.5506915304879,47.788982408448,48.5667244485078,48.9896629188347,50.647715444613,51.2926488557462,51.7447661385122,52.9266628234874,53.9980192262613,56.484156764149,56.8907827651202,58.7718424080378,60.3835826419665,60.4657761098124,63.723972084236,63.980362630477,64.4013248667282,64.471878380882,64.5006864965778,64.5644803149998,70.5006257288047,72.699851762528,76.7392678341469,80.9778549492002,81.9742264967517,91.1742925534211],[6.28774395881829,13.228512474526,14.5999151760477,23.0650023535594,27.2411751769667,28.4752550232415,28.8635115934328,29.8533299029706,31.3708901436179,31.6176857734508,32.3264598487047,32.9749080481052,32.992057307308,33.4435873902482,33.9672825202792,34.2398230328645,39.8321004995975,41.4133606026013,41.8273754987991,43.661756259436,43.911439810894,45.3373621840697,45.7012230051284,47.2259667243958,47.660057515906,47.8983483938662,48.6760904339259,49.0990289042529,50.7570814300311,51.4020148411643,51.8541321239304,53.0360288089055,54.1073852116794,56.5935227495671,57.0001487505383,58.8812083934559,60.4929486273847,60.5751420952305,63.8333380696542,64.0897286158951,64.5106908521464,64.5812443663002,64.610052481996,64.6738463004179,70.6099917142228,72.8092177479461,76.8486338195651,81.0872209346183,82.0835924821698,91.2836585388392],[6.38318666626399,13.3239551819717,14.6953578834934,23.1604450610051,27.3366178844124,28.5706977306872,28.9589543008785,29.9487726104163,31.4663328510636,31.7131284808965,32.4219025561504,33.0703507555509,33.0875000147537,33.5390300976939,34.0627252277249,34.3352657403102,39.9275432070432,41.508803310047,41.9228182062448,43.7571989668817,44.0068825183397,45.4328048915154,45.7966657125741,47.3214094318415,47.7555002233517,47.9937911013119,48.7715331413716,49.1944716116986,50.8525241374768,51.49745754861,51.9495748313761,53.1314715163512,54.2028279191251,56.6889654570128,57.095591457984,58.9766511009016,60.5883913348304,60.6705848026762,63.9287807770999,64.1851713233408,64.6061335595921,64.6766870737459,64.7054951894416,64.7692890078636,70.7054344216685,72.9046604553918,76.9440765270108,81.182663642064,82.1790351896155,91.3791012462849],[6.66801939400857,13.6087879097163,14.980190611238,23.4452777887497,27.621450612157,28.8555304584318,29.2437870286231,30.2336053381608,31.7511655788082,31.997961208641,32.706735283895,33.3551834832955,33.3723327424983,33.8238628254385,34.3475579554695,34.6200984680548,40.2123759347878,41.7936360377916,42.2076509339894,44.0420316946263,44.2917152460843,45.71763761926,46.0814984403187,47.6062421595861,48.0403329510963,48.2786238290564,49.0563658691162,49.4793043394431,51.1373568652214,51.7822902763546,52.2344075591207,53.4163042440958,54.4876606468697,56.9737981847574,57.3804241857286,59.2614838286462,60.8732240625749,60.9554175304208,64.2136135048444,64.4700040510854,64.8909662873367,64.9615198014904,64.9903279171862,65.0541217356082,70.9902671494131,73.1894931831364,77.2289092547554,81.4674963698086,82.4638679173601,91.6639339740295],[6.77180803360629,13.712576549314,15.0839792508357,23.5490664283474,27.7252392517547,28.9593190980295,29.3475756682208,30.3373939777586,31.8549542184059,32.1017498482388,32.8105239234927,33.4589721228932,33.476121382096,33.9276514650362,34.4513465950672,34.7238871076525,40.3161645743855,41.8974246773893,42.3114395735871,44.145820334224,44.395503885682,45.8214262588577,46.1852870799164,47.7100307991838,48.144121590694,48.3824124686542,49.1601545087139,49.5830929790409,51.2411455048191,51.8860789159523,52.3381961987184,53.5200928836935,54.5914492864674,57.0775868243551,57.4842128253263,59.3652724682439,60.9770127021727,61.0592061700185,64.3174021444422,64.5737926906831,64.9947549269344,65.0653084410881,65.0941165567839,65.1579103752059,71.0940557890108,73.2932818227341,77.3326978943531,81.5712850094063,82.5676565569578,91.7677226136272],[6.89288304796046,13.8336515636682,15.2050542651899,23.6701414427016,27.8463142661088,29.0803941123837,29.4686506825749,30.4584689921127,31.9760292327601,32.2228248625929,32.9315989378469,33.5800471372474,33.5971963964502,34.0487264793904,34.5724216094214,34.8449621220066,40.4372395887397,42.0184996917435,42.4325145879413,44.2668953485782,44.5165789000361,45.9425012732119,46.3063620942706,47.831105813538,48.2651966050482,48.5034874830083,49.2812295230681,49.704167993395,51.3622205191733,52.0071539303065,52.4592712130725,53.6411678980477,54.7125243008216,57.1986618387093,57.6052878396805,59.4863474825981,61.0980877165268,61.1802811843727,64.4384771587963,64.6948677050373,65.1158299412885,65.1863834554423,65.2151915711381,65.2789853895601,71.215130803365,73.4143568370883,77.4537729087073,81.6923600237605,82.688731571312,91.8887976279814],[7.21312490840151,14.1538934241092,15.5252961256309,23.9903833031426,28.1665561265499,29.4006359728247,29.788892543016,30.7787108525538,32.2962710932012,32.543066723034,33.2518407982879,33.9002889976884,33.9174382568913,34.3689683398314,34.8926634698625,35.1652039824477,40.7574814491808,42.3387415521846,42.7527564483823,44.5871372090192,44.8368207604772,46.262743133653,46.6266039547116,48.151347673979,48.5854384654893,48.8237293434494,49.6014713835091,50.0244098538361,51.6824623796144,52.3273957907475,52.7795130735136,53.9614097584887,55.0327661612626,57.5189036991504,57.9255297001215,59.8065893430392,61.4183295769679,61.5005230448137,64.7587190192374,65.0151095654784,65.4360718017296,65.5066253158834,65.5354334315792,65.5992272500012,71.535372663806,73.7345986975294,77.7740147691483,82.0126018842015,83.0089734317531,92.2090394884224],[7.67881932165884,14.6195878373665,15.9909905388883,24.4560777164,28.6322505398072,29.8663303860821,30.2545869562733,31.2444052658111,32.7619655064585,33.0087611362913,33.7175352115453,34.3659834109458,34.3831326701486,34.8346627530888,35.3583578831198,35.630898395705,41.2231758624381,42.8044359654419,43.2184508616397,45.0528316222765,45.3025151737345,46.7284375469103,47.0922983679689,48.6170420872363,49.0511328787466,49.2894237567067,50.0671657967664,50.4901042670934,52.1481567928717,52.7930902040049,53.2452074867709,54.427104171746,55.4984605745199,57.9845981124077,58.3912241133788,60.2722837562965,61.8840239902252,61.9662174580711,65.2244134324947,65.4808039787357,65.9017662149869,65.9723197291407,66.0011278448365,66.0649216632585,72.0010670770634,74.2002931107867,78.2397091824056,82.4782962974588,83.4746678450104,92.6747339016797],[7.989356858185,14.9301253738927,16.3015280754144,24.7666152529261,28.9427880763334,30.1768679226082,30.5651244927995,31.5549428023373,33.0725030429846,33.3192986728175,34.0280727480714,34.6765209474719,34.6936702066747,35.1452002896149,35.6688954196459,35.9414359322312,41.5337133989643,43.1149735019681,43.5289883981658,45.3633691588027,45.6130527102607,47.0389750834365,47.4028359044951,48.9275796237625,49.3616704152727,49.5999612932329,50.3777033332926,50.8006418036196,52.4586943293979,53.103627740531,53.5557450232971,54.7376417082722,55.8089981110461,58.2951356489339,58.701761649905,60.5828212928227,62.1945615267514,62.2767549945972,65.5349509690209,65.7913415152618,66.2123037515131,66.2828572656669,66.3116653813627,66.3754591997847,72.3116046135895,74.5108306473129,78.5502467189318,82.788833833985,83.7852053815366,92.9852714382059],[8.43959099510298,15.3803595108107,16.7517622123324,25.2168493898441,29.3930222132514,30.6271020595262,31.0153586297175,32.0051769392552,33.5227371799026,33.7695328097355,34.4783068849894,35.1267550843899,35.1439043435927,35.5954344265329,36.1191295565639,36.3916700691492,41.9839475358822,43.565207638886,43.9792225350838,45.8136032957207,46.0632868471787,47.4892092203544,47.8530700414131,49.3778137606805,49.8119045521907,50.0501954301509,50.8279374702106,51.2508759405376,52.9089284663158,53.553861877449,54.0059791602151,55.1878758451902,56.2592322479641,58.7453697858518,59.151995786823,61.0330554297406,62.6447956636694,62.7269891315152,65.9851851059389,66.2415756521798,66.6625378884311,66.7330914025848,66.7618995182806,66.8256933367026,72.7618387505075,74.9610647842308,79.0004808558498,83.239067970903,84.2354395184545,93.4355055751239],[8.89168342494915,15.8324519406568,17.2038546421786,25.6689418196903,29.8451146430975,31.0791944893724,31.4674510595636,32.4572693691014,33.9748296097488,34.2216252395816,34.9303993148356,35.5788475142361,35.5959967734389,36.0475268563791,36.5712219864101,36.8437624989953,42.4360399657284,44.0173000687322,44.43131496493,46.2656957255668,46.5153792770248,47.9413016502006,48.3051624712592,49.8299061905266,50.2639969820369,50.502287859997,51.2800299000567,51.7029683703837,53.361020896162,54.0059543072952,54.4580715900612,55.6399682750364,56.7113246778102,59.197462215698,59.6040882166691,61.4851478595868,63.0968880935155,63.1790815613614,66.437277535785,66.693668082026,67.1146303182772,67.185183832431,67.2139919481268,67.2777857665488,73.2139311803537,75.413157214077,79.4525732856959,83.6911604007491,84.6875319483007,93.88759800497],[9.21504184933228,16.15581036504,17.5272130665617,25.9923002440734,30.1684730674807,31.4025529137555,31.7908094839468,32.7806277934845,34.2981880341319,34.5449836639648,35.2537577392187,35.9022059386192,35.919355197822,36.3708852807622,36.8945804107932,37.1671209233785,42.7593983901115,44.3406584931153,44.7546733893131,46.58905414995,46.838737701408,48.2646600745837,48.6285208956424,50.1532646149098,50.58735540642,50.8256462843802,51.6033883244399,52.0263267947669,53.6843793205451,54.3293127316783,54.7814300144444,55.9633266994195,57.0346831021934,59.5208206400811,59.9274466410523,61.8085062839699,63.4202465178987,63.5024399857445,66.7606359601682,67.0170265064091,67.4379887426604,67.5085422568142,67.5373503725099,67.6011441909319,73.5372896047368,75.7365156384601,79.7759317100791,84.0145188251323,85.0108903726838,94.2109564293532],[10.3663880154617,17.3071565311694,18.6785592326911,27.1436464102028,31.3198192336101,32.5538990798849,32.9421556500762,33.931973959614,35.4495342002613,35.6963298300942,36.4051039053481,37.0535521047486,37.0707013639514,37.5222314468916,38.0459265769226,38.3184670895079,43.9107445562409,45.4920046592447,45.9060195554425,47.7404003160794,47.9900838675374,49.4160062407131,49.7798670617718,51.3046107810392,51.7387015725494,51.9769924505096,52.7547344905693,53.1776729608963,54.8357254866745,55.4806588978077,55.9327761805738,57.1146728655489,58.1860292683228,60.6721668062105,61.0787928071817,62.9598524500993,64.5715926840281,64.6537861518739,67.9119821262976,68.1683726725385,68.5893349087898,68.6598884229436,68.6886965386393,68.7524903570613,74.6886357708662,76.8878618045895,80.9272778762085,85.1658649912617,86.1622365388132,95.3623025954826],[10.5086293425389,17.4493978582466,18.8208005597683,27.28588773728,31.4620605606873,32.6961404069621,33.0843969771534,34.0742152866911,35.5917755273385,35.8385711571713,36.5473452324253,37.1957934318258,37.2129426910286,37.6644727739688,38.1881679039998,38.4607084165851,44.0529858833181,45.6342459863219,46.0482608825197,47.8826416431566,48.1323251946146,49.5582475677903,49.922108388849,51.4468521081164,51.8809428996266,52.1192337775867,52.8969758176465,53.3199142879734,54.9779668137517,55.6229002248849,56.075017507651,57.2569141926261,58.3282705954,60.8144081332877,61.2210341342589,63.1020937771765,64.7138340111052,64.7960274789511,68.0542234533747,68.3106139996157,68.731576235867,68.8021297500207,68.8309378657165,68.8947316841385,74.8308770979434,77.0301031316667,81.0695192032857,85.3081063183389,86.3044778658904,95.5045439225598],[11.2562539791633,18.197022494871,19.5684251963928,28.0335123739045,32.2096851973117,33.4437650435866,33.8320216137778,34.8218399233156,36.339400163963,36.5861957937958,37.2949698690498,37.9434180684503,37.9605673276531,38.4120974105933,38.9357925406243,39.2083330532095,44.8006105199426,46.3818706229464,46.7958855191442,48.630266279781,48.879949831239,50.3058722044148,50.6697330254734,52.1944767447408,52.6285675362511,52.8668584142112,53.6446004542709,54.0675389245979,55.7255914503762,56.3705248615094,56.8226421442754,58.0045388292505,59.0758952320244,61.5620327699122,61.9686587708833,63.849718413801,65.4614586477297,65.5436521155756,68.8018480899992,69.0582386362402,69.4792008724914,69.5497543866452,69.578562502341,69.642356320763,75.5785017345679,77.7777277682912,81.8171438399101,86.0557309549633,87.0521025025149,96.2521685591842],[11.4448441819445,18.3856126976522,19.757015399174,28.2221025766857,32.3982754000929,33.6323552463678,34.020611816559,35.0104301260968,36.5279903667442,36.774785996577,37.483560071831,38.1320082712315,38.1491575304343,38.6006876133745,39.1243827434055,39.3969232559907,44.9892007227238,46.5704608257276,46.9844757219254,48.8188564825622,49.0685400340202,50.494462407196,50.8583232282546,52.383066947522,52.8171577390323,53.0554486169924,53.8331906570521,54.2561291273791,55.9141816531574,56.5591150642906,57.0112323470566,58.1931290320317,59.2644854348056,61.7506229726934,62.1572489736645,64.0383086165822,65.6500488505109,65.7322423183568,68.9904382927804,69.2468288390214,69.6677910752726,69.7383445894264,69.7671527051222,69.8309465235442,75.7670919373491,77.9663179710724,82.0057340426913,86.2443211577445,87.2406927052961,96.4407587619654],[11.5301478175006,18.4709163332083,19.84231903473,28.3074062122417,32.483579035649,33.7176588819238,34.1059154521151,35.0957337616528,36.6132940023002,36.860089632133,37.568863707387,38.2173119067875,38.2344611659903,38.6859912489305,39.2096863789615,39.4822268915468,45.0745043582798,46.6557644612836,47.0697793574814,48.9041601181183,49.1538436695763,50.579766042752,50.9436268638107,52.4683705830781,52.9024613745883,53.1407522525484,53.9184942926082,54.3414327629351,55.9994852887134,56.6444186998466,57.0965359826127,58.2784326675878,59.3497890703617,61.8359266082494,62.2425526092206,64.1236122521382,65.735352486067,65.8175459539128,69.0757419283365,69.3321324745774,69.7530947108286,69.8236482249824,69.8524563406782,69.9162501591002,75.8523955729051,78.0516216066284,82.0910376782474,86.3296247933006,87.3259963408521,96.5260623975215],[12.1858704494607,19.1266389651684,20.4980416666901,28.9631288442018,33.1393016676091,34.3733815138839,34.7616380840752,35.7514563936129,37.2690166342603,37.5158122640932,38.2245863393471,38.8730345387476,38.8901837979504,39.3417138808906,39.8654090109216,40.1379495235069,45.7302269902399,47.3114870932437,47.7255019894415,49.5598827500784,49.8095663015364,51.2354886747121,51.5993494957708,53.1240932150382,53.5581840065484,53.7964748845086,54.5742169245683,54.9971553948953,56.6552079206735,57.3001413318067,57.7522586145728,58.9341552995479,60.0055117023218,62.4916492402095,62.8982752411807,64.7793348840983,66.3910751180271,66.4732685858729,69.7314645602966,69.9878551065375,70.4088173427888,70.4793708569426,70.5081789726383,70.5719727910603,76.5081182048652,78.7073442385885,82.7467603102075,86.9853474252607,87.9817189728122,97.1817850294816],[12.2517332067348,19.1925017224424,20.5639044239642,29.0289916014759,33.2051644248831,34.439244271158,34.8275008413492,35.817319150887,37.3348793915344,37.5816750213672,38.2904490966212,38.9388972960217,38.9560465552245,39.4075766381647,39.9312717681957,40.2038122807809,45.796089747514,47.3773498505178,47.7913647467156,49.6257455073524,49.8754290588104,51.3013514319862,51.6652122530448,53.1899559723122,53.6240467638225,53.8623376417826,54.6400796818424,55.0630181521693,56.7210706779476,57.3660040890808,57.8181213718468,59.000018056822,60.0713744595958,62.5575119974836,62.9641379984548,64.8451976413724,66.4569378753011,66.539131343147,69.7973273175706,70.0537178638116,70.4746801000628,70.5452336142166,70.5740417299124,70.6378355483344,76.5739809621393,78.7732069958626,82.8126230674815,87.0512101825347,88.0475817300863,97.2476477867556],[12.3139635098434,19.2547320255511,20.6261347270728,29.0912219045845,33.2673947279918,34.5014745742666,34.8897311444579,35.8795494539956,37.397109694643,37.6439053244759,38.3526793997298,39.0011275991303,39.0182768583331,39.4698069412733,39.9935020713043,40.2660425838896,45.8583200506226,47.4395801536264,47.8535950498242,49.6879758104611,49.9376593619191,51.3635817350948,51.7274425561535,53.2521862754209,53.6862770669311,53.9245679448913,54.702309984951,55.125248455278,56.7833009810562,57.4282343921894,57.8803516749555,59.0622483599306,60.1336047627045,62.6197423005922,63.0263683015634,64.907427944481,66.5191681784098,66.6013616462556,69.8595576206793,70.1159481669202,70.5369104031715,70.6074639173252,70.636272033021,70.700065851443,76.6362112652479,78.8354372989712,82.8748533705902,87.1134404856434,88.1098120331949,97.3098780898643],[13.0151275363017,19.9558960520094,21.3272987535311,29.7923859310428,33.96855875445,35.2026386007249,35.5908951709161,36.5807134804539,38.0982737211013,38.3450693509341,39.0538434261881,39.7022916255886,39.7194408847914,40.1709709677316,40.6946660977626,40.9672066103478,46.5594840770809,48.1407441800847,48.5547590762825,50.3891398369193,50.6388233883773,52.0647457615531,52.4286065826118,53.9533503018791,54.3874410933894,54.6257319713495,55.4034740114093,55.8264124817362,57.4844650075145,58.1293984186477,58.5815157014137,59.7634123863889,60.8347687891628,63.3209063270505,63.7275323280217,65.6085919709393,67.220332204868,67.3025256727139,70.5607216471375,70.8171121933785,71.2380744296297,71.3086279437835,71.3374360594793,71.4012298779013,77.3373752917062,79.5366013254295,83.5760173970484,87.8146045121017,88.8109760596532,98.0110421163226],[13.0909625254711,20.0317310411788,21.4031337427006,29.8682209202123,34.0443937436195,35.2784735898944,35.6667301600856,36.6565484696234,38.1741087102708,38.4209043401036,39.1296784153576,39.7781266147581,39.7952758739609,40.2468059569011,40.7705010869321,41.0430415995173,46.6353190662504,48.2165791692542,48.630594065452,50.4649748260888,50.7146583775468,52.1405807507226,52.5044415717812,54.0291852910486,54.4632760825589,54.701566960519,55.4793090005787,55.9022474709057,57.560299996684,58.2052334078172,58.6573506905832,59.8392473755584,60.9106037783322,63.39674131622,63.8033673171911,65.6844269601088,67.2961671940375,67.3783606618834,70.636556636307,70.892947182548,71.3139094187992,71.384462932953,71.4132710486488,71.4770648670708,77.4132102808757,79.612436314599,83.6518523862179,87.8904395012711,88.8868110488227,98.086877105492],[13.0965824628391,20.0373509785468,21.4087536800685,29.8738408575802,34.0500136809874,35.2840935272623,35.6723500974535,36.6621684069913,38.1797286476387,38.4265242774715,39.1352983527255,39.783746552126,39.8008958113288,40.252425894269,40.7761210243,41.0486615368852,46.6409390036183,48.2221991066221,48.6362140028199,50.4705947634568,50.7202783149147,52.1462006880905,52.5100615091492,54.0348052284166,54.4688960199268,54.7071868978869,55.4849289379467,55.9078674082736,57.5659199340519,58.2108533451851,58.6629706279511,59.8448673129263,60.9162237157002,63.4023612535879,63.8089872545591,65.6900468974767,67.3017871314054,67.3839805992513,70.6421765736749,70.8985671199159,71.3195293561671,71.3900828703209,71.4188909860167,71.4826848044387,77.4188302182436,79.6180562519669,83.6574723235858,87.8960594386391,88.8924309861906,98.09249704286],[13.2006368628695,20.1414053785772,21.5128080800989,29.9778952576106,34.1540680810179,35.3881479272927,35.776404497484,36.7662228070217,38.2837830476691,38.5305786775019,39.2393527527559,39.8878009521564,39.9049502113592,40.3564802942994,40.8801754243304,41.1527159369157,46.7449934036487,48.3262535066525,48.7402684028503,50.5746491634872,50.8243327149452,52.2502550881209,52.6141159091796,54.138859628447,54.5729504199572,54.8112412979173,55.5889833379771,56.011921808304,57.6699743340823,58.3149077452155,58.7670250279816,59.9489217129567,61.0202781157306,63.5064156536183,63.9130416545895,65.7941012975071,67.4058415314359,67.4880349992817,70.7462309737054,71.0026215199463,71.4235837561975,71.4941372703513,71.5229453860471,71.5867392044691,77.522884618274,79.7221106519973,83.7615267236163,88.0001138386695,88.996485386221,98.1965514428904],[13.4875867558491,20.4283552715568,21.7997579730785,30.2648451505902,34.4410179739975,35.6750978202723,36.0633543904636,37.0531727000014,38.5707329406488,38.8175285704816,39.5263026457355,40.174750845136,40.1919001043388,40.643430187279,41.1671253173101,41.4396658298953,47.0319432966284,48.6132033996322,49.0272182958299,50.8615990564668,51.1112826079248,52.5372049811006,52.9010658021592,54.4258095214266,54.8599003129369,55.098191190897,55.8759332309567,56.2988717012837,57.956924227062,58.6018576381951,59.0539749209612,60.2358716059363,61.3072280087102,63.793365546598,64.1999915475691,66.0810511904867,67.6927914244155,67.7749848922613,71.033180866685,71.289571412926,71.7105336491772,71.781087163331,71.8098952790268,71.8736890974488,77.8098345112536,80.009060544977,84.0484766165959,88.2870637316491,89.2834352792007,98.48350133587],[13.938613259073,20.8793817747807,22.2507844763024,30.7158716538141,34.8920444772213,36.1261243234962,36.5143808936874,37.5041992032252,39.0217594438726,39.2685550737054,39.9773291489594,40.6257773483599,40.6429266075627,41.0944566905029,41.6181518205339,41.8906923331191,47.4829697998522,49.064229902856,49.4782447990538,51.3126255596907,51.5623091111486,52.9882314843244,53.3520923053831,54.8768360246505,55.3109268161607,55.5492176941208,56.3269597341806,56.7498982045075,58.4079507302858,59.052884141419,59.505001424185,60.6868981091602,61.7582545119341,64.2443920498218,64.651018050793,66.5320776937106,68.1438179276393,68.2260113954852,71.4842073699088,71.7405979161498,72.161560152401,72.2321136665548,72.2609217822506,72.3247156006726,78.2608610144775,80.4600870482008,84.4995031198197,88.738090234873,89.7344617824245,98.9345278390939],[14.2642710085378,21.2050395242455,22.5764422257672,31.0415294032789,35.2177022266862,36.451782072961,36.8400386431523,37.8298569526901,39.3474171933374,39.5942128231703,40.3029868984242,40.9514350978247,40.9685843570275,41.4201144399677,41.9438095699987,42.216350082584,47.808627549317,49.3898876523208,49.8039025485186,51.6382833091555,51.8879668606135,53.3138892337892,53.6777500548479,55.2024937741153,55.6365845656255,55.8748754435857,56.6526174836454,57.0755559539724,58.7336084797506,59.3785418908838,59.8306591736499,61.012555858625,62.0839122613989,64.5700497992866,64.9766758002578,66.8577354431754,68.4694756771042,68.55166914495,71.8098651193737,72.0662556656146,72.4872179018659,72.5577714160196,72.5865795317154,72.6503733501374,78.5865187639423,80.7857447976656,84.8251608692846,89.0637479843378,90.0601195318893,99.2601855885587],[15.2160823948495,22.1568509105572,23.5282536120789,31.9933407895906,36.1695136129978,37.4035934592727,37.7918500294639,38.7816683390017,40.2992285796491,40.5460242094819,41.2547982847359,41.9032464841364,41.9203957433392,42.3719258262794,42.8956209563104,43.1681614688956,48.7604389356287,50.3416990386325,50.7557139348303,52.5900946954672,52.8397782469251,54.2657006201009,54.6295614411596,56.154305160427,56.5883959519372,56.8266868298973,57.6044288699571,58.027367340284,59.6854198660623,60.3303532771955,60.7824705599615,61.9643672449367,63.0357236477106,65.5218611855983,65.9284871865695,67.8095468294871,69.4212870634158,69.5034805312617,72.7616765056853,73.0180670519263,73.4390292881775,73.5095828023313,73.5383909180271,73.6021847364491,79.538330150254,81.7375561839773,85.7769722555962,90.0155593706495,91.011930918201,100.21199697487],[15.7484515893442,22.6892201050519,24.0606228065736,32.5257099840853,36.7018828074925,37.9359626537674,38.3242192239586,39.3140375334964,40.8315977741438,41.0783934039766,41.7871674792306,42.4356156786311,42.4527649378339,42.9042950207741,43.4279901508051,43.7005306633903,49.2928081301234,50.8740682331272,51.288083129325,53.1224638899618,53.3721474414198,54.7980698145956,55.1619306356543,56.6866743549216,57.1207651464319,57.359056024392,58.1367980644518,58.5597365347787,60.217789060557,60.8627224716902,61.3148397544562,62.4967364394314,63.5680928422053,66.054230380093,66.4608563810642,68.3419160239818,69.9536562579105,70.0358497257564,73.29404570018,73.550436246421,73.9713984826722,74.041951996826,74.0707601125218,74.1345539309438,80.0706993447487,82.269925378472,86.3093414500909,90.5479285651441,91.5443001126957,100.744366169365],[16.0242262201331,22.9649947358408,24.3363974373626,32.8014846148743,36.9776574382815,38.2117372845564,38.5999938547476,39.5898121642854,41.1073724049328,41.3541680347656,42.0629421100196,42.7113903094201,42.7285395686229,43.1800696515631,43.7037647815941,43.9763052941793,49.5685827609124,51.1498428639162,51.563857760114,53.3982385207508,53.6479220722088,55.0738444453846,55.4377052664432,56.9624489857106,57.3965397772209,57.634830655181,58.4125726952407,58.8355111655677,60.493563691346,61.1384971024792,61.5906143852452,62.7725110702203,63.8438674729942,66.330005010882,66.7366310118531,68.6176906547708,70.2294308886995,70.3116243565454,73.569820330969,73.82621087721,74.2471731134612,74.317726627615,74.3465347433108,74.4103285617328,80.3464739755377,82.545700009261,86.5851160808799,90.8237031959331,91.8200747434847,101.020140800154],[16.1267236307385,23.0674921464462,24.438894847968,32.9039820254797,37.0801548488869,38.3142346951618,38.702491265353,39.6923095748908,41.2098698155382,41.456665445371,42.165439520625,42.8138877200255,42.8310369792283,43.2825670621685,43.8062621921995,44.0788027047847,49.6710801715178,51.2523402745216,51.6663551707194,53.5007359313562,53.7504194828142,55.17634185599,55.5402026770486,57.064946396316,57.4990371878263,57.7373280657864,58.5150701058461,58.9380085761731,60.5960611019514,61.2409945130846,61.6931117958506,62.8750084808257,63.9463648835996,66.4325024214874,66.8391284224585,68.7201880653762,70.3319282993049,70.4141217671508,73.6723177415744,73.9287082878154,74.3496705240666,74.4202240382204,74.4490321539162,74.5128259723382,80.4489713861431,82.6481974198664,86.6876134914853,90.9262006065385,91.9225721540901,101.122638210759],[16.5022657945408,23.4430343102484,24.8144370117702,33.2795241892819,37.4556970126891,38.689776858964,39.0780334291552,40.067851738693,41.5854119793404,41.8322076091732,42.5409816844272,43.1894298838277,43.2065791430305,43.6581092259707,44.1818043560017,44.4543448685869,50.04662233532,51.6278824383238,52.0418973345216,53.8762780951584,54.1259616466164,55.5518840197922,55.9157448408508,57.4404885601182,57.8745793516285,58.1128702295886,58.8906122696484,59.3135507399753,60.9716032657536,61.6165366768868,62.0686539596528,63.250550644628,64.3219070474018,66.8080445852896,67.2146705862608,69.0957302291784,70.7074704631071,70.789663930953,74.0478599053766,74.3042504516176,74.7252126878688,74.7957662020226,74.8245743177184,74.8883681361404,80.8245135499453,83.0237395836686,87.0631556552875,91.3017427703407,92.2981143178923,101.498180374562],[16.5326911786864,23.4734596943941,24.8448623959159,33.3099495734275,37.4861223968348,38.7202022431096,39.1084588133009,40.0982771228387,41.6158373634861,41.8626329933189,42.5714070685728,43.2198552679733,43.2370045271762,43.6885346101164,44.2122297401474,44.4847702527326,50.0770477194657,51.6583078224695,52.0723227186673,53.9067034793041,54.1563870307621,55.5823094039379,55.9461702249965,57.4709139442639,57.9050047357742,58.1432956137343,58.921037653794,59.343976124121,61.0020286498993,61.6469620610324,62.0990793437985,63.2809760287736,64.3523324315475,66.8384699694353,67.2450959704064,69.1261556133241,70.7378958472528,70.8200893150987,74.0782852895223,74.3346758357633,74.7556380720145,74.8261915861683,74.8549997018641,74.9187935202861,80.8549389340909,83.0541649678143,87.0935810394332,91.3321681544864,92.328539702038,101.528605758707],[16.7384368858186,23.6792054015263,25.0506081030481,33.5156952805598,37.691868103967,38.9259479502419,39.3142045204331,40.3040228299709,41.8215830706183,42.0683787004511,42.7771527757051,43.4256009751056,43.4427502343084,43.8942803172486,44.4179754472796,44.6905159598648,50.2827934265979,51.8640535296017,52.2780684257995,54.1124491864363,54.3621327378943,55.7880551110701,56.1519159321287,57.6766596513961,58.1107504429064,58.3490413208665,59.1267833609262,59.5497218312532,61.2077743570315,61.8527077681647,62.3048250509307,63.4867217359059,64.5580781386797,67.0442156765675,67.4508416775386,69.3319013204563,70.943641554385,71.0258350222309,74.2840309966545,74.5404215428955,74.9613837791467,75.0319372933005,75.0607454089963,75.1245392274183,81.0606846412232,83.2599106749465,87.2993267465654,91.5379138616186,92.5342854091702,101.73435146584],[16.9811865149829,23.9219550306906,25.2933577322124,33.7584449097241,37.9346177331313,39.1686975794061,39.5569541495974,40.5467724591352,42.0643326997826,42.3111283296154,43.0199024048693,43.6683506042698,43.6854998634727,44.1370299464129,44.6607250764439,44.9332655890291,50.5255430557622,52.106803158766,52.5208180549638,54.3551988156006,54.6048823670586,56.0308047402344,56.394665561293,57.9194092805604,58.3535000720707,58.5917909500308,59.3695329900905,59.7924714604175,61.4505239861958,62.095457397329,62.547574680095,63.7294713650701,64.800827767844,67.2869653057318,67.6935913067029,69.5746509496206,71.1863911835493,71.2685846513952,74.5267806258188,74.7831711720598,75.204133408311,75.2746869224648,75.3034950381606,75.3672888565826,81.3034342703875,83.5026603041108,87.5420763757297,91.7806634907829,92.7770350383345,101.977101095004],[17.30641992663,24.2471884423377,25.6185911438595,34.0836783213712,38.2598511447784,39.4939309910532,39.8821875612445,40.8720058707823,42.3895661114297,42.6363617412625,43.3451358165164,43.9935840159169,44.0107332751198,44.46226335806,44.985958488091,45.2584990006762,50.8507764674093,52.4320365704131,52.8460514666109,54.6804322272477,54.9301157787057,56.3560381518815,56.7198989729401,58.2446426922075,58.6787334837178,58.9170243616779,59.6947664017376,60.1177048720646,61.7757573978429,62.4206908089761,62.8728080917421,64.0547047767172,65.1260611794911,67.6121987173789,68.01882471835,69.8998843612677,71.5116245951964,71.5938180630423,74.8520140374659,75.1084045837069,75.5293668199581,75.5999203341119,75.6287284498077,75.6925222682297,81.6286676820345,83.8278937157579,87.8673097873768,92.10589690243,93.1022684499816,102.302334506651],[18.416596716134,25.3573652318417,26.7287679333634,35.1938551108751,39.3700279342823,40.6041077805572,40.9923643507484,41.9821826602862,43.4997429009336,43.7465385307664,44.4553126060204,45.1037608054209,45.1209100646237,45.5724401475639,46.0961352775949,46.3686757901801,51.9609532569132,53.542213359917,53.9562282561148,55.7906090167516,56.0402925682096,57.4662149413854,57.8300757624441,59.3548194817114,59.7889102732217,60.0272011511818,60.8049431912416,61.2278816615685,62.8859341873468,63.53086759848,63.982984881246,65.1648815662212,66.236237968995,68.7223755068828,69.129001507854,71.0100611507716,72.6218013847003,72.7039948525462,75.9621908269698,76.2185813732108,76.639543609462,76.7100971236158,76.7389052393116,76.8026990577336,82.7388444715385,84.9380705052618,88.9774865768807,93.2160736919339,94.2124452394855,103.412511296155],[18.6887078646318,25.6294763803395,27.0008790818613,35.465966259373,39.6421390827802,40.8762189290551,41.2644754992463,42.2542938087841,43.7718540494315,44.0186496792643,44.7274237545183,45.3758719539188,45.3930212131216,45.8445512960618,46.3682464260928,46.640786938678,52.2330644054111,53.8143245084149,54.2283394046127,56.0627201652495,56.3124037167075,57.7383260898833,58.1021869109419,59.6269306302093,60.0610214217196,60.2993122996797,61.0770543397394,61.4999928100664,63.1580453358447,63.8029787469779,64.2550960297439,65.436992714719,66.5083491174929,68.9944866553807,69.4011126563518,71.2821722992695,72.8939125331982,72.9761060010441,76.2343019754677,76.4906925217087,76.9116547579599,76.9822082721137,77.0110163878095,77.0748102062315,83.0109556200364,85.2101816537597,89.2495977253786,93.4881848404318,94.4845563879834,103.684622444653],[19.1410211184084,26.0817896341161,27.4531923356378,35.9182795131495,40.0944523365567,41.3285321828316,41.7167887530228,42.7066070625606,44.224167303208,44.4709629330408,45.1797370082948,45.8281852076953,45.8453344668981,46.2968645498383,46.8205596798693,47.0931001924545,52.6853776591876,54.2666377621914,54.6806526583892,56.515033419026,56.764716970484,58.1906393436598,58.5545001647185,60.0792438839858,60.5133346754961,60.7516255534562,61.529367593516,61.9523060638429,63.6103585896212,64.2552920007544,64.7074092835204,65.8893059684956,66.9606623712694,69.4467999091572,69.8534259101284,71.734485553046,73.3462257869747,73.4284192548206,76.6866152292442,76.9430057754852,77.3639680117364,77.4345215258902,77.463329641586,77.527123460008,83.4632688738129,85.6624949075362,89.7019109791551,93.9404980942084,94.9368696417599,104.136935698429],[19.2788431706578,26.2196116863655,27.5910143878873,36.0561015653989,40.2322743888062,41.466354235081,41.8546108052723,42.8444291148101,44.3619893554575,44.6087849852903,45.3175590605442,45.9660072599447,45.9831565191476,46.4346866020878,46.9583817321188,47.230922244704,52.8231997114371,54.4044598144409,54.8184747106387,56.6528554712755,56.9025390227335,58.3284613959093,58.6923222169679,60.2170659362353,60.6511567277456,60.8894476057057,61.6671896457654,62.0901281160924,63.7481806418707,64.3931140530039,64.8452313357699,66.027128020745,67.0984844235189,69.5846219614067,69.9912479623778,71.8723076052955,73.4840478392242,73.5662413070701,76.8244372814937,77.0808278277347,77.5017900639859,77.5723435781397,77.6011516938355,77.6649455122575,83.6010909260623,85.8003169597857,89.8397330314046,94.0783201464578,95.0746916940094,104.274757750679],[19.3918935566512,26.3326620723589,27.7040647738806,36.1691519513923,40.3453247747996,41.5794046210744,41.9676611912657,42.9574795008035,44.4750397414508,44.7218353712837,45.4306094465376,46.0790576459381,46.0962069051409,46.5477369880811,47.0714321181121,47.3439726306974,52.9362500974304,54.5175102004342,54.931525096632,56.7659058572689,57.0155894087269,58.4415117819026,58.8053726029613,60.3301163222287,60.7642071137389,61.0024979916991,61.7802400317588,62.2031785020858,63.861231027864,64.5061644389972,64.9582817217633,66.1401784067384,67.2115348095123,69.6976723474,70.1042983483712,71.9853579912888,73.5970982252176,73.6792916930634,76.9374876674871,77.193878213728,77.6148404499793,77.6853939641331,77.7142020798289,77.7779958982509,83.7141413120557,85.9133673457791,89.952783417398,94.1913705324512,95.1877420800027,104.387808136672],[21.0317952269434,27.9725637426511,29.3439664441728,37.8090536216845,41.9852264450918,43.2193062913666,43.6075628615579,44.5973811710956,46.114941411743,46.3617370415758,47.0705111168298,47.7189593162303,47.7361085754331,48.1876386583733,48.7113337884043,48.9838743009896,54.5761517677226,56.1574118707264,56.5714267669242,58.4058075275611,58.6554910790191,60.0814134521948,60.4452742732535,61.9700179925209,62.4041087840311,62.6423996619912,63.420141702051,63.8430801723779,65.5011326981562,66.1460661092894,66.5981833920554,67.7800800770306,68.8514364798045,71.3375740176922,71.7442000186634,73.625259661581,75.2369998955098,75.3191933633556,78.5773893377793,78.8337798840202,79.2547421202714,79.3252956344252,79.354103750121,79.417897568543,85.3540429823479,87.5532690160712,91.5926850876902,95.8312722027434,96.8276437502949,106.027709806964],[23.319596104636,30.2603646203437,31.6317673218654,40.0968544993771,44.2730273227844,45.5071071690592,45.8953637392505,46.8851820487883,48.4027422894356,48.6495379192685,49.3583119945224,50.0067601939229,50.0239094531257,50.4754395360659,50.9991346660969,51.2716751786822,56.8639526454152,58.445212748419,58.8592276446168,60.6936084052537,60.9432919567117,62.3692143298874,62.7330751509461,64.2578188702135,64.6919096617237,64.9302005396839,65.7079425797436,66.1308810500706,67.7889335758488,68.433866986982,68.8859842697481,70.0678809547232,71.1392373574971,73.6253748953849,74.032000896356,75.9130605392736,77.5248007732024,77.6069942410482,80.8651902154719,81.1215807617128,81.5425429979641,81.6130965121179,81.6419046278136,81.7056984462356,87.6418438600405,89.8410698937638,93.8804859653828,98.119073080436,99.1154446279875,108.315510684657],[23.9504317548996,30.8912002706073,32.2626029721291,40.7276901496408,44.903862973048,46.1379428193229,46.5261993895141,47.5160176990519,49.0335779396993,49.2803735695321,49.9891476447861,50.6375958441866,50.6547451033894,51.1062751863296,51.6299703163606,51.9025108289458,57.4947882956789,59.0760483986827,59.4900632948805,61.3244440555173,61.5741276069753,63.0000499801511,63.3639108012097,64.8886545204771,65.3227453119874,65.5610361899475,66.3387782300072,66.7617167003342,68.4197692261125,69.0647026372457,69.5168199200117,70.6987166049868,71.7700730077607,74.2562105456485,74.6628365466196,76.5438961895373,78.155636423466,78.2378298913119,81.4960258657355,81.7524164119765,82.1733786482277,82.2439321623815,82.2727402780773,82.3365340964993,88.2726795103042,90.4719055440275,94.5113216156464,98.7499087306996,99.7462802782512,108.946346334921],[24.2592289114402,31.1999974271479,32.5714001286696,41.0364873061813,45.2126601295886,46.4467399758634,46.8349965460547,47.8248148555925,49.3423750962399,49.5891707260727,50.2979448013266,50.9463930007271,50.96354225993,51.4150723428701,51.9387674729012,52.2113079854864,57.8035854522195,59.3848455552233,59.798860451421,61.6332412120579,61.8829247635159,63.3088471366917,63.6727079577503,65.1974516770177,65.631542468528,65.8698333464881,66.6475753865478,67.0705138568748,68.7285663826531,69.3734997937862,69.8256170765523,71.0075137615274,72.0788701643013,74.5650077021891,74.9716337031602,76.8526933460779,78.4644335800066,78.5466270478524,81.8048230222761,82.0612135685171,82.4821758047683,82.5527293189221,82.5815374346179,82.6453312530399,88.5814766668447,90.7807027005681,94.820118772187,99.0587058872402,100.055077434792,109.255143491461],[29.7368926597281,36.6776611754358,38.0490638769576,46.5141510544693,50.6903238778765,51.9244037241514,52.3126602943426,53.3024786038804,54.8200388445278,55.0668344743606,55.7756085496146,56.4240567490151,56.4412060082179,56.8927360911581,57.4164312211891,57.6889717337743,63.2812492005074,64.8625093035112,65.276524199709,67.1109049603458,67.3605885118038,68.7865108849796,69.1503717060382,70.6751154253056,71.1092062168159,71.347497094776,72.1252391348357,72.5481776051627,74.206230130941,74.8511635420742,75.3032808248402,76.4851775098153,77.5565339125892,80.042671450477,80.4492974514481,82.3303570943658,83.9420973282945,84.0242907961404,87.282486770564,87.538877316805,87.9598395530562,88.03039306721,88.0592011829058,88.1229950013278,94.0591404151327,96.258366448856,100.297782520475,104.536369635528,105.53274118308,114.732807239749]],"type":"surface","x":[-3.79160974822952,-3.21180106444372,-2.93442533408609,-2.23948570656194,-1.65689485118863,-1.62443935718435,-1.59611574077461,-1.51158866544863,-1.48078830684263,-1.44485803668215,-1.34982293283585,-1.21162327066926,-1.1194680259074,-0.98585636159149,-0.851693229515672,-0.755733252895172,-0.414059299619805,-0.371847705946829,-0.149982316825593,-0.09401621207894,-0.0687014707382249,0.125891042360704,0.145436500810527,0.163903990695111,0.371981710378547,0.394486532393321,0.396154307276448,0.427033533055787,0.512188899192018,0.646035706452884,0.742678031713295,1.02513796314026,1.18312406031599,1.26496305113097,1.2953802240997,1.40682626830658,1.4158553175941,1.47691249674086,1.5489509747353,1.64546737333353,1.97492388577371,2.05567570010434,2.18990436397892,2.23080449186914,2.26435337007621,2.75101125208859,3.42993996456778,3.61714700692691,3.70878577582133,5.33433948212546],"y":[-3.96625309707395,-3.25357877831655,-3.11276389825864,-2.24357337219482,-1.81476622854792,-1.68805158423357,-1.64818561174969,-1.54655160856301,-1.39072935224815,-1.36538851182604,-1.29261197705307,-1.22602967025068,-1.22426879368609,-1.1779059314858,-1.1241332016812,-1.09614889076506,-0.521936896100953,-0.35957397272494,-0.317063149401063,-0.128709935204398,-0.103072564567768,0.043340365586029,0.0807013958788016,0.23726124785331,0.281833453138403,0.306301030285228,0.386159158000361,0.429586249039983,0.599834176427104,0.666055586690686,0.71247874224825,0.833835248073225,0.943841538120979,1.19911678208591,1.2408689176852,1.43401509400912,1.59950770057417,1.60794728094207,1.94249706331783,1.96882310451407,2.01204727691861,2.01929167320148,2.02224967478038,2.02879998919901,2.63832015677903,2.86413548426478,3.27890051936193,3.71411632837187,3.81642321445712,4.76108097024044],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="#cb317-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7272</span>)</span>
<span id="cb317-2"><a href="#cb317-2" aria-hidden="true" tabindex="-1"></a>rho<span class="ot">&lt;-</span><span class="fl">0.99</span></span>
<span id="cb317-3"><a href="#cb317-3" aria-hidden="true" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="dv">2</span><span class="sc">*</span><span class="fu">mvrnorm</span>(<span class="at">n=</span>n,<span class="at">mu=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="at">Sigma=</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,rho,rho,<span class="dv">1</span>),<span class="at">ncol=</span><span class="dv">2</span>,<span class="at">byrow=</span>T))</span>
<span id="cb317-4"><a href="#cb317-4" aria-hidden="true" tabindex="-1"></a>x1<span class="ot">&lt;-</span>x[,<span class="dv">1</span>]</span>
<span id="cb317-5"><a href="#cb317-5" aria-hidden="true" tabindex="-1"></a>x2<span class="ot">&lt;-</span>x[,<span class="dv">2</span>]</span>
<span id="cb317-6"><a href="#cb317-6" aria-hidden="true" tabindex="-1"></a>y<span class="ot">&lt;-</span>beta0<span class="sc">+</span>beta1<span class="sc">*</span>x1<span class="sc">+</span>beta2<span class="sc">*</span>x2<span class="sc">+</span><span class="fu">rnorm</span>(n,<span class="at">sd=</span>sigma) </span>
<span id="cb317-7"><a href="#cb317-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-8"><a href="#cb317-8" aria-hidden="true" tabindex="-1"></a>m4<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x1<span class="sc">+</span>x2)</span>
<span id="cb317-9"><a href="#cb317-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-10"><a href="#cb317-10" aria-hidden="true" tabindex="-1"></a>xx <span class="ot">&lt;-</span> <span class="fu">sort</span>(x1)</span>
<span id="cb317-11"><a href="#cb317-11" aria-hidden="true" tabindex="-1"></a>yy <span class="ot">&lt;-</span> <span class="fu">sort</span>(x2)</span>
<span id="cb317-12"><a href="#cb317-12" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">outer</span>(xx, yy, <span class="cf">function</span>(a, b){<span class="fu">predict</span>(m4,<span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">x1=</span>a,<span class="at">x2=</span>b))})</span>
<span id="cb317-13"><a href="#cb317-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-14"><a href="#cb317-14" aria-hidden="true" tabindex="-1"></a>axx<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x1&quot;</span>)</span>
<span id="cb317-15"><a href="#cb317-15" aria-hidden="true" tabindex="-1"></a>axy<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;x2&quot;</span>)</span>
<span id="cb317-16"><a href="#cb317-16" aria-hidden="true" tabindex="-1"></a>axz<span class="ot">&lt;-</span><span class="fu">list</span>(<span class="at">title=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb317-17"><a href="#cb317-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-18"><a href="#cb317-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_ly</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb317-19"><a href="#cb317-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_markers</span>(<span class="at">x=</span>x1,</span>
<span id="cb317-20"><a href="#cb317-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span>x2,</span>
<span id="cb317-21"><a href="#cb317-21" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span>y) <span class="sc">%&gt;%</span></span>
<span id="cb317-22"><a href="#cb317-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_surface</span>(<span class="at">x=</span><span class="sc">~</span>xx,</span>
<span id="cb317-23"><a href="#cb317-23" aria-hidden="true" tabindex="-1"></a>              <span class="at">y=</span><span class="sc">~</span>yy,</span>
<span id="cb317-24"><a href="#cb317-24" aria-hidden="true" tabindex="-1"></a>              <span class="at">z=</span><span class="sc">~</span>fit,</span>
<span id="cb317-25"><a href="#cb317-25" aria-hidden="true" tabindex="-1"></a>              <span class="at">showscale=</span><span class="cn">FALSE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb317-26"><a href="#cb317-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layout</span>(<span class="at">scene =</span> <span class="fu">list</span>(<span class="at">xaxis=</span>axx,<span class="at">yaxis=</span>axy,<span class="at">zaxis=</span>axz))</span></code></pre></div>
<div id="htmlwidget-c74e6ccada59d58cf83c" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-c74e6ccada59d58cf83c">{"x":{"visdat":{"351115924ec1":["function () ","plotlyVisDat"]},"cur_data":"351115924ec1","attrs":{"351115924ec1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"x":[0.670447232350085,-1.52081986853802,0.551097448896429,2.75809179715889,3.29818281382937,0.180002104549399,-1.99567685626452,-0.585334790655924,0.0917772684483983,0.427746220865478,-0.48893767800831,2.25426561957652,-2.42707453768341,0.101677188693447,3.59313410685851,-2.05796397926283,4.75179660687796,2.16836518421425,-0.548086147958929,-0.592730451548333,2.81320184249814,0.281096441858762,-1.62586985030409,-0.591212166502939,-2.06039261721247,2.56729633087264,2.39687738613548,2.36337331778596,1.9883323826121,2.09722080475462,-1.74115622451856,-4.07597421713483,-3.264857723815,-0.349608871758296,0.845580921775133,-4.44585033319053,-2.11519570831633,-0.543066183934002,3.38067849605031,0.891601535342018,-1.45176684910048,2.41968648637948,-1.00606930056544,-4.22639470950259,1.51694312999049,-1.24040682971251,5.29724181435705,-2.48735664250955,-0.0589047620367544,-0.434049739259119],"y":[0.523420770970757,-1.14148814085572,0.636497404945093,2.8456096015782,2.90455764172021,-0.221381784385977,-1.96519755158619,-0.432422158485776,-0.0444837009711016,0.746512772728603,-0.581798781345011,1.9193599893433,-2.29885890070173,-0.287019362858232,3.46908876110364,-2.60529114605309,4.97966142633536,1.9191957639727,-0.0849868851701209,-0.308006124432938,2.75045974920866,-0.117183796875793,-1.10319298762528,-0.67325988137785,-2.01388979806851,2.71241988398414,2.68480722207595,2.43649694013844,1.85462155057681,1.44377479614985,-1.55068272497971,-4.07102184072386,-2.85151353513735,-0.051043524092399,0.867949000287586,-4.41912104116896,-2.4528864773632,-0.364096589148385,3.05516993496982,0.886023056899303,-1.59716252921171,2.30660907946819,-1.09351877394224,-3.76140022222062,0.979389194830804,-1.21252199204031,5.40128873293331,-2.62781024418413,-0.0586197593960398,-0.761237398137361],"z":[50.6409862363115,37.6188942379965,55.1156164424235,77.5102510295675,87.1233239485785,50.3561833320536,25.1372620699713,39.0191394242595,46.4224241922122,52.2997015553769,46.5263752457263,80.2300454004135,27.087399361868,52.2015264072309,102.070846785859,22.3725435957777,112.989353473564,82.2552873486129,44.3145697493114,45.8100286592283,86.2179419362683,47.5966772030684,28.4384247702086,46.0745083433043,19.5298434735908,82.5305414889222,92.2471871987802,78.2822766455684,70.0870401735438,65.9996396891847,36.4889292390901,-6.50693325250774,11.7862377584625,46.5978064369615,61.6455456355576,0.313379426531433,20.3124310783652,44.8697061109606,90.2140176457216,62.1155582100481,22.950649980682,91.1833364918415,39.2156225795799,-3.08027946925855,66.2656019434205,31.2092572462291,119.677556198975,11.4840757625055,55.3765479957603,36.1724188004595],"type":"scatter3d","mode":"markers","inherit":true},"351115924ec1.1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"showscale":false,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"x1"},"yaxis":{"title":"x2"},"zaxis":{"title":"y"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[0.670447232350085,-1.52081986853802,0.551097448896429,2.75809179715889,3.29818281382937,0.180002104549399,-1.99567685626452,-0.585334790655924,0.0917772684483983,0.427746220865478,-0.48893767800831,2.25426561957652,-2.42707453768341,0.101677188693447,3.59313410685851,-2.05796397926283,4.75179660687796,2.16836518421425,-0.548086147958929,-0.592730451548333,2.81320184249814,0.281096441858762,-1.62586985030409,-0.591212166502939,-2.06039261721247,2.56729633087264,2.39687738613548,2.36337331778596,1.9883323826121,2.09722080475462,-1.74115622451856,-4.07597421713483,-3.264857723815,-0.349608871758296,0.845580921775133,-4.44585033319053,-2.11519570831633,-0.543066183934002,3.38067849605031,0.891601535342018,-1.45176684910048,2.41968648637948,-1.00606930056544,-4.22639470950259,1.51694312999049,-1.24040682971251,5.29724181435705,-2.48735664250955,-0.0589047620367544,-0.434049739259119],"y":[0.523420770970757,-1.14148814085572,0.636497404945093,2.8456096015782,2.90455764172021,-0.221381784385977,-1.96519755158619,-0.432422158485776,-0.0444837009711016,0.746512772728603,-0.581798781345011,1.9193599893433,-2.29885890070173,-0.287019362858232,3.46908876110364,-2.60529114605309,4.97966142633536,1.9191957639727,-0.0849868851701209,-0.308006124432938,2.75045974920866,-0.117183796875793,-1.10319298762528,-0.67325988137785,-2.01388979806851,2.71241988398414,2.68480722207595,2.43649694013844,1.85462155057681,1.44377479614985,-1.55068272497971,-4.07102184072386,-2.85151353513735,-0.051043524092399,0.867949000287586,-4.41912104116896,-2.4528864773632,-0.364096589148385,3.05516993496982,0.886023056899303,-1.59716252921171,2.30660907946819,-1.09351877394224,-3.76140022222062,0.979389194830804,-1.21252199204031,5.40128873293331,-2.62781024418413,-0.0586197593960398,-0.761237398137361],"z":[50.6409862363115,37.6188942379965,55.1156164424235,77.5102510295675,87.1233239485785,50.3561833320536,25.1372620699713,39.0191394242595,46.4224241922122,52.2997015553769,46.5263752457263,80.2300454004135,27.087399361868,52.2015264072309,102.070846785859,22.3725435957777,112.989353473564,82.2552873486129,44.3145697493114,45.8100286592283,86.2179419362683,47.5966772030684,28.4384247702086,46.0745083433043,19.5298434735908,82.5305414889222,92.2471871987802,78.2822766455684,70.0870401735438,65.9996396891847,36.4889292390901,-6.50693325250774,11.7862377584625,46.5978064369615,61.6455456355576,0.313379426531433,20.3124310783652,44.8697061109606,90.2140176457216,62.1155582100481,22.950649980682,91.1833364918415,39.2156225795799,-3.08027946925855,66.2656019434205,31.2092572462291,119.677556198975,11.4840757625055,55.3765479957603,36.1724188004595],"type":"scatter3d","mode":"markers","marker":{"color":"rgba(31,119,180,1)","line":{"color":"rgba(31,119,180,1)"}},"error_y":{"color":"rgba(31,119,180,1)"},"error_x":{"color":"rgba(31,119,180,1)"},"line":{"color":"rgba(31,119,180,1)"},"frame":null},{"colorbar":{"title":"fit","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[-7.30511331684118,-4.52306614368943,-2.04853604594088,5.22337863370135,7.01123998881321,7.19121510285338,8.40925001118324,9.64025537887877,11.9177598528984,12.3069136530431,15.2482901121345,15.6197618299866,18.3223796755377,18.8900900426787,19.1961491277151,19.2734665096423,21.9290958850242,22.6322218947615,23.3631890859368,24.5570235182742,25.103088998419,25.5513701740231,25.7190986854298,26.2436814463433,27.0764432459399,27.3337645126376,27.544493487683,27.6050435951888,27.6574704245169,32.196225551693,33.0999464723259,33.9792014161867,34.9497331300392,35.0941829823794,35.8403754463905,39.5517963509777,42.8353288908129,43.3514131208451,43.3527256281058,46.4476630341958,47.4857411313082,49.4702642702452,49.6909477073419,49.9949665025005,50.7554146088794,51.2265338422193,52.4302438884969,55.7383287639791,67.8109919811732,71.1806838820583],[-6.21841594876296,-3.4363687756112,-0.961838677862657,6.31007600177958,8.09793735689144,8.27791247093161,9.49594737926147,10.726952746957,13.0044572209766,13.3936110211213,16.3349874802127,16.7064591980648,19.4090770436159,19.9767874107569,20.2828464957933,20.3601638777206,23.0157932531024,23.7189192628397,24.449886454015,25.6437208863524,26.1897863664972,26.6380675421013,26.805796053508,27.3303788144215,28.1631406140182,28.4204618807158,28.6311908557612,28.691740963267,28.7441677925951,33.2829229197712,34.1866438404041,35.0658987842649,36.0364304981175,36.1808803504576,36.9270728144687,40.638493719056,43.9220262588912,44.4381104889233,44.439422996184,47.534360402274,48.5724384993864,50.5569616383234,50.7776450754202,51.0816638705787,51.8421119769576,52.3132312102975,53.5169412565751,56.8250261320573,68.8976893492514,72.2673812501365],[-5.47356580359873,-2.69151863044697,-0.216988532698425,7.05492614694381,8.84278750205567,9.02276261609584,10.2407975244257,11.4718028921212,13.7493073661408,14.1384611662855,17.079837625377,17.4513093432291,20.1539271887801,20.7216375559211,21.0276966409576,21.1050140228848,23.7606433982667,24.463769408004,25.1947365991792,26.3885710315167,26.9346365116615,27.3829176872656,27.5506461986723,28.0752289595858,28.9079907591824,29.1653120258801,29.3760410009255,29.4365911084312,29.4890179377593,34.0277730649354,34.9314939855683,35.8107489294292,36.7812806432817,36.9257304956218,37.671922959633,41.3833438642202,44.6668764040554,45.1829606340876,45.1842731413483,48.2792105474383,49.3172886445507,51.3018117834877,51.5224952205844,51.8265140157429,52.5869621221219,53.0580813554617,54.2617914017393,57.5698762772216,69.6425394944156,73.0122313953007],[-1.45709020054521,1.32495697260655,3.79948707035509,11.0714017499973,12.8592631051092,13.0392382191494,14.2572731274792,15.4882784951747,17.7657829691944,18.1549367693391,21.0963132284305,21.4677849462826,24.1704027918337,24.7381131589746,25.0441722440111,25.1214896259383,27.7771190013202,28.4802450110575,29.2112122022328,30.4050466345702,30.951112114715,31.3993932903191,31.5671218017258,32.0917045626393,32.9244663622359,33.1817876289336,33.392516603979,33.4530667114848,33.5054935408129,38.0442486679889,38.9479695886218,39.8272245324827,40.7977562463352,40.9422060986754,41.6883985626865,45.3998194672737,48.6833520071089,49.1994362371411,49.2007487444018,52.2956861504918,53.3337642476042,55.3182873865412,55.5389708236379,55.8429896187964,56.6034377251754,57.0745569585152,58.2782670047928,61.5863518802751,73.6590150974692,77.0287069983543],[2.39292906318556,5.17497623633732,7.64950633408587,14.9214210137281,16.70928236884,16.8892574828801,18.10729239121,19.3382977589055,21.6158022329251,22.0049560330698,24.9463324921613,25.3178042100134,28.0204220555644,28.5881324227054,28.8941915077419,28.9715088896691,31.627138265051,32.3302642747883,33.0612314659635,34.255065898301,34.8011313784458,35.2494125540499,35.4171410654566,35.94172382637,36.7744856259667,37.0318068926644,37.2425358677098,37.3030859752155,37.3555128045436,41.8942679317197,42.7979888523526,43.6772437962135,44.647775510066,44.7922253624061,45.5384178264173,49.2498387310045,52.5333712708397,53.0494555008719,53.0507680081326,56.1457054142226,57.1837835113349,59.1683066502719,59.3889900873687,59.6930088825272,60.4534569889061,60.924576222246,62.1282862685236,65.4363711440058,77.5090343611999,80.878726262085],[2.6914331687351,5.47348034188685,7.9480104396354,15.2199251192776,17.0077864743895,17.1877615884297,18.4057964967595,19.6368018644551,21.9143063384747,22.3034601386194,25.2448365977108,25.6163083155629,28.318926161114,28.8866365282549,29.1926956132914,29.2700129952186,31.9256423706005,32.6287683803378,33.3597355715131,34.5535700038505,35.0996354839953,35.5479166595994,35.7156451710061,36.2402279319196,37.0729897315162,37.3303109982139,37.5410399732593,37.6015900807651,37.6540169100932,42.1927720372692,43.0964929579021,43.975747901763,44.9462796156155,45.0907294679557,45.8369219319668,49.548342836554,52.8318753763892,53.3479596064214,53.3492721136821,56.4442095197721,57.4822876168845,59.4668107558215,59.6874941929182,59.9915129880767,60.7519610944557,61.2230803277956,62.4267903740731,65.7348752495554,77.8075384667495,81.1772303676346],[4.23579050775893,7.01783768091069,9.49236777865923,16.7642824583015,18.5521438134133,18.7321189274535,19.9501538357834,21.1811592034789,23.4586636774985,23.8478174776432,26.7891939367346,27.1606656545867,29.8632835001378,30.4309938672788,30.7370529523152,30.8143703342425,33.4699997096243,34.1731257193616,34.9040929105369,36.0979273428743,36.6439928230191,37.0922739986232,37.2600025100299,37.7845852709434,38.61734707054,38.8746683372377,39.0853973122831,39.1459474197889,39.198374249117,43.7371293762931,44.640850296926,45.5201052407868,46.4906369546394,46.6350868069795,47.3812792709906,51.0927001755779,54.376232715413,54.8923169454452,54.8936294527059,57.9885668587959,59.0266449559083,61.0111680948453,61.2318515319421,61.5358703271006,62.2963184334795,62.7674376668194,63.971147713097,67.2792325885792,79.3518958057733,82.7215877066584],[4.50716370779048,7.28921088094224,9.76374097869079,17.035655658333,18.8235170134449,19.003492127485,20.2215270358149,21.4525324035104,23.73003687753,24.1191906776748,27.0605671367662,27.4320388546183,30.1346567001694,30.7023670673103,31.0084261523468,31.085743534274,33.7413729096559,34.4444989193932,35.1754661105684,36.3693005429059,36.9153660230507,37.3636471986548,37.5313757100615,38.055958470975,38.8887202705716,39.1460415372693,39.3567705123147,39.4173206198205,39.4697474491485,44.0085025763246,44.9122234969575,45.7914784408184,46.7620101546709,46.906460007011,47.6526524710222,51.3640733756094,54.6476059154446,55.1636901454768,55.1650026527375,58.2599400588275,59.2980181559399,61.2825412948769,61.5032247319736,61.8072435271321,62.5676916335111,63.0388108668509,64.2425209131285,67.5506057886108,79.6232690058049,82.99296090669],[4.51918980410798,7.30123697725974,9.77576707500828,17.0476817546505,18.8355431097624,19.0155182238025,20.2335531321324,21.4645584998279,23.7420629738475,24.1312167739923,27.0725932330837,27.4440649509358,30.1466827964869,30.7143931636278,31.0204522486643,31.0977696305915,33.7533990059734,34.4565250157107,35.1874922068859,36.3813266392234,36.9273921193682,37.3756732949723,37.543401806379,38.0679845672925,38.9007463668891,39.1580676335868,39.3687966086322,39.4293467161379,39.481773545466,44.0205286726421,44.924249593275,45.8035045371359,46.7740362509884,46.9184861033285,47.6646785673397,51.3760994719269,54.6596320117621,55.1757162417943,55.177028749055,58.271966155145,59.3100442522574,61.2945673911944,61.5152508282911,61.8192696234496,62.5797177298286,63.0508369631684,64.254547009446,67.5626318849283,79.6352951021224,83.0049870030075],[4.82762233133124,7.609669504483,10.0841996022315,17.3561142818738,19.1439756369856,19.3239507510258,20.5419856593557,21.7729910270512,24.0504955010708,24.4396493012155,27.3810257603069,27.752497478159,30.4551153237101,31.0228256908511,31.3288847758875,31.4062021578148,34.0618315331966,34.7649575429339,35.4959247341092,36.6897591664466,37.2358246465914,37.6841058221955,37.8518343336022,38.3764170945157,39.2091788941124,39.46650016081,39.6772291358554,39.7377792433612,39.7902060726893,44.3289611998654,45.2326821204983,46.1119370643591,47.0824687782117,47.2269186305518,47.9731110945629,51.6845319991502,54.9680645389854,55.4841487690175,55.4854612762782,58.5803986823682,59.6184767794806,61.6029999184176,61.8236833555144,62.1277021506729,62.8881502570518,63.3592694903917,64.5629795366693,67.8710644121515,79.9437276293456,83.3134195302307],[6.08795412867557,8.87000130182733,11.3445313995759,18.6164460792181,20.40430743433,20.5842825483701,21.8023174567,23.0333228243955,25.3108272984151,25.6999810985598,28.6413575576513,29.0128292755034,31.7154471210544,32.2831574881954,32.5892165732319,32.6665339551591,35.322163330541,36.0252893402783,36.7562565314535,37.950090963791,38.4961564439358,38.9444376195399,39.1121661309466,39.63674889186,40.4695106914567,40.7268319581544,40.9375609331998,40.9981110407055,41.0505378700336,45.5892929972097,46.4930139178426,47.3722688617035,48.342800575556,48.4872504278961,49.2334428919073,52.9448637964945,56.2283963363297,56.7444805663619,56.7457930736226,59.8407304797126,60.8788085768249,62.863331715762,63.0840151528587,63.3880339480172,64.1484820543962,64.619601287736,65.8233113340136,69.1313962094958,81.2040594266899,84.5737513275751],[6.65882762614538,9.44087479929714,11.9154048970457,19.1873195766879,20.9751809317998,21.1551560458399,22.3731909541698,23.6041963218653,25.8817007958849,26.2708545960297,29.2122310551211,29.5837027729732,32.2863206185243,32.8540309856652,33.1600900707017,33.2374074526289,35.8930368280108,36.5961628377481,37.3271300289233,38.5209644612608,39.0670299414056,39.5153111170097,39.6830396284164,40.2076223893299,41.0403841889265,41.2977054556242,41.5084344306696,41.5689845381753,41.6214113675034,46.1601664946795,47.0638874153124,47.9431423591733,48.9136740730258,49.0581239253659,49.8043163893771,53.5157372939643,56.7992698337995,57.3153540638317,57.3166665710924,60.4116039771824,61.4496820742948,63.4342052132318,63.6548886503285,63.958907445487,64.719355551866,65.1904747852058,66.3941848314834,69.7022697069657,81.7749329241598,85.1446248250449],[7.17901269492346,9.96105986807522,12.4355899658238,19.707504645466,21.4953660005779,21.675341114618,22.8933760229479,24.1243813906434,26.401885864663,26.7910396648077,29.7324161238992,30.1038878417513,32.8065056873023,33.3742160544433,33.6802751394798,33.757592521407,36.4132218967888,37.1163479065262,37.8473150977014,39.0411495300389,39.5872150101836,40.0354961857878,40.2032246971945,40.7278074581079,41.5605692577046,41.8178905244023,42.0286194994477,42.0891696069534,42.1415964362815,46.6803515634576,47.5840724840905,48.4633274279514,49.4338591418039,49.578308994144,50.3245014581551,54.0359223627424,57.3194549025776,57.8355391326098,57.8368516398705,60.9317890459604,61.9698671430728,63.9543902820098,64.1750737191066,64.4790925142651,65.239540620644,65.7106598539839,66.9143699002615,70.2224547757437,82.2951179929378,85.6648098938229],[7.52094849597334,10.3029956691251,12.7775257668736,20.0494404465159,21.8373018016277,22.0172769156679,23.2353118239978,24.4663171916933,26.7438216657129,27.1329754658576,30.074351924949,30.4458236428011,33.1484414883522,33.7161518554932,34.0222109405296,34.0995283224569,36.7551576978387,37.458283707576,38.1892508987513,39.3830853310887,39.9291508112335,40.3774319868376,40.5451604982443,41.0697432591578,41.9025050587544,42.1598263254521,42.3705553004975,42.4311054080033,42.4835322373314,47.0222873645075,47.9260082851404,48.8052632290012,49.7757949428538,49.9202447951939,50.666437259205,54.3778581637923,57.6613907036274,58.1774749336596,58.1787874409203,61.2737248470103,62.3118029441227,64.2963260830597,64.5170095201565,64.821028315315,65.5814764216939,66.0525956550338,67.2563057013114,70.5643905767936,82.6370537939877,86.0067456948728],[8.5675581609527,11.3496053341045,13.824135431853,21.0960501114952,22.8839114666071,23.0638865806473,24.2819214889771,25.5129268566727,27.7904313306923,28.179585130837,31.1209615899284,31.4924333077805,34.1950511533316,34.7627615204725,35.068820605509,35.1461379874362,37.8017673628181,38.5048933725554,39.2358605637307,40.4296949960681,40.9757604762129,41.424041651817,41.5917701632237,42.1163529241372,42.9491147237338,43.2064359904315,43.4171649654769,43.4777150729827,43.5301419023108,48.0688970294868,48.9726179501197,49.8518728939806,50.8224046078331,50.9668544601733,51.7130469241844,55.4244678287716,58.7080003686068,59.224084598639,59.2253971058997,62.3203345119897,63.3584126091021,65.3429357480391,65.5636191851358,65.8676379802943,66.6280860866733,67.0992053200132,68.3029153662907,71.611000241773,83.6836634589671,87.0533553598522],[9.72794754577061,12.5099947189224,14.9845248166709,22.2564394963131,24.044300851425,24.2242759654652,25.442310873795,26.6733162414906,28.9508207155102,29.3399745156549,32.2813509747463,32.6528226925984,35.3554405381495,35.9231509052904,36.2292099903269,36.3065273722541,38.962156747636,39.6652827573733,40.3962499485486,41.590084380886,42.1361498610308,42.5844310366349,42.7521595480416,43.2767423089551,44.1095041085517,44.3668253752494,44.5775543502948,44.6381044578006,44.6905312871287,49.2292864143048,50.1330073349376,51.0122622787985,51.982793992651,52.1272438449912,52.8734363090023,56.5848572135895,59.8683897534247,60.3844739834569,60.3857864907176,63.4807238968076,64.51880199392,66.503325132857,66.7240085699537,67.0280273651123,67.7884754714912,68.2595947048311,69.4633047511087,72.7713896265909,84.844052843785,88.2137447446701],[11.7747132280668,14.5567604012185,17.0312904989671,24.3032051786093,26.0910665337212,26.2710416477613,27.4890765560912,28.7200819237867,30.9975863978063,31.386740197951,34.3281166570425,34.6995883748946,37.4022062204456,37.9699165875866,38.275975672623,38.3532930545503,41.0089224299321,41.7120484396694,42.4430156308447,43.6368500631822,44.1829155433269,44.6311967189311,44.7989252303378,45.3235079912512,46.1562697908479,46.4135910575455,46.6243200325909,46.6848701400967,46.7372969694248,51.2760520966009,52.1797730172338,53.0590279610946,54.0295596749472,54.1740095272873,54.9202019912984,58.6316228958857,61.9151554357209,62.4312396657531,62.4325521730137,65.5274895791037,66.5655676762161,68.5500908151531,68.7707742522499,69.0747930474084,69.8352411537873,70.3063603871272,71.5100704334048,74.818155308887,86.8908185260811,90.2605104269662],[11.7822314512733,14.5642786244251,17.0388087221736,24.3107234018159,26.0985847569277,26.2785598709679,27.4965947792977,28.7276001469933,31.0051046210129,31.3942584211576,34.335634880249,34.7071065981011,37.4097244436522,37.9774348107932,38.2834938958296,38.3608112777569,41.0164406531387,41.719566662876,42.4505338540513,43.6443682863887,44.1904337665335,44.6387149421376,44.8064434535443,45.3310262144578,46.1637880140544,46.4211092807521,46.6318382557975,46.6923883633033,46.7448151926314,51.2835703198075,52.1872912404404,53.0665461843012,54.0370778981538,54.1815277504939,54.927720214505,58.6391411190923,61.9226736589274,62.4387578889596,62.4400703962203,65.5350078023103,66.5730858994227,68.5576090383597,68.7782924754565,69.082311270615,69.8427593769939,70.3138786103338,71.5175886566114,74.8256735320936,86.8983367492877,90.2680286501728],[11.8113349609343,14.5933821340861,17.0679122318346,24.3398269114768,26.1276882665887,26.3076633806289,27.5256982889587,28.7567036566543,31.0342081306739,31.4233619308186,34.36473838991,34.7362101077621,37.4388279533132,38.0065383204541,38.3125974054906,38.3899147874178,41.0455441627997,41.748670172537,42.4796373637123,43.6734717960497,44.2195372761945,44.6678184517986,44.8355469632053,45.3601297241188,46.1928915237154,46.4502127904131,46.6609417654585,46.7214918729643,46.7739187022924,51.3126738294684,52.2163947501013,53.0956496939622,54.0661814078147,54.2106312601549,54.956823724166,58.6682446287532,61.9517771685884,62.4678613986206,62.4691739058813,65.5641113119713,66.6021894090837,68.5867125480207,68.8073959851174,69.111414780276,69.8718628866549,70.3429821199947,71.5466921662724,74.8547770417546,86.9274402589487,90.2971321598338],[11.9957822824614,14.7778294556132,17.2523595533617,24.524274233004,26.3121355881158,26.492110702156,27.7101456104859,28.9411509781814,31.218655452201,31.6078092523457,34.5491857114371,34.9206574292892,37.6232752748403,38.1909856419813,38.4970447270177,38.574362108945,41.2299914843268,41.9331174940641,42.6640846852394,43.8579191175768,44.4039845977216,44.8522657733257,45.0199942847324,45.5445770456459,46.3773388452426,46.6346601119402,46.8453890869856,46.9059391944914,46.9583660238195,51.4971211509956,52.4008420716285,53.2800970154893,54.2506287293419,54.395078581682,55.1412710456931,58.8526919502804,62.1362244901156,62.6523087201477,62.6536212274084,65.7485586334984,66.7866367306108,68.7711598695478,68.9918433066446,69.2958621018031,70.056310208182,70.5274294415219,71.7311394877995,75.0392243632817,87.1118875804758,90.4815794813609],[12.020640071941,14.8026872450927,17.2772173428413,24.5491320224835,26.3369933775954,26.5169684916355,27.7350033999654,28.9660087676609,31.2435132416805,31.6326670418253,34.5740435009167,34.9455152187688,37.6481330643199,38.2158434314608,38.5219025164973,38.5992198984245,41.2548492738064,41.9579752835437,42.6889424747189,43.8827769070564,44.4288423872012,44.8771235628053,45.044852074212,45.5694348351255,46.4021966347221,46.6595179014198,46.8702468764652,46.930796983971,46.983223813299,51.5219789404751,52.425699861108,53.3049548049689,54.2754865188214,54.4199363711615,55.1661288351727,58.8775497397599,62.1610822795951,62.6771665096273,62.678479016888,65.773416422978,66.8114945200904,68.7960176590274,69.0167010961241,69.3207198912826,70.0811679976616,70.5522872310014,71.755997277279,75.0640821527613,87.1367453699553,90.5064372708405],[12.2886728702761,15.0707200434279,17.5452501411764,24.8171648208186,26.6050261759305,26.7850012899707,28.0030361983005,29.2340415659961,31.5115460400157,31.9006998401604,34.8420762992518,35.2135480171039,37.916165862655,38.4838762297959,38.7899353148324,38.8672526967596,41.5228820721415,42.2260080818788,42.9569752730541,44.1508097053915,44.6968751855363,45.1451563611404,45.3128848725471,45.8374676334606,46.6702294330572,46.9275506997549,47.1382796748003,47.1988297823061,47.2512566116342,51.7900117388102,52.6937326594431,53.572987603304,54.5435193171565,54.6879691694967,55.4341616335078,59.145582538095,62.4291150779302,62.9451993079624,62.9465118152231,66.0414492213131,67.0795273184255,69.0640504573625,69.2847338944592,69.5887526896177,70.3492007959967,70.8203200293365,72.0240300756141,75.3321149510964,87.4047781682905,90.7744700691756],[12.5604662177221,15.3425133908738,17.8170434886224,25.0889581682646,26.8768195233765,27.0567946374166,28.2748295457465,29.505834913442,31.7833393874616,32.1724931876063,35.1138696466978,35.4853413645499,38.1879592101009,38.7556695772419,39.0617286622783,39.1390460442056,41.7946754195874,42.4978014293247,43.2287686205,44.4226030528375,44.9686685329822,45.4169497085864,45.5846782199931,46.1092609809065,46.9420227805032,47.1993440472008,47.4100730222462,47.470623129752,47.5230499590801,52.0618050862562,52.9655260068891,53.8447809507499,54.8153126646025,54.9597625169426,55.7059549809537,59.417375885541,62.7009084253762,63.2169926554084,63.218305162669,66.3132425687591,67.3513206658714,69.3358438048084,69.5565272419052,69.8605460370637,70.6209941434426,71.0921133767825,72.2958234230601,75.6039082985423,87.6765715157364,91.0462634166215],[12.9785993552072,15.7606465283589,18.2351766261075,25.5070913057497,27.2949526608616,27.4749277749017,28.6929626832316,29.9239680509271,32.2014725249467,32.5906263250914,35.5320027841829,35.903474502035,38.606092347586,39.173802714727,39.4798617997634,39.5571791816907,42.2128085570725,42.9159345668098,43.6469017579851,44.8407361903226,45.3868016704673,45.8350828460715,46.0028113574782,46.5273941183916,47.3601559179883,47.6174771846859,47.8282061597313,47.8887562672371,47.9411830965652,52.4799382237413,53.3836591443742,54.262914088235,55.2334458020876,55.3778956544277,56.1240881184388,59.8355090230261,63.1190415628613,63.6351257928935,63.6364383001541,66.7313757062442,67.7694538033565,69.7539769422935,69.9746603793903,70.2786791745488,71.0391272809277,71.5102465142676,72.7139565605452,76.0220414360274,88.0947046532215,91.4643965541066],[14.4181040059202,17.200151179072,19.6746812768205,26.9465959564627,28.7344573115746,28.9144324256148,30.1324673339446,31.3634727016402,33.6409771756598,34.0301309758045,36.9715074348959,37.342979152748,40.0455969982991,40.6133073654401,40.9193664504765,40.9966838324037,43.6523132077856,44.3554392175229,45.0864064086982,46.2802408410356,46.8263063211804,47.2745874967845,47.4423160081912,47.9668987691047,48.7996605687013,49.056981835399,49.2677108104444,49.3282609179502,49.3806877472783,53.9194428744544,54.8231637950873,55.7024187389481,56.6729504528006,56.8174003051408,57.5635927691519,61.2750136737391,64.5585462135743,65.0746304436065,65.0759429508672,68.1708803569572,69.2089584540696,71.1934815930066,71.4141650301033,71.7181838252619,72.4786319316408,72.9497511649807,74.1534612112583,77.4615460867405,89.5342093039346,92.9039012048197],[15.1642492319717,17.9462964051234,20.420826502872,27.6927411825142,29.4806025376261,29.6605776516662,30.8786125599961,32.1096179276916,34.3871224017112,34.7762762018559,37.7176526609474,38.0891243787995,40.7917422243505,41.3594525914915,41.665511676528,41.7428290584552,44.398458433837,45.1015844435744,45.8325516347496,47.0263860670871,47.5724515472319,48.020732722836,48.1884612342427,48.7130439951561,49.5458057947528,49.8031270614505,50.0138560364959,50.0744061440016,50.1268329733297,54.6655881005058,55.5693090211387,56.4485639649996,57.4190956788521,57.5635455311922,58.3097379952033,62.0211588997906,65.3046914396258,65.820775669658,65.8220881769187,68.9170255830087,69.955103680121,71.939626819058,72.1603102561548,72.4643290513133,73.2247771576922,73.6958963910321,74.8996064373097,78.2076913127919,90.280354529986,93.6500464308711],[15.21327152219,17.9953186953418,20.4698487930903,27.7417634727325,29.5296248278444,29.7095999418846,30.9276348502144,32.15864021791,34.4361446919296,34.8252984920743,37.7666749511657,38.1381466690178,40.8407645145689,41.4084748817099,41.7145339667463,41.7918513486735,44.4474807240554,45.1506067337927,45.881573924968,47.0754083573054,47.6214738374502,48.0697550130543,48.237483524461,48.7620662853745,49.5948280849711,49.8521493516688,50.0628783267142,50.12342843422,50.1758552635481,54.7146103907242,55.6183313113571,56.4975862552179,57.4681179690704,57.6125678214106,58.3587602854217,62.0701811900089,65.3537137298441,65.8697979598763,65.871110467137,68.966047873227,70.0041259703394,71.9886491092764,72.2093325463731,72.5133513415317,73.2737994479106,73.7449186812505,74.9486287275281,78.2567136030103,90.3293768202044,93.6990687210895],[15.6011197736281,18.3831669467798,20.8576970445284,28.1296117241706,29.9174730792824,30.0974481933226,31.3154831016525,32.546488469348,34.8239929433676,35.2131467435123,38.1545232026037,38.5259949204559,41.2286127660069,41.7963231331479,42.1023822181843,42.1796996001116,44.8353289754934,45.5384549852307,46.269422176406,47.4632566087434,48.0093220888882,48.4576032644923,48.625331775899,49.1499145368125,49.9826763364092,50.2399976031068,50.4507265781522,50.511276685658,50.5637035149861,55.1024586421622,56.0061795627951,56.8854345066559,57.8559662205085,58.0004160728486,58.7466085368597,62.458029441447,65.7415619812822,66.2576462113144,66.258958718575,69.353896124665,70.3919742217774,72.3764973607144,72.5971807978112,72.9011995929697,73.6616476993486,74.1327669326885,75.3364769789661,78.6445618544483,90.7172250716424,94.0869169725275],[16.101717336067,18.8837645092187,21.3582946069673,28.6302092866095,30.4180706417214,30.5980457557615,31.8160806640914,33.0470860317869,35.3245905058065,35.7137443059512,38.6551207650427,39.0265924828948,41.7292103284459,42.2969206955868,42.6029797806233,42.6802971625505,45.3359265379324,46.0390525476697,46.7700197388449,47.9638541711824,48.5099196513272,48.9582008269313,49.125929338338,49.6505120992515,50.4832738988481,50.7405951655458,50.9513241405912,51.0118742480969,51.064301077425,55.6030562046011,56.506777125234,57.3860320690949,58.3565637829474,58.5010136352875,59.2472060992987,62.9586270038859,66.2421595437211,66.7582437737533,66.759556281014,69.854493687104,70.8925717842164,72.8770949231534,73.0977783602501,73.4017971554086,74.1622452617876,74.6333644951274,75.837074541405,79.1451594168873,91.2178226340813,94.5875145349665],[16.8278957141725,19.6099428873243,22.0844729850728,29.3563876647151,31.1442490198269,31.3242241338671,32.542259042197,33.7732644098925,36.0507688839121,36.4399226840568,39.3812991431482,39.7527708610003,42.4553887065514,43.0230990736924,43.3291581587288,43.4064755406561,46.0621049160379,46.7652309257752,47.4961981169505,48.6900325492879,49.2360980294327,49.6843792050368,49.8521077164435,50.376690477357,51.2094522769537,51.4667735436513,51.6775025186967,51.7380526262025,51.7904794555306,56.3292345827067,57.2329555033396,58.1122104472004,59.082742161053,59.2271920133931,59.9733844774042,63.6848053819915,66.9683379218267,67.4844221518589,67.4857346591195,70.5806720652095,71.6187501623219,73.6032733012589,73.8239567383557,74.1279755335142,74.8884236398931,75.359542873233,76.5632529195106,79.8713377949928,91.9440010121869,95.313692913072],[17.4387046449159,20.2207518180677,22.6952819158162,29.9671965954585,31.7550579505703,31.9350330646105,33.1530679729404,34.3840733406359,36.6615778146555,37.0507316148002,39.9921080738916,40.3635797917437,43.0661976372948,43.6339080044358,43.9399670894722,44.0172844713995,46.6729138467813,47.3760398565186,48.1070070476939,49.3008414800313,49.8469069601761,50.2951881357802,50.4629166471869,50.9874994081004,51.820261207697,52.0775824743947,52.2883114494401,52.3488615569459,52.401288386274,56.9400435134501,57.843764434083,58.7230193779438,59.6935510917964,59.8380009441365,60.5841934081476,64.2956143127349,67.57914685257,68.0952310826022,68.0965435898629,71.1914809959529,72.2295590930653,74.2140822320023,74.4347656690991,74.7387844642576,75.4992325706365,75.9703518039764,77.174061850254,80.4821467257362,92.5548099429303,95.9245018438154],[18.0296992769069,20.8117464500586,23.2862765478072,30.5581912274494,32.3460525825613,32.5260276966014,33.7440626049313,34.9750679726268,37.2525724466464,37.6417262467911,40.5831027058826,40.9545744237347,43.6571922692857,44.2249026364267,44.5309617214632,44.6082791033904,47.2639084787722,47.9670344885095,48.6980016796848,49.8918361120223,50.437901592167,50.8861827677712,51.0539112791779,51.5784940400913,52.411255839688,52.6685771063856,52.879306081431,52.9398561889368,52.9922830182649,57.531038145441,58.4347590660739,59.3140140099348,60.2845457237873,60.4289955761274,61.1751880401385,64.8866089447258,68.170141484561,68.6862257145932,68.6875382218539,71.7824756279439,72.8205537250562,74.8050768639932,75.02576030109,75.3297790962485,76.0902272026274,76.5613464359673,77.7650564822449,81.0731413577271,93.1458045749212,96.5154964758063],[18.8969238946303,21.678971067782,24.1535011655306,31.4254158451728,33.2132772002847,33.3932523143248,34.6112872226547,35.8422925903502,38.1197970643698,38.5089508645146,41.450327323606,41.8217990414581,44.5244168870092,45.0921272541501,45.3981863391866,45.4755037211138,48.1311330964957,48.834259106233,49.5652262974082,50.7590607297457,51.3051262098905,51.7534073854946,51.9211358969013,52.4457186578148,53.2784804574114,53.5358017241091,53.7465306991545,53.8070808066603,53.8595076359884,58.3982627631644,59.3019836837973,60.1812386276582,61.1517703415107,61.2962201938508,62.042412657862,65.7538335624492,69.0373661022844,69.5534503323166,69.5547628395773,72.6497002456673,73.6877783427797,75.6723014817167,75.8929849188134,76.1970037139719,76.9574518203509,77.4285710536907,78.6322810999683,81.9403659754506,94.0130291926447,97.3827210935298],[19.1248081420242,21.9068553151759,24.3813854129245,31.6533000925667,33.4411614476786,33.6211365617187,34.8391714700486,36.0701768377441,38.3476813117637,38.7368351119084,41.6782115709999,42.049683288852,44.752301134403,45.320011501544,45.6260705865805,45.7033879685077,48.3590173438896,49.0621433536269,49.7931105448021,50.9869449771396,51.5330104572844,51.9812916328885,52.1490201442952,52.6736029052086,53.5063647048053,53.763685971503,53.9744149465484,54.0349650540541,54.0873918833822,58.6261470105583,59.5298679311912,60.4091228750521,61.3796545889046,61.5241044412447,62.2702969052559,65.9817178098431,69.2652503496783,69.7813345797105,69.7826470869712,72.8775844930612,73.9156625901735,75.9001857291105,76.1208691662073,76.4248879613658,77.1853360677448,77.6564553010846,78.8601653473622,82.1682502228444,94.2409134400385,97.6106053409236],[22.2213661323569,25.0034133055086,27.4779434032572,34.7498580828994,36.5377194380113,36.7176945520514,37.9357294603813,39.1667348280768,41.4442393020964,41.8333931022411,44.7747695613326,45.1462412791847,47.8488591247357,48.4165694918767,48.7226285769132,48.7999459588404,51.4555753342223,52.1587013439596,52.8896685351348,54.0835029674723,54.6295684476171,55.0778496232212,55.2455781346279,55.7701608955413,56.602922695138,56.8602439618357,57.0709729368811,57.1315230443868,57.1839498737149,61.722705000891,62.6264259215239,63.5056808653848,64.4762125792373,64.6206624315774,65.3668548955886,69.0782758001758,72.361808340011,72.8778925700432,72.8792050773039,75.9741424833939,77.0122205805063,78.9967437194432,79.21742715654,79.5214459516985,80.2818940580775,80.7530132914173,81.9567233376949,85.2648082131771,97.3374714303712,100.707163331256],[24.5555850124923,27.337632185644,29.8121622833926,37.0840769630348,38.8719383181467,39.0519134321868,40.2699483405167,41.5009537082122,43.7784581822318,44.1676119823765,47.108988441468,47.4804601593201,50.1830780048711,50.7507883720121,51.0568474570485,51.1341648389758,53.7897942143576,54.4929202240949,55.2238874152702,56.4177218476077,56.9637873277524,57.4120685033566,57.5797970147633,58.1043797756767,58.9371415752734,59.194462841971,59.4051918170164,59.4657419245222,59.5181687538503,64.0569238810264,64.9606448016593,65.8398997455201,66.8104314593727,66.9548813117128,67.7010737757239,71.4124946803112,74.6960272201464,75.2121114501786,75.2134239574392,78.3083613635292,79.3464394606416,81.3309625995786,81.5516460366754,81.8556648318339,82.6161129382128,83.0872321715527,84.2909422178303,87.5990270933125,99.6716903105066,103.041382211392],[25.0947772180414,27.8768243911932,30.3513544889417,37.6232691685839,39.4111305236958,39.591105637736,40.8091405460658,42.0401459137614,44.317650387781,44.7068041879257,47.6481806470171,48.0196523648692,50.7222702104203,51.2899805775612,51.5960396625977,51.6733570445249,54.3289864199068,55.0321124296441,55.7630796208194,56.9569140531568,57.5029795333016,57.9512607089057,58.1189892203124,58.6435719812259,59.4763337808225,59.7336550475202,59.9443840225656,60.0049341300714,60.0573609593995,64.5961160865755,65.4998370072084,66.3790919510693,67.3496236649218,67.494073517262,68.2402659812731,71.9516868858603,75.2352194256955,75.7513036557277,75.7526161629884,78.8475535690784,79.8856316661908,81.8701548051278,82.0908382422245,82.394857037383,83.155305143762,83.6264243771018,84.8301344233794,88.1382192988617,100.210882516056,103.580574416941],[25.4470689871613,28.2291161603131,30.7036462580616,37.9755609377039,39.7634222928157,39.9433974068559,41.1614323151858,42.3924376828813,44.6699421569009,45.0590959570456,48.000472416137,48.3719441339891,51.0745619795402,51.6422723466812,51.9483314317176,52.0256488136449,54.6812781890267,55.384404198764,56.1153713899393,57.3092058222767,57.8552713024215,58.3035524780256,58.4712809894323,58.9958637503458,59.8286255499424,60.0859468166401,60.2966757916855,60.3572258991913,60.4096527285194,64.9484078556955,65.8521287763284,66.7313837201892,67.7019154340418,67.8463652863819,68.592557750393,72.3039786549803,75.5875111948154,76.1035954248476,76.1049079321083,79.1998453381983,80.2379234353107,82.2224465742477,82.4431300113445,82.747148806503,83.5075969128819,83.9787161462218,85.1824261924994,88.4905110679816,100.563174285176,103.932866186061],[25.8724295929013,28.6544767660531,31.1290068638016,38.4009215434439,40.1887828985557,40.3687580125959,41.5867929209257,42.8177982886213,45.0953027626409,45.4844565627856,48.425833021877,48.7973047397291,51.4999225852802,52.0676329524212,52.3736920374576,52.4510094193848,55.1066387947667,55.809764804504,56.5407319956793,57.7345664280167,58.2806319081615,58.7289130837656,58.8966415951723,59.4212243560858,60.2539861556824,60.5113074223801,60.7220363974255,60.7825865049313,60.8350133342594,65.3737684614355,66.2774893820684,67.1567443259292,68.1272760397817,68.2717258921219,69.017918356133,72.7293392607202,76.0128718005554,76.5289560305876,76.5302685378483,79.6252059439383,80.6632840410507,82.6478071799877,82.8684906170844,83.172509412243,83.9329575186219,84.4040767519618,85.6077867982394,88.9158716737216,100.988534890916,104.358226791801],[26.4127076066918,29.1947547798436,31.6692848775921,38.9411995572343,40.7290609123462,40.9090360263864,42.1270709347162,43.3580763024118,45.6355807764314,46.0247345765761,48.9661110356675,49.3375827535196,52.0402005990707,52.6079109662117,52.9139700512481,52.9912874331753,55.6469168085572,56.3500428182945,57.0810100094698,58.2748444418072,58.820909921952,59.2691910975561,59.4369196089628,59.9615023698763,60.7942641694729,61.0515854361706,61.262314411216,61.3228645187218,61.3752913480499,65.914046475226,66.8177673958589,67.6970223397197,68.6675540535722,68.8120039059124,69.5581963699235,73.2696172745107,76.5531498143459,77.0692340443781,77.0705465516388,80.1654839577288,81.2035620548412,83.1880851937782,83.4087686308749,83.7127874260335,84.4732355324124,84.9443547657523,86.1480648120299,89.4561496875121,101.528812904706,104.898504805591],[26.5786125959716,29.3606597691234,31.8351898668719,39.1071045465141,40.894965901626,41.0749410156662,42.292975923996,43.5239812916916,45.8014857657112,46.1906395658559,49.1320160249473,49.5034877427994,52.2061055883505,52.7738159554914,53.0798750405279,53.1571924224551,55.812821797837,56.5159478075743,57.2469149987496,58.440749431087,58.9868149112318,59.4350960868359,59.6028245982426,60.1274073591561,60.9601691587527,61.2174904254504,61.4282194004958,61.4887695080016,61.5411963373297,66.0799514645058,66.9836723851386,67.8629273289995,68.833459042852,68.9779088951922,69.7241013592033,73.4355222637905,76.7190548036257,77.2351390336579,77.2364515409186,80.3313889470086,81.369467044121,83.353990183058,83.5746736201547,83.8786924153133,84.6391405216922,85.110259755032,86.3139698013097,89.6220546767919,101.694717893986,105.064409794871],[26.6915583878672,29.473605561019,31.9481356587675,39.2200503384098,41.0079116935216,41.1878868075618,42.4059217158917,43.6369270835872,45.9144315576068,46.3035853577515,49.2449618168429,49.616433534695,52.3190513802461,52.8867617473871,53.1928208324235,53.2701382143508,55.9257675897326,56.6288935994699,57.3598607906452,58.5536952229826,59.0997607031274,59.5480418787315,59.7157703901382,60.2403531510517,61.0731149506483,61.330436217346,61.5411651923914,61.6017152998972,61.6541421292253,66.1928972564014,67.0966181770343,67.9758731208951,68.9464048347477,69.0908546870878,69.8370471510989,73.5484680556861,76.8320005955213,77.3480848255535,77.3493973328142,80.4443347389042,81.4824128360166,83.4669359749536,83.6876194120504,83.9916382072089,84.7520863135878,85.2232055469277,86.4269155932053,89.7350004686875,101.807663685882,105.177355586767],[27.4224908051711,30.2045379783229,32.6790680760714,39.9509827557137,41.7388441108255,41.9188192248657,43.1368541331955,44.3678595008911,46.6453639749107,47.0345177750554,49.9758942341468,50.3473659519989,53.04998379755,53.617694164691,53.9237532497274,54.0010706316546,56.6567000070365,57.3598260167738,58.0907932079491,59.2846276402865,59.8306931204313,60.2789742960354,60.4467028074421,60.9712855683556,61.8040473679522,62.0613686346499,62.2720976096953,62.3326477172011,62.3850745465292,66.9238296737053,67.8275505943381,68.706805538199,69.6773372520516,69.8217871043917,70.5679795684028,74.27940047299,77.5629330128252,78.0790172428574,78.0803297501181,81.1752671562081,82.2133452533205,84.1978683922575,84.4185518293543,84.7225706245128,85.4830187308917,85.9541379642316,87.1578480105092,90.4659328859914,102.538596103185,105.908288004071],[28.36726919622,31.1493163693718,33.6238464671203,40.8957611467626,42.6836225018744,42.8635976159146,44.0816325242445,45.31263789194,47.5901423659596,47.9792961661043,50.9206726251957,51.2921443430478,53.9947621885989,54.5624725557399,54.8685316407763,54.9458490227036,57.6014783980854,58.3046044078227,59.035571598998,60.2294060313354,60.7754715114802,61.2237526870843,61.391481198491,61.9160639594045,62.7488257590011,63.0061470256988,63.2168760007442,63.27742610825,63.3298529375781,67.8686080647542,68.7723289853871,69.6515839292479,70.6221156431005,70.7665654954406,71.5127579594517,75.2241788640389,78.5077114038741,79.0237956339063,79.025108141167,82.120045547257,83.1581236443694,85.1426467833064,85.3633302204032,85.6673490155617,86.4277971219406,86.8989163552805,88.1026264015581,91.4107112770403,103.483374494234,106.853066395119],[28.6401623680532,31.422209541205,33.8967396389535,41.1686543185957,42.9565156737076,43.1364907877478,44.3545256960776,45.5855310637732,47.8630355377928,48.2521893379375,51.1935657970289,51.565037514881,54.2676553604321,54.835365727573,55.1414248126095,55.2187421945367,57.8743715699186,58.5774975796559,59.3084647708312,60.5022992031686,61.0483646833134,61.4966458589175,61.6643743703242,62.1889571312377,63.0217189308343,63.279040197532,63.4897691725774,63.5503192800832,63.6027461094113,68.1415012365873,69.0452221572202,69.9244771010811,70.8950088149336,71.0394586672738,71.7856511312849,75.4970720358721,78.7806045757073,79.2966888057395,79.2980013130002,82.3929387190902,83.4310168162026,85.4155399551396,85.6362233922363,85.9402421873948,86.7006902937738,87.1718095271137,88.3755195733912,91.6836044488735,103.756267666068,107.125959566953],[31.0416845357562,33.8237317089079,36.2982618066565,43.5701764862987,45.3580378414106,45.5380129554508,46.7560478637806,47.9870532314761,50.2645577054958,50.6537115056405,53.5950879647319,53.966559682584,56.6691775281351,57.236887895276,57.5429469803125,57.6202643622397,60.2758937376216,60.9790197473589,61.7099869385342,62.9038213708716,63.4498868510164,63.8981680266205,64.0658965380272,64.5904792989407,65.4232410985373,65.680562365235,65.8912913402804,65.9518414477862,66.0042682771142,70.5430234042903,71.4467443249232,72.3259992687841,73.2965309826366,73.4409808349768,74.1871732989879,77.8985942035751,81.1821267434103,81.6982109734425,81.6995234807032,84.7944608867932,85.8325389839056,87.8170621228426,88.0377455599393,88.3417643550978,89.1022124614768,89.5733316948166,90.7770417410942,94.0851266165765,106.157789833771,109.527481734656],[31.4501855312686,34.2322327044203,36.7067628021689,43.9786774818111,45.766538836923,45.9465139509631,47.164548859293,48.3955542269885,50.6730587010081,51.0622125011528,54.0035889602443,54.3750606780964,57.0776785236474,57.6453888907884,57.9514479758249,58.0287653577521,60.684394733134,61.3875207428713,62.1184879340465,63.312322366384,63.8583878465288,64.3066690221329,64.4743975335396,64.998980294453,65.8317420940497,66.0890633607474,66.2997923357928,66.3603424432985,66.4127692726266,70.9515243998027,71.8552453204356,72.7345002642965,73.705031978149,73.8494818304891,74.5956742945003,78.3070951990875,81.5906277389227,82.1067119689549,82.1080244762156,85.2029618823056,86.2410399794179,88.225563118355,88.4462465554517,88.7502653506102,89.5107134569892,89.981832690329,91.1855427366066,94.4936276120889,106.566290829283,109.935982730168],[32.5022203309191,35.2842675040709,37.7587976018194,45.0307122814617,46.8185736365735,46.9985487506137,48.2165836589435,49.4475890266391,51.7250935006587,52.1142473008034,55.0556237598948,55.4270954777469,58.129713323298,58.697423690439,59.0034827754754,59.0808001574026,61.7364295327845,62.4395555425218,63.1705227336971,64.3643571660345,64.9104226461793,65.3587038217834,65.5264323331901,66.0510150941036,66.8837768937002,67.1410981603979,67.3518271354433,67.4123772429491,67.4648040722772,72.0035591994533,72.9072801200862,73.786535063947,74.7570667777996,74.9015166301397,75.6477090941508,79.359129998738,82.6426625385732,83.1587467686054,83.1600592758661,86.2549966819561,87.2930747790685,89.2775979180055,89.4982813551023,89.8023001502608,90.5627482566397,91.0338674899796,92.2375775362572,95.5456624117394,107.618325628933,110.988017529819],[38.2396695158738,41.0217166890255,43.4962467867741,50.7681614664163,52.5560228215282,52.7359979355683,53.9540328438982,55.1850382115937,57.4625426856133,57.851696485758,60.7930729448495,61.1645446627016,63.8671625082526,64.4348728753936,64.7409319604301,64.8182493423573,67.4738787177392,68.1770047274765,68.9079719186517,70.1018063509892,70.647871831134,71.0961530067381,71.2638815181448,71.7884642790582,72.6212260786549,72.8785473453525,73.0892763203979,73.1498264279037,73.2022532572318,77.7410083844079,78.6447293050408,79.5239842489017,80.4945159627542,80.6389658150943,81.3851582791055,85.0965791836927,88.3801117235279,88.8961959535601,88.8975084608208,91.9924458669108,93.0305239640231,95.0150471029601,95.2357305400569,95.5397493352154,96.3001974415943,96.7713166749342,97.9750267212118,101.283111596694,113.355774813888,116.725466714773],[40.9405976645243,43.722644837676,46.1971749354246,53.4690896150668,55.2569509701787,55.4369260842188,56.6549609925487,57.8859663602442,60.1634708342638,60.5526246344085,63.4940010935,63.8654728113521,66.5680906569031,67.1358010240441,67.4418601090806,67.5191774910078,70.1748068663897,70.877932876127,71.6089000673022,72.8027344996397,73.3487999797845,73.7970811553886,73.9648096667953,74.4893924277087,75.3221542273054,75.579475494003,75.7902044690484,75.8507545765542,75.9031814058823,80.4419365330584,81.3456574536913,82.2249123975522,83.1954441114047,83.3398939637448,84.086086427756,87.7975073323432,91.0810398721784,91.5971241022106,91.5984366094713,94.6933740155613,95.7314521126736,97.7159752516106,97.9366586887074,98.2406774838659,99.0011255902449,99.4722448235847,100.675954869862,103.984039745345,116.056702962539,119.426394863424]],"type":"surface","x":[-4.44585033319053,-4.22639470950259,-4.07597421713483,-3.264857723815,-2.48735664250955,-2.42707453768341,-2.11519570831633,-2.06039261721247,-2.05796397926283,-1.99567685626452,-1.74115622451856,-1.62586985030409,-1.52081986853802,-1.45176684910048,-1.24040682971251,-1.00606930056544,-0.592730451548333,-0.591212166502939,-0.585334790655924,-0.548086147958929,-0.543066183934002,-0.48893767800831,-0.434049739259119,-0.349608871758296,-0.0589047620367544,0.0917772684483983,0.101677188693447,0.180002104549399,0.281096441858762,0.427746220865478,0.551097448896429,0.670447232350085,0.845580921775133,0.891601535342018,1.51694312999049,1.9883323826121,2.09722080475462,2.16836518421425,2.25426561957652,2.36337331778596,2.39687738613548,2.41968648637948,2.56729633087264,2.75809179715889,2.81320184249814,3.29818281382937,3.38067849605031,3.59313410685851,4.75179660687796,5.29724181435705],"y":[-4.41912104116896,-4.07102184072386,-3.76140022222062,-2.85151353513735,-2.62781024418413,-2.60529114605309,-2.4528864773632,-2.29885890070173,-2.01388979806851,-1.96519755158619,-1.59716252921171,-1.55068272497971,-1.21252199204031,-1.14148814085572,-1.10319298762528,-1.09351877394224,-0.761237398137361,-0.67325988137785,-0.581798781345011,-0.432422158485776,-0.364096589148385,-0.308006124432938,-0.287019362858232,-0.221381784385977,-0.117183796875793,-0.0849868851701209,-0.0586197593960398,-0.051043524092399,-0.0444837009711016,0.523420770970757,0.636497404945093,0.746512772728603,0.867949000287586,0.886023056899303,0.979389194830804,1.44377479614985,1.85462155057681,1.9191957639727,1.9193599893433,2.30660907946819,2.43649694013844,2.68480722207595,2.71241988398414,2.75045974920866,2.8456096015782,2.90455764172021,3.05516993496982,3.46908876110364,4.97966142633536,5.40128873293331],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m1)</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   48.855298   11.052508    2.505395</code></pre>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m2)</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   49.630994    9.878279    3.054908</code></pre>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m3)</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   50.389177    3.369722    9.739047</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="#cb324-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(m4)</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   50.027890    4.951786    7.992110</code></pre>
</details>
<div id="variance-inflation-factor" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Variance Inflation Factor</h3>
<p>In this section we introduce the <strong>Variance Inflation Factor (VIF)</strong>, which can be used for measuring the effect of multicollinearity on the variances (or standard errors) of the parameter estimators. We will present the results without proof.</p>
<p>We will be looking for an alternative expression for <span class="math inline">\(\var{\hat\beta_j}\)</span> that consists of two factors:</p>
<ul>
<li><p>factor 1: the variance of <span class="math inline">\(\hat\beta_j\)</span> as if there were no multicollinearity</p></li>
<li><p>factor 2: the variance inflation factor (VIF)</p></li>
</ul>
<p>To do so, we will need an auxiliary regression model that has <span class="math inline">\(x_j\)</span> acting as the outcome variable, and the other <span class="math inline">\(p-2\)</span> regressors acting as regressors. In particular,</p>
<p><span class="math display" id="eq:tmp72765267">\[\begin{equation}
  \tag{3.4}
  X_{ij} = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_{j-1}x_{ij-1} + \beta_{j+1}x_{ij+1} + \cdots + \beta_{p-1}x_{ip-1} +\eps_i^*
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\E{\eps_i^*}=0\)</span> and <span class="math inline">\(\var{\eps_i^*}\)</span> constant, and with <span class="math inline">\(X_{ij}\)</span> the notation for <span class="math inline">\(x_{ij}\)</span> considered as outcome.
For this model, the coefficient of determination is denoted by <span class="math inline">\(R_j^2\)</span>, which can be understood by considering two extreme situations:</p>
<ul>
<li><p><span class="math inline">\(R_j^2=0\)</span>: the variability of regressor <span class="math inline">\(j\)</span> cannot be explained by a linear model with the other regressors acting as the regressors. This extreme situation happens if regressor <span class="math inline">\(j\)</span> is linearly independent of the other regressors.</p></li>
<li><p><span class="math inline">\(R_j^2=1\)</span>: the variability of regressor <span class="math inline">\(j\)</span> can be completely explained by a linear model of the other regressors. This extreme situation happens if regressor <span class="math inline">\(j\)</span> is a linear combination of the other regressors (for all <span class="math inline">\(n\)</span> sample observations).</p></li>
</ul>
<p>Hence, <span class="math inline">\(R_j^2\)</span> measures the to what extent the multicollinearity affects regressor <span class="math inline">\(j\)</span>.</p>
<p>Before presenting the alternative expression for <span class="math inline">\(\var{\hat\beta_j}\)</span>, we recall its expression when <span class="math inline">\(x_j\)</span> is the only regressor in a simple linear regression model (see Equation <a href="#eq:SigmaBetaLSE">(2.6)</a>):
<span class="math display">\[
  \var{\hat\beta_j} = \frac{\sigma_j^2}{\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2}
\]</span>
in which <span class="math inline">\(\sigma_j^2\)</span> is the residual variance in the simple linear regression model with only <span class="math inline">\(x_j\)</span> as regressor.</p>
<p>We are now ready to give an alternative expression of <span class="math inline">\(\var{\hat\beta_j}\)</span> in the multiple linear regression model <a href="#eq:Mod5">(3.1)</a>:</p>
<p><span class="math display" id="eq:tmp76198716981">\[\begin{equation}
  \tag{3.5}
 \var{\hat\beta_j} = \frac{\sigma^2}{\sum_{i=1}^n (x_{ij}-\bar{x}_j)^2} \frac{1}{1-R_j^2} ,
\end{equation}\]</span></p>
<p>in which <span class="math inline">\(\sigma^2\)</span> is the residual variance of model <a href="#eq:Mod5">(3.1)</a> (i.e. the model that includes all <span class="math inline">\(p-1\)</span> regressors).</p>
<p>Equation <a href="#eq:tmp76198716981">(3.5)</a> tells us that <span class="math inline">\(\var{\hat\beta_j}\)</span> is the product of</p>
<ul>
<li><p>the variance of <span class="math inline">\(\hat\beta_j\)</span> in a model without collinearity (<span class="math inline">\(R_j^2=0\)</span>) for regressor <span class="math inline">\(j\)</span></p></li>
<li><p>the variance inflation factor (VIF)</p></li>
</ul>
<p>When all regressors are uncorrelated, we say we have <strong>orthogonal regressors</strong>. In this case all <span class="math inline">\(R_j^2=0\)</span> and the variances of the parameter estimators obtain their minimal values (most precise estimators), for given sample size, given error term variance and for given observed values of the regressors.</p>
<p>What can we do to remediate a large VIF? You may want to remove one or more regressors from the model (if this is allowed and makes sense keeping the research question in mind).</p>
<p>Finally, what is a large, problematic value for the VIF? Some guidelines give 10 as a threshold, others give 5. As always in statistics, don’t ever apply a strict threshold!</p>
</div>
</div>
<div id="example-lead-concentration-5" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We consider the model with the three regressors (<em>Ld73</em>, <em>Totyrs</em> and <em>Age</em>), and check the multicollinearity. One reason for doing so, is that we noted that the significance of <em>Totyrs</em> disappeared when <em>Age</em> was added to the model. Although we had a reasonable explanation, we also saw a moderately large correlation between <em>Totyrs</em> and <em>Age</em>. This correlation could have caused multicollinearity, which in turn may have caused the insignificance of the effect of <em>Totyrs</em>.</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="#cb326-1" aria-hidden="true" tabindex="-1"></a>R2.Ld73<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Ld73<span class="sc">~</span>Totyrs<span class="sc">+</span>Age, <span class="at">data=</span>lead,</span>
<span id="cb326-2"><a href="#cb326-2" aria-hidden="true" tabindex="-1"></a>                    <span class="at">subset=</span><span class="sc">!</span><span class="fu">is.na</span>(MAXFWT)))<span class="sc">$</span>r.squared</span>
<span id="cb326-3"><a href="#cb326-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>R2.Ld73)</span></code></pre></div>
<pre><code>## [1] 1.072777</code></pre>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="#cb328-1" aria-hidden="true" tabindex="-1"></a>R2.Totyrs<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Totyrs<span class="sc">~</span>Ld73<span class="sc">+</span>Age, <span class="at">data=</span>lead,</span>
<span id="cb328-2"><a href="#cb328-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">subset=</span><span class="sc">!</span><span class="fu">is.na</span>(MAXFWT)))<span class="sc">$</span>r.squared</span>
<span id="cb328-3"><a href="#cb328-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>R2.Totyrs)</span></code></pre></div>
<pre><code>## [1] 1.541473</code></pre>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="#cb330-1" aria-hidden="true" tabindex="-1"></a>R2.Age<span class="ot">&lt;-</span><span class="fu">summary</span>(<span class="fu">lm</span>(Age<span class="sc">~</span>Ld73<span class="sc">+</span>Totyrs, <span class="at">data=</span>lead,</span>
<span id="cb330-2"><a href="#cb330-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">subset=</span><span class="sc">!</span><span class="fu">is.na</span>(MAXFWT)))<span class="sc">$</span>r.squared</span>
<span id="cb330-3"><a href="#cb330-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>R2.Age)</span></code></pre></div>
<pre><code>## [1] 1.592838</code></pre>
<p>The VIF of <em>Ld73</em> is very close to 1, and hence is not problematic at all. The VIFs of <em>Totyrs</em> and <em>Age</em> are slightly larger (<span class="math inline">\(1.5\)</span> to <span class="math inline">\(1.6\)</span>), but this is still not considered to be problematic. If the VIF<span class="math inline">\(\approx 2\)</span>, it means that the variance of the parameter estimator is 2 times larger than if there were no multicollinearity, and hence the standard error is <span class="math inline">\(\sqrt{2}=1.4\)</span> larger as compared to no multicollinearity.</p>
<p>Here is a faster way for calculating the VIFs.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(<span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Totyrs<span class="sc">+</span>Age<span class="sc">+</span>Ld73, <span class="at">data=</span>lead))</span></code></pre></div>
<pre><code>##   Totyrs      Age     Ld73 
## 1.541473 1.592838 1.072777</code></pre>
</div>
<div id="leverage" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Leverage</h2>
<p>Residual plots are also useful for identifying outliers. Recall that it is not adviced for the statistician to remove observations only because they were identified as outliers. On the other hand, it is a task of the statistician to identify outliers and to report them so that the scientists who are closer to the data and the study can check whether perhaps something went wrong that may explain the outlying behavior of this data point.
Sometimes an outlier does not stronly affect the parameter estimates and the conlcusions. Such outliers are usually not problematic. Other times an outlier may be very <strong>influential</strong> in the sense that this outlying observation has a strong effect on the numerical values of the parameter estimates. Such outliers are problematic and worrisome.</p>
<p>In this section we introduce the <strong>leverage</strong> of an observation, as a measure of the observation’s influence on the regression fit.</p>
<p>In matrix notation, the vector of predictions can be written as
<span class="math display">\[
  \hat{\mb{Y}} = \mb{X}\hat{\mb\beta} = \mb{X}(\mb{X}^t\mb{X})^{-1}\mb{X}^t\mb{Y} = \mb{HY},
\]</span>
where the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mb{H}\)</span> is generally known as the <strong>hat-matrix</strong>.</p>
<p>Note that the hat matrix is <em>idempotent</em>, i.e. 
<span class="math display">\[
  \mb{H}\mb{H}^t=\mb{H}^t\mb{H} = \mb{HH} =\mb{H}.
\]</span></p>
<p>The <span class="math inline">\(i\)</span>th element of <span class="math inline">\(\hat{\mb{Y}}\)</span>, i.e. <span class="math inline">\(\hat{Y}_i\)</span>, can be written as.
<span class="math display">\[
  \sum_{j=1}^n h_{ij} Y_j
\]</span>
with <span class="math inline">\(h_{ij}\)</span> the element on position <span class="math inline">\((i,j)\)</span> of the matrix matrix <span class="math inline">\(\mb{H}\)</span>.
This equation demonstrates that the predictions are linear functions of the outcomes (it is an example of a <em>linear predictor</em>).</p>
<p>Without proof we give here the following property:
<span class="math display">\[
  \sum_{j=1}^n h_{ij}=1 \;\;\text{ for all } \;\; i=1,\ldots, n.
\]</span></p>
<p>For the prediction of observation <span class="math inline">\(i\)</span> we can write</p>
<p><span class="math display" id="eq:tmp7167979207">\[\begin{equation}
  \hat{Y}_i = \mb{h}_i^t\mb{Y}
  \tag{3.6}
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\mb{h}_i^t\)</span> the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(\mb{H}\)</span>. Sicne the sum of the elements of <span class="math inline">\(\mb{h}_i\)</span> always equals 1, Equation <a href="#eq:tmp7167979207">(3.6)</a> shows that the prediction <span class="math inline">\(\hat{Y}_i\)</span> is a weighted mean of the sample outcomes <span class="math inline">\(Y_1,\ldots, Y_n\)</span>.</p>
<p>This interpretation allows us to evaluate the elements of the vector <span class="math inline">\(\mb{h}_i\)</span>:</p>
<ul>
<li><p>if <span class="math inline">\(h_{ij}\)</span> is large (relative to the other elements), then outcome <span class="math inline">\(Y_j\)</span> strongly affects the prediction <span class="math inline">\(\hat{Y}_i\)</span>.</p></li>
<li><p>A global measure for the influence of observation <span class="math inline">\(Y_i\)</span> on the predictions <span class="math inline">\(\hat{Y}_1,\ldots, \hat{Y}_n\)</span> is given by
<span class="math display">\[
 \sum_{j=1}^n h_{ij}^2 = \mb{h}_i^t\mb{h}_i = h_{ii} 
 \]</span>
(the final equality follows from <span class="math inline">\(\mb{HH}=\mb{H}\)</span>).
The square (<span class="math inline">\(h_{ij}^2\)</span>) is used because both large positive and large negative <span class="math inline">\(h_{ij}\)</span> imply that <span class="math inline">\(Y_i\)</span> is influential.</p></li>
</ul>
<p>The <strong>leverage</strong> of observation <span class="math inline">\(i\)</span> is defined as <span class="math inline">\(h_{ii}\)</span> and it is thus a global measure of the influence of observation <span class="math inline">\(i\)</span> on the predictions.</p>
<p>It can also be shown that <span class="math inline">\(\sum_{i=1}^n h_{ii} =p\)</span>. This may help in thresholding the individual <span class="math inline">\(h_{ii}\)</span> leverage values, i.e. the avere of the <span class="math inline">\(h_{ii}\)</span> is thus given by <span class="math inline">\(p/n\)</span>. Leverages much larger than <span class="math inline">\(p/n\)</span> may be called influential.</p>
<p>If an observation <span class="math inline">\(i\)</span> is identified as an outlier, and if its leverage <span class="math inline">\(h_{ii}\)</span> is large, then we call observation <span class="math inline">\(i\)</span> an <strong>influential outlier</strong>.</p>
</div>
<div id="example-lead-concentration-6" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We will look for influential outliers in the Lead Concentration example.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="#cb334-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">+</span>Totyrs,<span class="at">data=</span>lead,<span class="at">x=</span>T)</span>
<span id="cb334-2"><a href="#cb334-2" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span>m<span class="sc">$</span>x</span>
<span id="cb334-3"><a href="#cb334-3" aria-hidden="true" tabindex="-1"></a>H<span class="ot">&lt;-</span>X<span class="sc">%*%</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)<span class="sc">%*%</span><span class="fu">t</span>(X)</span>
<span id="cb334-4"><a href="#cb334-4" aria-hidden="true" tabindex="-1"></a>h<span class="ot">&lt;-</span><span class="fu">diag</span>(H)</span>
<span id="cb334-5"><a href="#cb334-5" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(h)</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(h,<span class="at">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb336-2"><a href="#cb336-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(h)<span class="sc">/</span><span class="fu">nrow</span>(lead[indNA,]),<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<p>Another and simpler way of computing the leverages:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="#cb337-1" aria-hidden="true" tabindex="-1"></a>h<span class="ot">&lt;-</span><span class="fu">influence</span>(m)<span class="sc">$</span>h</span></code></pre></div>
</div>
<div id="assessment-of-the-model-assumptions-and-remedial-measures" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Assessment of the Model Assumptions and Remedial Measures</h2>
<p>In Section <a href="#S:AssessAssumptions">2.8</a> we illustrated methods for assessing the model assumptions for simple linear regression. In this section we extent these methods to multiple linear regression models. We will also briefly explain what can be done in case of model violations.</p>
<div id="residual-analysis" class="section level3 unnumbered">
<h3>Residual analysis</h3>
<p>In principle all methods of Section <a href="#S:AssessAssumptions">2.8</a> are still applicable:</p>
<ul>
<li><p><strong>normal QQ-plots</strong> of the residuals for checking the assumption of normality of the error terms</p></li>
<li><p><strong>residual plots</strong> constructed as residuals versus a regressor. For multiple linear regression, such a plot can be plotted for each regressor separately. These plots can be used for checking the correctness of the model <span class="math inline">\(m(\mb{x})\)</span> as a function of <span class="math inline">\(\mb{x}\)</span> and for checking the homoscedasticity assumption. For the latter purpose also squared residuals or absolute values of the residuals may be plotted.</p></li>
</ul>
<p>Regarding the residual plots: although preference is given to making a residual plot for each regressor separately, some people construct only one single residual plot, but with the predicted outcomes <span class="math inline">\(\hat{Y}_i\)</span> on the horizontal axis. This is explained in the next paragraph.</p>
<p>Consider the model
<span class="math display">\[
  Y_i = m(\mb{x}) + \eps_i = \mb\beta^t\mb{x} + \eps_i,
\]</span>
with <span class="math inline">\(\E{\eps_i \mid \mb{x}}=0\)</span> and <span class="math inline">\(\var{\eps_i \mid \mb{x}}=\sigma^2\)</span>, <span class="math inline">\(i=1,\ldots, n\)</span>.</p>
<p>The expression <span class="math inline">\(\E{\eps_i \mid \mb{x}}=0\)</span> implies</p>
<ul>
<li><p><span class="math inline">\(\E{\eps_i \mid x_j}=0\)</span> for all <span class="math inline">\(j=1,\ldots, p-1\)</span> (this is why individual residual plots are meaningful)</p></li>
<li><p><span class="math inline">\(\E{\eps_i \mid \mb\beta^t\mb{x}}=0\)</span> for all <span class="math inline">\(\mb\beta\)</span> and hence als for <span class="math inline">\(\mb\beta=\hat{\mb\beta}\)</span>. This explains why residual plots of residuals versus <span class="math inline">\(\hat{Y}_i=\hat{\mb\beta}^t\mb{x}_i\)</span> may be plotted.</p></li>
</ul>
<p>Similar arguments apply to <span class="math inline">\(\var{\eps_i \mid \mb{x}}=\sigma^2\)</span>.</p>
<p>Finally, we note that <strong>(influential) outlier detection</strong> and <strong>multicolinearity</strong> assessment may also be part of the assessment.</p>
</div>
</div>
<div id="example-lead-concentration-7" class="section level2 unnumbered">
<h2>Example (Lead Concentration)</h2>
<p>We will now assess the model assumptions for the additive model with the following regressors: <em>Ld73</em>, <em>Totyrs</em> and <em>Age</em>.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="#cb338-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">+</span>Totyrs<span class="sc">+</span>Age, <span class="at">data=</span>lead)</span>
<span id="cb338-2"><a href="#cb338-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb338-3"><a href="#cb338-3" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span><span class="fu">residuals</span>(m)</span>
<span id="cb338-4"><a href="#cb338-4" aria-hidden="true" tabindex="-1"></a>YHat<span class="ot">&lt;-</span><span class="fu">predict</span>(m)</span>
<span id="cb338-5"><a href="#cb338-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb338-6"><a href="#cb338-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(e)</span>
<span id="cb338-7"><a href="#cb338-7" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(e)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="#cb339-1" aria-hidden="true" tabindex="-1"></a>indNA<span class="ot">&lt;-</span><span class="sc">!</span><span class="fu">is.na</span>(lead<span class="sc">$</span>MAXFWT)</span>
<span id="cb339-2"><a href="#cb339-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb339-3"><a href="#cb339-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb339-4"><a href="#cb339-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Ld73[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Blood lead concentration (microgram / 100ml)&quot;</span>,</span>
<span id="cb339-5"><a href="#cb339-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb339-6"><a href="#cb339-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb339-7"><a href="#cb339-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Number of years living near smelter&quot;</span>,</span>
<span id="cb339-8"><a href="#cb339-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb339-9"><a href="#cb339-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb339-10"><a href="#cb339-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Age[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Age (years)&quot;</span>,</span>
<span id="cb339-11"><a href="#cb339-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb339-12"><a href="#cb339-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb339-13"><a href="#cb339-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(YHat[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Predicted MAXFWT (taps per 10 seconds)&quot;</span>,</span>
<span id="cb339-14"><a href="#cb339-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb339-15"><a href="#cb339-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-103-2.png" width="672" /></p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>The normal QQ plot shows an asymmetric deviation from normality in the left tail. However, since the sample size is not very small (<span class="math inline">\(n=83\)</span>), we do not consider this a serious problem and we believe that all inference will still be approximately correct.</p>
<p>None of the residual plots show any severe deviation from the model assumptions. The plots do not indicate large outliers, but we will also assess the leverage of the observations.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="#cb341-1" aria-hidden="true" tabindex="-1"></a>h<span class="ot">&lt;-</span><span class="fu">influence</span>(m)<span class="sc">$</span>h</span>
<span id="cb341-2"><a href="#cb341-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(h,<span class="at">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb341-3"><a href="#cb341-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(h)<span class="sc">/</span><span class="fu">nrow</span>(lead[indNA,]),<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<p>This gives not indication of strong influential outliers.</p>
<p>Finally, we assess multicollinearity (the results have been discussed before).</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(m)</span></code></pre></div>
<pre><code>##     Ld73   Totyrs      Age 
## 1.072777 1.541473 1.592838</code></pre>
</div>
<div id="exercise-lead-concentration-2" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Consider now the lead concentration data, and assess the assumptions of the model with <em>Ld73</em>, <em>Totyrs</em> and their interaction effect.</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="#cb344-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73<span class="sc">*</span>Totyrs, <span class="at">data=</span>lead)</span>
<span id="cb344-2"><a href="#cb344-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-3"><a href="#cb344-3" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span><span class="fu">residuals</span>(m)</span>
<span id="cb344-4"><a href="#cb344-4" aria-hidden="true" tabindex="-1"></a>YHat<span class="ot">&lt;-</span><span class="fu">predict</span>(m)</span>
<span id="cb344-5"><a href="#cb344-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb344-6"><a href="#cb344-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(e)</span>
<span id="cb344-7"><a href="#cb344-7" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(e)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="#cb345-1" aria-hidden="true" tabindex="-1"></a>indNA<span class="ot">&lt;-</span><span class="sc">!</span><span class="fu">is.na</span>(lead<span class="sc">$</span>MAXFWT)</span>
<span id="cb345-2"><a href="#cb345-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb345-3"><a href="#cb345-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb345-4"><a href="#cb345-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Ld73[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Blood lead concentration (microgram / 100ml)&quot;</span>,</span>
<span id="cb345-5"><a href="#cb345-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb345-6"><a href="#cb345-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb345-7"><a href="#cb345-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Number of years living near smelter&quot;</span>,</span>
<span id="cb345-8"><a href="#cb345-8" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb345-9"><a href="#cb345-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb345-10"><a href="#cb345-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[indNA]<span class="sc">*</span>lead<span class="sc">$</span>Ld73[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Interaction&quot;</span>,</span>
<span id="cb345-11"><a href="#cb345-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb345-12"><a href="#cb345-12" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb345-13"><a href="#cb345-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(YHat[indNA],e, <span class="at">xlab=</span><span class="st">&quot;Predicted MAXFWT (taps per 10 seconds)&quot;</span>,</span>
<span id="cb345-14"><a href="#cb345-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb345-15"><a href="#cb345-15" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-106-2.png" width="672" /></p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="#cb346-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>None of the graphs show a severe deviation from the model assumptions.</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="#cb347-1" aria-hidden="true" tabindex="-1"></a>h<span class="ot">&lt;-</span><span class="fu">influence</span>(m)<span class="sc">$</span>h</span>
<span id="cb347-2"><a href="#cb347-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(h,<span class="at">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb347-3"><a href="#cb347-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(h)<span class="sc">/</span><span class="fu">nrow</span>(lead[indNA,]),<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="#cb348-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(m)</span></code></pre></div>
<pre><code>##        Ld73      Totyrs Ld73:Totyrs 
##    9.043449   11.256362   20.105660</code></pre>
<p>The leverage plot shows one observation with a clearly larger leverage than the others, but as there were no outliers detected, the leverage is not very important here.</p>
<p>The VIFs, on the other hand, are quite large: 10 to 20. This happens often in models that include interaction terms. Since the interaction is basically the product of regressors in the model, it is not uncommon that these interactions show a large correlation with the regressors they are based on.</p>
</details>
</div>
<div id="exercise-blood-pressure-3" class="section level2 unnumbered">
<h2>Exercise: Blood pressure</h2>
<p>Consider now the blood pressure example and assess the model assumption of the model with dose, gender and their interaction term.</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="#cb350-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>dose<span class="sc">*</span>gender,<span class="at">data=</span>BloodPressure)</span>
<span id="cb350-2"><a href="#cb350-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-3"><a href="#cb350-3" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span><span class="fu">residuals</span>(m)</span>
<span id="cb350-4"><a href="#cb350-4" aria-hidden="true" tabindex="-1"></a>YHat<span class="ot">&lt;-</span><span class="fu">predict</span>(m)</span>
<span id="cb350-5"><a href="#cb350-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-6"><a href="#cb350-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(e)</span>
<span id="cb350-7"><a href="#cb350-7" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(e)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-108-1.png" width="672" /></p>
<p>The residuals appear to be nicely normally distributed.</p>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="#cb351-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">2</span>))</span>
<span id="cb351-2"><a href="#cb351-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BloodPressure<span class="sc">$</span>dose,e, <span class="at">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb351-3"><a href="#cb351-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BloodPressure<span class="sc">$</span>dose,<span class="fu">abs</span>(e), <span class="at">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)</span>
<span id="cb351-4"><a href="#cb351-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb351-5"><a href="#cb351-5" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(e<span class="sc">~</span>BloodPressure<span class="sc">$</span>gender, <span class="at">xlab=</span><span class="st">&quot;gender (0: man; 1: woman)&quot;</span>, </span>
<span id="cb351-6"><a href="#cb351-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb351-7"><a href="#cb351-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BloodPressure<span class="sc">$</span>dose[BloodPressure<span class="sc">$</span>gender<span class="sc">==</span><span class="dv">0</span>],e[BloodPressure<span class="sc">$</span>gender<span class="sc">==</span><span class="dv">0</span>], </span>
<span id="cb351-8"><a href="#cb351-8" aria-hidden="true" tabindex="-1"></a>                                                   <span class="at">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>,</span>
<span id="cb351-9"><a href="#cb351-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;man&quot;</span>)</span>
<span id="cb351-10"><a href="#cb351-10" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb351-11"><a href="#cb351-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BloodPressure<span class="sc">$</span>dose[BloodPressure<span class="sc">$</span>gender<span class="sc">==</span><span class="dv">1</span>],e[BloodPressure<span class="sc">$</span>gender<span class="sc">==</span><span class="dv">1</span>], </span>
<span id="cb351-12"><a href="#cb351-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;dose (mg/day)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>,</span>
<span id="cb351-13"><a href="#cb351-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;woman&quot;</span>)</span>
<span id="cb351-14"><a href="#cb351-14" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb351-15"><a href="#cb351-15" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(YHat,e, <span class="at">xlab=</span><span class="st">&quot;Predicted blood pressure reduction (mmHg)&quot;</span>,</span>
<span id="cb351-16"><a href="#cb351-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb351-17"><a href="#cb351-17" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-109-1.png" width="672" /></p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>The residual plots with dose on the horizontal axis may suggest a small increase of the variance with the dose, which would be a violation of the constant-variance assumption. The same could also be seen from the residual plot with the predicted outcomes on the horizontal axis. However, the increase is only very small and is therefore probably not problematic.</p>
<p>The boxlot shows now deviations from the model assumptions.</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="#cb353-1" aria-hidden="true" tabindex="-1"></a>h<span class="ot">&lt;-</span><span class="fu">influence</span>(m)<span class="sc">$</span>h</span>
<span id="cb353-2"><a href="#cb353-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(h,<span class="at">xlab=</span><span class="st">&quot;i&quot;</span>,<span class="at">ylab=</span><span class="st">&quot;leverage&quot;</span>,<span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb353-3"><a href="#cb353-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">sum</span>(h)<span class="sc">/</span><span class="fu">nrow</span>(BloodPressure),<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-110-1.png" width="672" /></p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="#cb354-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vif</span>(m)</span></code></pre></div>
<pre><code>##        dose      gender dose:gender 
##    1.766162    2.263726    2.933834</code></pre>
<p>No influential outliers are detected and no multicollinearity is indicated.</p>
</details>
</div>
<div id="example-bacterial-count" class="section level2 unnumbered">
<h2>Example (Bacterial count)</h2>
<p>We have data from an experiment in which the antibacterial effect of metalic nanoparticles is investigated. For each of 5 doses, 7 experiments are replicated. For each experiment, the number of bacterial colonies on a plate are counted; this is known as the <em>colony forming units</em> (CFU). The smaller this number, the better the antibacterial activity of the nanoparticles.</p>
<p>We will read the data, make a plot and fit a linear regression model for which we will assess the model assumptions.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/Nano.RData&quot;</span>)</span>
<span id="cb356-2"><a href="#cb356-2" aria-hidden="true" tabindex="-1"></a><span class="fu">skim</span>(Nano)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-111">Table 3.2: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Nano</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">35</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="odd">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<colgroup>
<col width="11%" />
<col width="8%" />
<col width="11%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="4%" />
<col width="5%" />
<col width="4%" />
<col width="5%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">concentration</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.33</td>
<td align="right">0.27</td>
<td align="right">0.05</td>
<td align="right">0.1</td>
<td align="right">0.25</td>
<td align="right">0.5</td>
<td align="right">0.75</td>
<td align="left">&lt;U+2587&gt;&lt;U+2583&gt;&lt;U+2581&gt;&lt;U+2583&gt;&lt;U+2583&gt;</td>
</tr>
<tr class="even">
<td align="left">CFU</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">31.86</td>
<td align="right">14.47</td>
<td align="right">12.00</td>
<td align="right">19.0</td>
<td align="right">33.00</td>
<td align="right">45.5</td>
<td align="right">52.00</td>
<td align="left">&lt;U+2587&gt;&lt;U+2582&gt;&lt;U+2585&gt;&lt;U+2583&gt;&lt;U+2587&gt;</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="#cb357-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration,Nano<span class="sc">$</span>CFU, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;CFU&quot;</span>,</span>
<span id="cb357-2"><a href="#cb357-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb357-3"><a href="#cb357-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb357-4"><a href="#cb357-4" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(CFU<span class="sc">~</span>concentration, <span class="at">data=</span>Nano)</span>
<span id="cb357-5"><a href="#cb357-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ concentration, data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.1054 -2.4675 -0.0708  3.1016  5.2741 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     49.381      0.880   56.12   &lt;2e-16 ***
## concentration  -53.103      2.089  -25.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.236 on 33 degrees of freedom
## Multiple R-squared:  0.9514, Adjusted R-squared:   0.95 
## F-statistic: 646.4 on 1 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="#cb359-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration,Nano<span class="sc">$</span>CFU, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;CFU&quot;</span>,</span>
<span id="cb359-2"><a href="#cb359-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span>
<span id="cb359-3"><a href="#cb359-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">coef</span>(m), <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-111-1.png" width="672" /></p>
<p>The plot with the fitted regression line already shows a deviation from the linearity assumption. This becomes also clear from the residual plot.</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="#cb360-1" aria-hidden="true" tabindex="-1"></a>e<span class="ot">&lt;-</span><span class="fu">residuals</span>(m)</span>
<span id="cb360-2"><a href="#cb360-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb360-3"><a href="#cb360-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb360-4"><a href="#cb360-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration, e, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb360-5"><a href="#cb360-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span>
<span id="cb360-6"><a href="#cb360-6" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(e<span class="sc">~</span>Nano<span class="sc">$</span>concentration, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb360-7"><a href="#cb360-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-112-1.png" width="672" /></p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="#cb361-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>The next plot looks at the constancy of variance assumption.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="#cb362-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(<span class="fu">abs</span>(e)<span class="sc">~</span>Nano<span class="sc">$</span>concentration,  <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)</span>
<span id="cb362-2"><a href="#cb362-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">mean</span>(<span class="fu">abs</span>(e)), <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-113-1.png" width="672" /></p>
<p>This plot does not give a clear picture. It seems to suggest that the variance for the concentration of <span class="math inline">\(0.1\)</span> w/v% is smaller than for the other concentrations. However, we should be careful in interpreting this graph, because we have already discovered a lack-of-fit to the linear regression line. Thus we know that we have larger residuals (in absolute value) for concentrations for which the residuals are not zero on average. The small absolute values of the residuals for the concentration of <span class="math inline">\(0.1\)</span> w/v% is here a consequence of the good fit of the corresponding observations to the regression line (as compared to the poor fit for many other concentrations). Hence, we should first try to improve the model fit, and then assess again the constant-variance assumption.</p>
<p>Next, we look at the normality of the residuals.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(e)</span>
<span id="cb363-2"><a href="#cb363-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(e)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
<p>This seems to suggest some (symmetric) deviation from the normality assumption, but just like before, we should be careful with the interpretatation of this normal QQ plot, because the model does not fit well.</p>
<div id="remedial-measures" class="section level3 unnumbered">
<h3>Remedial measures</h3>
<p>In the previous example (bacterial counts) we discovered a <em>devation from the linearity assumption</em>. Here are a few possible solutions:</p>
<ul>
<li><p><strong>transformation of the regressor</strong> <span class="math inline">\(x\)</span> to e.g. <span class="math inline">\(x^2\)</span>, <span class="math inline">\(\sqrt{x}\)</span>, <span class="math inline">\(\log(x)\)</span>, <span class="math inline">\(\exp(x)\)</span>, <span class="math inline">\(\exp(-x)\)</span>, <span class="math inline">\(1/x\)</span>, <span class="math inline">\(\ldots\)</span>. Note that even nonlinear transformations still result in linear regression models (the model is still linear in the <span class="math inline">\(\beta\)</span>-parameters).</p></li>
<li><p><strong>addition of other regressors</strong>. In our example, however, we only have the concentration. Still we can add a regressor. For example, we could add <span class="math inline">\(x^2\)</span> to the model, resulting in <span class="math inline">\(m(x)=\beta_0+\beta_1 x + \beta_2 x^2\)</span>. Even more terms can be added.</p></li>
<li><p><strong>transformation of the outcome</strong> <span class="math inline">\(Y\)</span>. Although this is very common, it comes with an important issue. Suppose we perform a log-transformation on the outcome, i.e. we formulate the linear regression model as
<span class="math display">\[
 \log Y_i = \beta_0 + \beta_1 x_i + \eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. This model thus implies
<span class="math display">\[
\E{\log(Y) \mid x} = \beta_0 + \beta_1 x.
\]</span>
What is now the interpretation of <span class="math inline">\(\beta_1\)</span>? Let’s see:
<span class="math display">\[
 \beta_1 = \E{\log(Y) \mid x+1} - \E{\log(Y) \mid x} .
\]</span>
Although this interpretation is correct, it is not a simple and convenient interpretation. Some people make the following <strong>error</strong>:
<span class="math display">\[
 \beta_1 = \log\E{Y \mid x+1} - \log\E{Y \mid x} = \log \frac{\E{Y \mid x+1}}{\E{Y \mid x}},
\]</span>
and thus
<span class="math display">\[
 \exp\beta_1 = \frac{\E{Y \mid x+1}}{\E{Y \mid x}}.
\]</span>
This would have been a conventient interpretation, but unfortunately this is wrong. The reason is of course:
<span class="math display">\[
 \E{\log(Y)} \neq \log\E{Y}.
\]</span>
More generally, for a nonlinear transformation <span class="math inline">\(g(\cdot)\)</span>,
<span class="math display">\[
 \E{g(Y)} \neq g\left(\E{Y}\right).
\]</span>
In the GLM course, you will formulate models for <span class="math inline">\(g\left(\E{Y}\right)\)</span> and then the paramaters will get again convenient interpretations.</p></li>
</ul>
</div>
</div>
<div id="example-bacterial-count-1" class="section level2 unnumbered">
<h2>Example (Bacterial count)</h2>
<p>We now try a few transformations, and each time we look at the residuals plots.</p>
<p>First we try adding extra terms (quadratic and cubic effects of the concentration).</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="#cb364-1" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">lm</span>(CFU<span class="sc">~</span>concentration<span class="sc">+</span><span class="fu">I</span>(concentration<span class="sc">^</span><span class="dv">2</span>), <span class="at">data=</span>Nano)</span>
<span id="cb364-2"><a href="#cb364-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ concentration + I(concentration^2), data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3561 -0.8739  0.1261  0.7771  3.1261 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         54.1241     0.6245   86.67  &lt; 2e-16 ***
## concentration      -98.2164     4.4529  -22.06  &lt; 2e-16 ***
## I(concentration^2)  57.1521     5.4932   10.40 8.52e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.57 on 32 degrees of freedom
## Multiple R-squared:  0.9889, Adjusted R-squared:  0.9882 
## F-statistic:  1428 on 2 and 32 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="#cb366-1" aria-hidden="true" tabindex="-1"></a>e1<span class="ot">&lt;-</span>m1<span class="sc">$</span>residuals</span>
<span id="cb366-2"><a href="#cb366-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(e1<span class="sc">~</span>Nano<span class="sc">$</span>concentration, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb366-3"><a href="#cb366-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-115-1.png" width="672" /></p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="#cb367-1" aria-hidden="true" tabindex="-1"></a>m2<span class="ot">&lt;-</span><span class="fu">lm</span>(CFU<span class="sc">~</span>concentration<span class="sc">+</span><span class="fu">I</span>(concentration<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(concentration<span class="sc">^</span><span class="dv">3</span>), <span class="at">data=</span>Nano)</span>
<span id="cb367-2"><a href="#cb367-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ concentration + I(concentration^2) + I(concentration^3), 
##     data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7207 -0.6404  0.2793  0.8579  3.2961 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          55.2872     0.9648  57.305  &lt; 2e-16 ***
## concentration      -117.0779    12.8673  -9.099 2.91e-10 ***
## I(concentration^2)  117.4626    39.0848   3.005  0.00522 ** 
## I(concentration^3)  -50.0683    32.1390  -1.558  0.12942    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.536 on 31 degrees of freedom
## Multiple R-squared:  0.9897, Adjusted R-squared:  0.9887 
## F-statistic:   995 on 3 and 31 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="#cb369-1" aria-hidden="true" tabindex="-1"></a>e2<span class="ot">&lt;-</span>m2<span class="sc">$</span>residuals</span>
<span id="cb369-2"><a href="#cb369-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(e2<span class="sc">~</span>Nano<span class="sc">$</span>concentration, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb369-3"><a href="#cb369-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-115-2.png" width="672" /></p>
<p>These extra terms seem to improve the fit when looking at the residual plots. Let us now also look at the fitted regression lines.</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="#cb370-1" aria-hidden="true" tabindex="-1"></a>concentrations<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="at">by=</span><span class="fl">0.05</span>)</span>
<span id="cb370-2"><a href="#cb370-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb370-3"><a href="#cb370-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb370-4"><a href="#cb370-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration,Nano<span class="sc">$</span>CFU, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;CFU&quot;</span>,</span>
<span id="cb370-5"><a href="#cb370-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>, </span>
<span id="cb370-6"><a href="#cb370-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;model with linear and quadratic terms&quot;</span>)</span>
<span id="cb370-7"><a href="#cb370-7" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(concentrations,<span class="fu">predict</span>(m1,<span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">concentration=</span>concentrations)), <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb370-8"><a href="#cb370-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration,Nano<span class="sc">$</span>CFU, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;CFU&quot;</span>,</span>
<span id="cb370-9"><a href="#cb370-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>,</span>
<span id="cb370-10"><a href="#cb370-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;model with linear, quadratic and cubic terms&quot;</span>)</span>
<span id="cb370-11"><a href="#cb370-11" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(concentrations,<span class="fu">predict</span>(m2,<span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">concentration=</span>concentrations)), <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-116-1.png" width="672" /></p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="#cb371-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>This seems indeed to improve the fit.</p>
<p>Let us try to apply a non-linear transformation to the concentration, e.g. <span class="math inline">\(\exp(-x)\)</span> because we see an exponentially decreasing trend in the data.</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="#cb372-1" aria-hidden="true" tabindex="-1"></a>m3<span class="ot">&lt;-</span><span class="fu">lm</span>(CFU<span class="sc">~</span><span class="fu">I</span>(<span class="fu">exp</span>(<span class="sc">-</span>concentration)), <span class="at">data=</span>Nano)</span>
<span id="cb372-2"><a href="#cb372-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = CFU ~ I(exp(-concentration)), data = Nano)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6755 -1.6025 -0.2068  1.8434  3.8434 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             -26.214      1.550  -16.91   &lt;2e-16 ***
## I(exp(-concentration))   78.184      2.028   38.55   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.164 on 33 degrees of freedom
## Multiple R-squared:  0.9783, Adjusted R-squared:  0.9776 
## F-statistic:  1486 on 1 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="#cb374-1" aria-hidden="true" tabindex="-1"></a>e3<span class="ot">&lt;-</span>m3<span class="sc">$</span>residuals</span>
<span id="cb374-2"><a href="#cb374-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(e3<span class="sc">~</span>Nano<span class="sc">$</span>concentration, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb374-3"><a href="#cb374-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-117-1.png" width="672" /></p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="#cb375-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration,Nano<span class="sc">$</span>CFU, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;CFU&quot;</span>,</span>
<span id="cb375-2"><a href="#cb375-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>,</span>
<span id="cb375-3"><a href="#cb375-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;model with exp(concentration)&quot;</span>)</span>
<span id="cb375-4"><a href="#cb375-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(concentrations,<span class="fu">predict</span>(m3,<span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">concentration=</span>concentrations)), <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-117-2.png" width="672" /></p>
<p>This improves the fit slightly but not as good as before. Next we try a logarithmic transformation to the outcome.</p>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="#cb376-1" aria-hidden="true" tabindex="-1"></a>m4<span class="ot">&lt;-</span><span class="fu">lm</span>(<span class="fu">I</span>(<span class="fu">log</span>(CFU))<span class="sc">~</span>concentration, <span class="at">data=</span>Nano)</span>
<span id="cb376-2"><a href="#cb376-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = I(log(CFU)) ~ concentration, data = Nano)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.111146 -0.020796 -0.000825  0.040423  0.076379 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    3.99315    0.01331  299.92   &lt;2e-16 ***
## concentration -1.98326    0.03160  -62.76   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.04897 on 33 degrees of freedom
## Multiple R-squared:  0.9917, Adjusted R-squared:  0.9914 
## F-statistic:  3939 on 1 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="#cb378-1" aria-hidden="true" tabindex="-1"></a>e4<span class="ot">&lt;-</span>m4<span class="sc">$</span>residuals</span>
<span id="cb378-2"><a href="#cb378-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(e4<span class="sc">~</span>Nano<span class="sc">$</span>concentration, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb378-3"><a href="#cb378-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">col=</span><span class="dv">4</span>,<span class="at">lty=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-118-1.png" width="672" /></p>
<p>This also seems to improve the fit, or at least it improves the fit on the scale of the log-transformed outcome. Let us now plot the data and the fitted regression line. However, the fit was on the scale of the log-transformed outcome and so we will need to backtransform to the original scale.</p>
<p>Based on the fitted model we have
<span class="math display">\[
  \hat{m}^\prime(x) = \hat\beta_0 + \hat\beta_1 x
\]</span>
with <span class="math inline">\(\hat{m}^\prime\)</span> an estimate of <span class="math inline">\(\E{\log(Y) \mid x}\)</span>. It would be tempting to consider <span class="math inline">\(\exp(\hat\beta_0 + \hat\beta_1 x)\)</span> as an estimate of <span class="math inline">\(\E{Y \mid x}\)</span>, but as explained earlier, the nonlinearity of the logarithmic function does not allow us to do so. Let’s do it anyway (which I really <em>do not</em> advise to do – I actually advice never to apply a nonlinear transformation to the outcome).</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="#cb379-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(Nano<span class="sc">$</span>concentration,Nano<span class="sc">$</span>CFU, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;CFU&quot;</span>,</span>
<span id="cb379-2"><a href="#cb379-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>,</span>
<span id="cb379-3"><a href="#cb379-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;model with log-transformed outcome&quot;</span>)</span>
<span id="cb379-4"><a href="#cb379-4" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(concentrations,<span class="fu">exp</span>(<span class="fu">predict</span>(m4,<span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">concentration=</span>concentrations))), <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-119-1.png" width="672" /></p>
<p>We can also look at the residuals on the original scale (after the poor backtransformation).</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="#cb380-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(<span class="fu">predict</span>(m4)<span class="sc">-</span>Nano<span class="sc">$</span>CFU<span class="sc">~</span>Nano<span class="sc">$</span>concentration, <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>,</span>
<span id="cb380-2"><a href="#cb380-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="st">&quot;residuals after backtransformation&quot;</span>)</span>
<span id="cb380-3"><a href="#cb380-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-120-1.png" width="672" /></p>
<p>We will continue will models <em>m2</em> and <em>m4</em> and check the normallity and constant-variance assumptions.</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="#cb381-1" aria-hidden="true" tabindex="-1"></a>e2<span class="ot">&lt;-</span>m2<span class="sc">$</span>residuals</span>
<span id="cb381-2"><a href="#cb381-2" aria-hidden="true" tabindex="-1"></a>e4<span class="ot">&lt;-</span>m4<span class="sc">$</span>residuals</span>
<span id="cb381-3"><a href="#cb381-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb381-4"><a href="#cb381-4" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb381-5"><a href="#cb381-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(e2, <span class="at">main=</span><span class="st">&quot;linear, quadratic and cubic concentration effects&quot;</span>)</span>
<span id="cb381-6"><a href="#cb381-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(e2)</span>
<span id="cb381-7"><a href="#cb381-7" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(e4, <span class="at">main=</span><span class="st">&quot;log-transformed CFU count&quot;</span>)</span>
<span id="cb381-8"><a href="#cb381-8" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(<span class="fu">as.numeric</span>(e4))</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-121-1.png" width="672" /></p>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="#cb382-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="#cb383-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb383-2"><a href="#cb383-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(<span class="fu">abs</span>(e2)<span class="sc">~</span>Nano<span class="sc">$</span>concentration,  <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>,</span>
<span id="cb383-3"><a href="#cb383-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="st">&quot;linear, quadratic and cubic concentration effects&quot;</span>)</span>
<span id="cb383-4"><a href="#cb383-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">mean</span>(<span class="fu">abs</span>(e2)),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span>
<span id="cb383-5"><a href="#cb383-5" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(<span class="fu">abs</span>(e4)<span class="sc">~</span>Nano<span class="sc">$</span>concentration,  <span class="at">xlab=</span><span class="st">&quot;concentration (w/v %)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>,</span>
<span id="cb383-6"><a href="#cb383-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="st">&quot;log-transformed CFU count&quot;</span>)</span>
<span id="cb383-7"><a href="#cb383-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">mean</span>(<span class="fu">abs</span>(e4)),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-122-1.png" width="672" /></p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>None of the two models give a graph that shows a clear systematic pattern. However, the graph for model <em>m2</em> suggests a much smaller variance of the residuals for the largest concentration. This phenomenon cannot be seen for model <em>m4</em>. This may be explained as follows. Count data (as for our bacterial count data example) typically show an increasing variance with the mean outcome (as for a Poisson distribution). This may explain that for model <em>m2</em> (no transformed outcome) that we still see the smaller variances for the larger concentrations (that correspond to the smaller mean CFUs). In model <em>m4</em> the outcome is log-transformed and this typically <em>squeezes</em> the larger values more than the smaller values, and hence it has a variance-stabilising effect on e.g. count data.</p>
<p>Outside the scope of this course: it is possible to not only model the conditinal mean as a function of the regressors, but also the conditional variance <span class="math inline">\(\var{Y \mid x}\)</span> can be modelled. This could allow for variance-heteroscedasticity.</p>
</div>
<div id="model-selection" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Model Selection</h2>
<p>In this section we will briefly look into the problem of model selection. In general terms, model selection is the process of selecting a final set of regressors based on the sample data. Based on this selected set of regressors, the model fit can be used for answering the original research question.</p>
<p>We will not present a single best method, but we will rather briefly touch upon two methods:</p>
<ul>
<li><p>selection methods based on hypothesis testing. This method is still very popular, but we will argue that it should no longer be appied.</p></li>
<li><p>selection methods for building prediction models.</p></li>
</ul>
<div id="selection-methods-based-on-hypothesis-testing" class="section level3 unnumbered">
<h3>Selection methods based on hypothesis testing</h3>
<p>We limit the discussion to additive models.</p>
<p>There are three different strategies:</p>
<ul>
<li><p><strong>forward selection</strong>: The methods starts with a model with only an intercept. In each step one extra regressor can be added. When no extra regressor is selected by the algorithm, the procedure stops.</p></li>
<li><p><strong>backward elimination</strong>: The methods starts with all regressors included as main effects in the model. In each step a regressor can be removed from the model. When no additional regressor is to eliminated by the algorithm, the procedure stops.</p></li>
<li><p><strong>stepwise selection</strong>. The method starts as with forward selection, but after each added regressor the algorithm checks whether one of the previously selected regressors can be removed from the model. The procedure stops when no regressor is to be added or removed.</p></li>
</ul>
<p>All three strategies make use of hypothesis testing in each step:</p>
<ul>
<li><p>a regressor is added to the model it its parameter is significantly different from zero at the <span class="math inline">\(\alpha_\text{in}\)</span> level of significance. When more than one regressor is significant, the regressor with the smallest <span class="math inline">\(p\)</span>-value is selected.</p></li>
<li><p>a regressor is removed from the model if its parameter not significantly different from zero at the <span class="math inline">\(\alpha_\text{out}\)</span> level of significance. When more than on regressor is not significant, the regressor with the largest <span class="math inline">\(p\)</span>-value is removed.</p></li>
</ul>
<p>For the stepwise procedure, <span class="math inline">\(\alpha_\text{in}&lt;\alpha_\text{out}\)</span>.</p>
</div>
</div>
<div id="example-lead-concentration-8" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>We illustrate the stepwise model selection procedure with the lead concentration example. The set of pontential regressors is: <em>Age</em>, <em>Sex</em>, <em>Iqf</em>, <em>Ld72</em>, <em>Ld73</em> and <em>Totyrs</em>.</p>
<p>The stepwise method is now illustrated, with <span class="math inline">\(\alpha_\text{in}=0.05\)</span> and <span class="math inline">\(\alpha_\text{out}=0.10\)</span>.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="#cb385-1" aria-hidden="true" tabindex="-1"></a>scope<span class="ot">&lt;-</span>(<span class="sc">~</span>Age<span class="sc">+</span>Sex<span class="sc">+</span>Iqf<span class="sc">+</span>Ld72<span class="sc">+</span>Ld73<span class="sc">+</span>Totyrs)</span>
<span id="cb385-2"><a href="#cb385-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb385-3"><a href="#cb385-3" aria-hidden="true" tabindex="-1"></a>m0<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span><span class="dv">1</span>, <span class="at">data=</span>lead)</span>
<span id="cb385-4"><a href="#cb385-4" aria-hidden="true" tabindex="-1"></a><span class="fu">add1</span>(m0,<span class="at">scope=</span>scope, <span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ 1
##        Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;              13635.8 425.43                      
## Age     1    5808.5  7827.3 381.36 60.1089 2.299e-11 ***
## Sex     1     235.4 13400.4 425.99  1.4230   0.23640    
## Iqf     1     353.8 13282.0 425.25  2.1579   0.14571    
## Ld72    1     961.7 12674.1 421.36  6.1464   0.01524 *  
## Ld73    1    1586.8 12049.0 417.17 10.6672   0.00160 ** 
## Totyrs  1    1476.0 12159.8 417.93  9.8318   0.00239 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Several regressors show significant effects at the <span class="math inline">\(\alpha_\text{in}=0.05\)</span> level of significance and <em>Age</em> has the smallest p-value and hence this regressor is added to the model.</p>
<p>So will will add <em>Age</em> to the model. Since this is the first regressor added, it makes no sense to test whether it can be removed from the model. So we continue with testing for an extra regressor.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="#cb387-1" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">update</span>(m0,.<span class="sc">~</span>.<span class="sc">+</span>Age)</span>
<span id="cb387-2"><a href="#cb387-2" aria-hidden="true" tabindex="-1"></a><span class="fu">add1</span>(m1,<span class="at">scope=</span>scope,<span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ Age
##        Df Sum of Sq    RSS    AIC F value   Pr(&gt;F)   
## &lt;none&gt;              7827.3 381.36                    
## Sex     1      8.18 7819.1 383.28  0.0837 0.773081   
## Iqf     1    516.54 7310.7 377.70  5.6524 0.019819 * 
## Ld72    1    433.93 7393.4 378.63  4.6954 0.033219 * 
## Ld73    1    686.30 7141.0 375.75  7.6885 0.006912 **
## Totyrs  1     40.53 7786.8 382.93  0.4164 0.520583   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Now <em>Ld73</em> can be added to the model. The new model has now two regressors and so it now makes sense to check whether a regressor can be removed.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="#cb389-1" aria-hidden="true" tabindex="-1"></a>m2<span class="ot">&lt;-</span><span class="fu">update</span>(m1, .<span class="sc">~</span>.<span class="sc">+</span>Ld73)</span>
<span id="cb389-2"><a href="#cb389-2" aria-hidden="true" tabindex="-1"></a><span class="fu">drop1</span>(m2,<span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## MAXFWT ~ Age + Ld73
##        Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;               7141.0 375.75                      
## Age     1    4908.0 12049.0 417.17 54.9844 1.119e-10 ***
## Ld73    1     686.3  7827.3 381.36  7.6885  0.006912 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Both regressors are significant at the <span class="math inline">\(\alpha_\text{out}=0.10\)</span> level of significance and will thus remain in the model. We continu with checking whether an extra regressor can be added.</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="#cb391-1" aria-hidden="true" tabindex="-1"></a><span class="fu">add1</span>(m2,<span class="at">scope=</span>scope,<span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ Age + Ld73
##        Df Sum of Sq    RSS    AIC F value  Pr(&gt;F)  
## &lt;none&gt;              7141.0 375.75                  
## Sex     1      7.95 7133.0 377.65  0.0880 0.76749  
## Iqf     1    414.95 6726.0 372.78  4.8738 0.03017 *
## Ld72    1      4.45 7136.5 377.69  0.0492 0.82502  
## Totyrs  1      2.25 7138.7 377.72  0.0250 0.87489  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The regressor <em>Iqf</em> can be added.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="#cb393-1" aria-hidden="true" tabindex="-1"></a>m3<span class="ot">&lt;-</span><span class="fu">update</span>(m2, .<span class="sc">~</span>.<span class="sc">+</span>Iqf)</span>
<span id="cb393-2"><a href="#cb393-2" aria-hidden="true" tabindex="-1"></a><span class="fu">drop1</span>(m3,<span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Single term deletions
## 
## Model:
## MAXFWT ~ Age + Ld73 + Iqf
##        Df Sum of Sq     RSS    AIC F value    Pr(&gt;F)    
## &lt;none&gt;               6726.0 372.78                      
## Age     1    5080.0 11806.0 417.47 59.6665 2.995e-11 ***
## Ld73    1     584.7  7310.7 377.70  6.8677   0.01052 *  
## Iqf     1     415.0  7141.0 375.75  4.8738   0.03017 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>None of the regressors can be removed. So we will continue with trying to add an other regressor.</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="#cb395-1" aria-hidden="true" tabindex="-1"></a><span class="fu">add1</span>(m3,<span class="at">scope=</span>scope,<span class="at">test=</span><span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Single term additions
## 
## Model:
## MAXFWT ~ Age + Ld73 + Iqf
##        Df Sum of Sq    RSS    AIC F value Pr(&gt;F)
## &lt;none&gt;              6726.0 372.78               
## Sex     1    0.7841 6725.3 374.77  0.0091 0.9243
## Ld72    1   18.0038 6708.0 374.55  0.2093 0.6486
## Totyrs  1    0.8200 6725.2 374.77  0.0095 0.9226</code></pre>
<p>Now no further effects are significant at the <span class="math inline">\(\alpha_\text{in}\)</span> level of significance and hence the procedure stops. The final model is shown next.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Age + Ld73 + Iqf, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -31.725  -3.208   0.347   6.271  17.630 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18.59992    8.58465   2.167   0.0333 *  
## Age          2.65538    0.34376   7.724    3e-11 ***
## Ld73        -0.24433    0.09323  -2.621   0.0105 *  
## Iqf          0.15339    0.06948   2.208   0.0302 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.227 on 79 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.5067, Adjusted R-squared:  0.488 
## F-statistic: 27.05 on 3 and 79 DF,  p-value: 3.881e-12</code></pre>
<p>These hypothesis testing based methods show serious issues and raises questions:</p>
<ul>
<li><p>Why would we do it? If there is a clear research question (e.g. assessing whether there is an effect of the blood lead concentration on the average MAXFWT), there is no reason to do model selection. It makes much more sense to think about the presence of confounders that could be added the model (whether significant of not).</p></li>
<li><p>If the purpose is to build a model for predicting outcomes (e.g. predicting the MAXFWT for a child living near the smelter), then it makes sense to select regressors (<em>predictors</em>) that together make up a good prediction model. However, as we have argued before, significance (<span class="math inline">\(p\)</span>-value) of a regression coefficient has not very much to do with the regressor being important for improving the prediction capabilities of the model. To name two arguments:</p>
<ul>
<li><p>the <span class="math inline">\(\beta\)</span> parameters describe the relation between the regressor and the <em>mean</em> outcome (not individual outcomes)</p></li>
<li><p><span class="math inline">\(p\)</span>-values are sensitive to the sample size.</p></li>
</ul></li>
<li><p>A non-significant result is not a proof that the parameter equals zero.</p></li>
<li><p>The final model is the result of multiple hypothesis tests (later we will discuss the problem of multiple hypothesis testing or multiplicity).</p></li>
<li><p>In case the research goal is to assess the effect of a single regressor (e.g. the blood lead concentration), then we want to analyse the data with a model that gives a relevant interperation to the parameter of that regressor (= <em>target parameter</em>). Recall that the interpretation of an effect parameter is conditional on the other regressors in the model. Thus in each step of the model selection procedure, other regressors are in the model and hence the final interpretation of the target parameter depends on the finally selected model. It would be much better to assure that the target parameter has an interpretation that directly serves the research question.</p></li>
<li><p>At some stage, multicollinearity may exist and affect the <span class="math inline">\(p\)</span>-values and hence the model selection procedure.</p></li>
<li><p>None of the three strategies guarantees that the finally selected model is the best model in whatever sense.</p></li>
</ul>
<div id="model-selection-for-building-a-prediction-model" class="section level3 unnumbered">
<h3>Model selection for building a prediction model</h3>
<p>Suppose now that the goal is to build a model for predicting the MAXFWT score of a child. We limit the search to additive models with the following potential regressors (<strong>predictors</strong> in this context): <em>Age</em>, <em>Sex</em>, <em>Iqf</em>, <em>Ld72</em>, <em>Ld73</em> and <em>Totyrs</em>.</p>
<p>Recall from Section <a href="#S:PI">2.12</a> that a good prediction model should result in a small <strong>mean squared error</strong> or <strong>expected conditional test error</strong>, for a given <span class="math inline">\(\mb{x}\)</span>,
<span class="math display">\[
  \text{Err}(x) = \Ef{\mb{Y} Y^*}{(\hat{Y}(x)-Y^*)^2} .
\]</span>
This can be further averaged over all <span class="math inline">\(\mb{x}\)</span>, resulting in the <strong>expected test error</strong>,
<span class="math display">\[
  \text{Err} = \Ef{\mb{Y} \mb{X} Y^* }{(\hat{Y}(X)-Y^*)^2} 
\]</span>
in which the expectation is over the sample data <span class="math inline">\((\mb{Y},\mb{X})\)</span> and over the to-be-predicted outcomes <span class="math inline">\(Y^*\)</span>. In the context of building prediction models, the sample data is referred to as the <strong>training data</strong>.</p>
<p>A popular method for estimating the expected test error is via the <strong>cross validation</strong> (CV) method. Here we only present the <strong>Leave one out cross validation</strong> (LOOCV) method:</p>
<p>For each sample observation <span class="math inline">\((\mb{x}_i, Y_i)\)</span>:</p>
<ul>
<li><p>fit model based an all sample data except for observation <span class="math inline">\(i\)</span>. Let <span class="math inline">\(\hat{\mb\beta}_{-i}\)</span> denote the parameter estimate</p></li>
<li><p>predict the outcome for the observation <span class="math inline">\(i\)</span> that was left out: <span class="math inline">\(\hat{Y}^*_i=\hat{\mb\beta}_{-i}^t\mb{x}_i\)</span></p></li>
<li><p>compute the prediction error <span class="math inline">\(e^*_i=\hat{Y}^*_i-Y_i\)</span></p></li>
</ul>
<p>An estimate of the expected test error is then given by
<span class="math display">\[
 \text{CV} = \frac{1}{n} \sum_{i=1}^n e^{*2}_i.
\]</span></p>
<p>It can be shown that CV can be computed without the need for refitting <span class="math inline">\(n\)</span> linear models:
<span class="math display">\[
  \text{CV} = \frac{1}{n} \sum_{i=1}^n \left(\frac{\hat{Y}_i - Y_i}{1-h_{ii}}\right)^2. 
\]</span></p>
<p>With this criterion we can build a prediction model with similar strategies as before: forward selection, backward elimination or stepwise selection. In each step a predictor is added or removed based on the CV criterion.</p>
<p>Yet another strategy is <strong>best subset</strong> model selection. This method evaluates all models in a predefined set of models. For example, limitting the scope to additive models, and considering <span class="math inline">\(p-1\)</span> potential predictors, we will have to evaluate <span class="math inline">\(2^{p-1}\)</span> models (i.e. each predictor can be either in or out of the model). For each model the CV criterion is computed and the model with the smallest CV is selected as the final model.</p>
</div>
</div>
<div id="exercise-lead-concentration-3" class="section level2 unnumbered">
<h2>Exercise: Lead concentration</h2>
<p>Apply the forward selection strategy with CV as a selection criterion. What is your selected model, and how well does it perform for predicting new outcomes?</p>
<details>
<summary>
Try to first make the exercise yourself. You can expand this page to see a solution.
</summary>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="#cb399-1" aria-hidden="true" tabindex="-1"></a>m.age<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age, <span class="at">data=</span>lead)</span>
<span id="cb399-2"><a href="#cb399-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.age<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.age)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 99.15088</code></pre>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="#cb401-1" aria-hidden="true" tabindex="-1"></a>m.Ld73<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld73, <span class="at">data=</span>lead)</span>
<span id="cb401-2"><a href="#cb401-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Ld73<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Ld73)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 153.2846</code></pre>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="#cb403-1" aria-hidden="true" tabindex="-1"></a>m.Ld72<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Ld72, <span class="at">data=</span>lead)</span>
<span id="cb403-2"><a href="#cb403-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Ld72<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Ld72)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 159.1378</code></pre>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="#cb405-1" aria-hidden="true" tabindex="-1"></a>m.Totyrs<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Totyrs, <span class="at">data=</span>lead)</span>
<span id="cb405-2"><a href="#cb405-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Totyrs<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Totyrs)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 152.7551</code></pre>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="#cb407-1" aria-hidden="true" tabindex="-1"></a>m.Iqf<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Iqf, <span class="at">data=</span>lead)</span>
<span id="cb407-2"><a href="#cb407-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Iqf<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Iqf)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 169.8563</code></pre>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="#cb409-1" aria-hidden="true" tabindex="-1"></a>m.sex<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Sex, <span class="at">data=</span>lead)</span>
<span id="cb409-2"><a href="#cb409-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.sex<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.sex)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 168.8579</code></pre>
<p>The predictor <em>Age</em> gives the smalles CV criterion and is thus added to the regression model.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="#cb411-1" aria-hidden="true" tabindex="-1"></a>m.Ld73<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73, <span class="at">data=</span>lead)</span>
<span id="cb411-2"><a href="#cb411-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Ld73<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Ld73)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 93.97038</code></pre>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="#cb413-1" aria-hidden="true" tabindex="-1"></a>m.Ld72<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld72, <span class="at">data=</span>lead)</span>
<span id="cb413-2"><a href="#cb413-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Ld72<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Ld72)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 95.38186</code></pre>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="#cb415-1" aria-hidden="true" tabindex="-1"></a>m.Totyrs<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Totyrs, <span class="at">data=</span>lead)</span>
<span id="cb415-2"><a href="#cb415-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Totyrs<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Totyrs)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 99.99147</code></pre>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="#cb417-1" aria-hidden="true" tabindex="-1"></a>m.Iqf<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Iqf, <span class="at">data=</span>lead)</span>
<span id="cb417-2"><a href="#cb417-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Iqf<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Iqf)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 94.07439</code></pre>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="#cb419-1" aria-hidden="true" tabindex="-1"></a>m.sex<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Sex, <span class="at">data=</span>lead)</span>
<span id="cb419-2"><a href="#cb419-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.sex<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.sex)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 101.1353</code></pre>
<p>Now the predictor <em>Ld73</em> gives the smallest CV and is added to the model.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="#cb421-1" aria-hidden="true" tabindex="-1"></a>m.Ld72<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Ld72, <span class="at">data=</span>lead)</span>
<span id="cb421-2"><a href="#cb421-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Ld72<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Ld72)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 96.07107</code></pre>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="#cb423-1" aria-hidden="true" tabindex="-1"></a>m.Totyrs<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Totyrs, <span class="at">data=</span>lead)</span>
<span id="cb423-2"><a href="#cb423-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Totyrs<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Totyrs)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 95.55408</code></pre>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="#cb425-1" aria-hidden="true" tabindex="-1"></a>m.Iqf<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Iqf, <span class="at">data=</span>lead)</span>
<span id="cb425-2"><a href="#cb425-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Iqf<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Iqf)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 90.57014</code></pre>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="#cb427-1" aria-hidden="true" tabindex="-1"></a>m.sex<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Sex, <span class="at">data=</span>lead)</span>
<span id="cb427-2"><a href="#cb427-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.sex<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.sex)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 95.95992</code></pre>
<p>We now add <em>Iqf</em> to the model.</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="#cb429-1" aria-hidden="true" tabindex="-1"></a>m.Ld72<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Iqf<span class="sc">+</span>Ld72, <span class="at">data=</span>lead)</span>
<span id="cb429-2"><a href="#cb429-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Ld72<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Ld72)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 92.72711</code></pre>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="#cb431-1" aria-hidden="true" tabindex="-1"></a>m.Totyrs<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Iqf<span class="sc">+</span>Totyrs, <span class="at">data=</span>lead)</span>
<span id="cb431-2"><a href="#cb431-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.Totyrs<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.Totyrs)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 91.86483</code></pre>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="#cb433-1" aria-hidden="true" tabindex="-1"></a>m.sex<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Age<span class="sc">+</span>Ld73<span class="sc">+</span>Iqf<span class="sc">+</span>Sex, <span class="at">data=</span>lead)</span>
<span id="cb433-2"><a href="#cb433-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>((m.sex<span class="sc">$</span>residuals<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">influence</span>(m.sex)<span class="sc">$</span>h))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 92.5748</code></pre>
<p>None of these CVs is smaller than the previous CV (<span class="math inline">\(90.57\)</span>) and hence the model selection procedure stops here.</p>
<p>The fact that adding more terms to the model would increase the CV, means that adding more terms would result in <em>overfitting</em>.</p>
<p>Based on the finally selected model, we estimate that the <em>expected test error</em> equals <span class="math inline">\(90.57\)</span>, and its square root (in the same units as the MAXFWT outcome) equals <span class="math inline">\(9.5\)</span> taps per 10 seconds.
More elaborate methods for model building are outside the scope of this course. A final note: the reported estimate of the expected test error is actually still over-optimistic, because it is the result of a minimisation procedure. It would be good to have an independent test dataset for computing a final (and unbiased) estimate of the expected test error.</p>
</details>

</div>
</div>
<div id="Ch:DesignCausal" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Design-related Topics and Causal Inference</h1>
<div id="confounding" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Confounding</h2>
<div id="causal-diagrams-and-controling-for-confounders" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Causal diagrams and controling for confounders</h3>
</div>
</div>
<div id="example-lead-concentration-9" class="section level2 unnumbered">
<h2>Example (Lead concentration)</h2>
<p>Recall the lead concentration example. We came to the weird conclusion that the number of years living near the lead smelter has a positive effect on the mean MAXFWT score (whether corrected for blood lead levels or not). When adding age to the additive model we saw that the effect of <em>Totyrs</em> became negative (as intuitively expected) and nearly neglectable. This is an example of <strong>confounding</strong>. Age plays here the role of a <strong>confounder</strong>. To keep things simple, we will ignore <em>Ld73</em>. Here is the regression analysis.</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="#cb435-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(MAXFWT<span class="sc">~</span>Totyrs<span class="sc">+</span>Age, <span class="at">data=</span>lead)</span>
<span id="cb435-2"><a href="#cb435-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MAXFWT ~ Totyrs + Age, data = lead)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -33.908  -3.906   1.649   6.760  17.981 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  24.1249     3.8136   6.326 1.35e-08 ***
## Totyrs       -0.2512     0.3893  -0.645    0.521    
## Age           2.9465     0.4396   6.703 2.63e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.866 on 80 degrees of freedom
##   (19 observations deleted due to missingness)
## Multiple R-squared:  0.4289, Adjusted R-squared:  0.4147 
## F-statistic: 30.05 on 2 and 80 DF,  p-value: 1.849e-10</code></pre>
<p>The confounding effect of age can be explained as follows:</p>
<ul>
<li><p>the outcome variable (MAXFWT) is affected by age (the older the child the better its motor functions)</p></li>
<li><p>the regressor <em>Totyrs</em> is also affected by age (the older the child, the larger the probability that it has lived for a longer time near the lead smelter)</p></li>
</ul>
<p>The possitive correlations between MAXFWT and age, and between <em>Totyrs</em> and age can be seen from the next analysis.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="#cb437-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead[,<span class="fu">c</span>(<span class="st">&quot;MAXFWT&quot;</span>,<span class="st">&quot;Age&quot;</span>,<span class="st">&quot;Totyrs&quot;</span>)], <span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>##           MAXFWT       Age    Totyrs
## MAXFWT 1.0000000 0.6526679 0.3290015
## Age    0.6526679 1.0000000 0.5725720
## Totyrs 0.3290015 0.5725720 1.0000000</code></pre>
<p>These conceptual relationships (i.e. derived from reasoning and prior knowledge and <em>not</em> based on the data) can be visualised in a <strong>causal graph</strong>; see Figure <a href="#fig:CausalLead">4.1</a>.
The causal graph shows the directions of the potential causal relationships. Note that these directions are not inferred from the data, but they are based on prior knowledge. It is up to the data analyses to estimate the strength of the effects (effect sizes).</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="#cb439-1" aria-hidden="true" tabindex="-1"></a>g1 <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag {</span></span>
<span id="cb439-2"><a href="#cb439-2" aria-hidden="true" tabindex="-1"></a><span class="st">    Totyrs [pos=&quot;0,1&quot;]</span></span>
<span id="cb439-3"><a href="#cb439-3" aria-hidden="true" tabindex="-1"></a><span class="st">    MAXFWT [pos=&quot;2,1&quot;]</span></span>
<span id="cb439-4"><a href="#cb439-4" aria-hidden="true" tabindex="-1"></a><span class="st">    Age [pos=&quot;1,0&quot;]</span></span>
<span id="cb439-5"><a href="#cb439-5" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb439-6"><a href="#cb439-6" aria-hidden="true" tabindex="-1"></a><span class="st">    Totyrs &lt;- Age -&gt; MAXFWT</span></span>
<span id="cb439-7"><a href="#cb439-7" aria-hidden="true" tabindex="-1"></a><span class="st">}&#39;</span>)</span>
<span id="cb439-8"><a href="#cb439-8" aria-hidden="true" tabindex="-1"></a>g2 <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag {</span></span>
<span id="cb439-9"><a href="#cb439-9" aria-hidden="true" tabindex="-1"></a><span class="st">    Totyrs [pos=&quot;0,1&quot;]</span></span>
<span id="cb439-10"><a href="#cb439-10" aria-hidden="true" tabindex="-1"></a><span class="st">    MAXFWT [pos=&quot;2,1&quot;]</span></span>
<span id="cb439-11"><a href="#cb439-11" aria-hidden="true" tabindex="-1"></a><span class="st">    Age [pos=&quot;1,0&quot;]</span></span>
<span id="cb439-12"><a href="#cb439-12" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb439-13"><a href="#cb439-13" aria-hidden="true" tabindex="-1"></a><span class="st">    Totyrs -&gt; MAXFWT </span></span>
<span id="cb439-14"><a href="#cb439-14" aria-hidden="true" tabindex="-1"></a><span class="st">    Totyrs &lt;- Age -&gt; MAXFWT</span></span>
<span id="cb439-15"><a href="#cb439-15" aria-hidden="true" tabindex="-1"></a><span class="st">}&#39;</span>)</span>
<span id="cb439-16"><a href="#cb439-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb439-17"><a href="#cb439-17" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb439-18"><a href="#cb439-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(g1)</span>
<span id="cb439-19"><a href="#cb439-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(g2)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CausalLead"></span>
<img src="DASM2_files/figure-html/CausalLead-1.png" alt="Two causal graphs or diagrams of the Lead concentration example. Age is a confounder. Left: Totyrs has no direct causal effect on MAXFWT. Right: Totyrs has a direct causal effect on MAXFWT." width="672" />
<p class="caption">
Figure 4.1: Two causal graphs or diagrams of the Lead concentration example. Age is a confounder. Left: Totyrs has no direct causal effect on MAXFWT. Right: Totyrs has a direct causal effect on MAXFWT.
</p>
</div>
<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb440-1"><a href="#cb440-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>An edge (arrow) can be interpreted as follows (here in terms of the example):</p>
<ul>
<li><p>if age is increased, the variable <em>Totyrs</em> tends to increase (or decrease). Note that this is (not necessarily) a deterministic effect; there is room for random variation.</p></li>
<li><p>similarly, if age is increased, the MAXFWT score tends to increase (or decrease)</p></li>
<li><p>the interpretation of the edge between <em>Totyrs</em> and MAXFWT needs to account for age, because there is an edge from age arriving into <em>Totyrs</em> and into MAXFWT:</p>
<ul>
<li><p>when there is no edge between <em>Totyrs</em> and MAXFWT, then the causal graph tells us that <em>Totyrs</em> and MAXFWT are <strong>conditionally indepdent</strong>, i.e. given age, there is no dependence between <em>Totyrs</em> and MAXFWT.</p></li>
<li><p>when there is an edge between <em>Totyrs</em> and MAXFWT, then, even when conditioning on age, there still is a direct effect from <em>Totyrs</em> on MAXFWT</p></li>
</ul></li>
</ul>
<p>The next chunck of R code illustrates the conditional (in)dependence for the lead concentration example. Since age is a continuous regressor, it is hard to illustrate the conditoning exactly, because for every different value of age we have at most a few observations. Therefore, we will group the subjects into 4 equally large age categories, and we will then condition on the age category.</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="#cb441-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lead<span class="sc">$</span>Age)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   3.750   6.375   8.667   9.054  12.062  15.917</code></pre>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="#cb443-1" aria-hidden="true" tabindex="-1"></a>age.quartiles<span class="ot">&lt;-</span><span class="fu">quantile</span>(lead<span class="sc">$</span>Age[<span class="sc">!</span><span class="fu">is.na</span>(lead<span class="sc">$</span>MAXFWT)])</span>
<span id="cb443-2"><a href="#cb443-2" aria-hidden="true" tabindex="-1"></a>Ind1<span class="ot">&lt;-</span><span class="fu">between</span>(lead<span class="sc">$</span>Age,age.quartiles[<span class="dv">1</span>],age.quartiles[<span class="dv">2</span>])</span>
<span id="cb443-3"><a href="#cb443-3" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead<span class="sc">$</span>Totyrs[Ind1],lead<span class="sc">$</span>MAXFWT[Ind1],<span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] -0.04566625</code></pre>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="#cb445-1" aria-hidden="true" tabindex="-1"></a>Ind2<span class="ot">&lt;-</span><span class="fu">between</span>(lead<span class="sc">$</span>Age,age.quartiles[<span class="dv">2</span>],age.quartiles[<span class="dv">3</span>])</span>
<span id="cb445-2"><a href="#cb445-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead<span class="sc">$</span>Totyrs[Ind2],lead<span class="sc">$</span>MAXFWT[Ind2],<span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.06312945</code></pre>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="#cb447-1" aria-hidden="true" tabindex="-1"></a>Ind3<span class="ot">&lt;-</span><span class="fu">between</span>(lead<span class="sc">$</span>Age,age.quartiles[<span class="dv">3</span>],age.quartiles[<span class="dv">4</span>])</span>
<span id="cb447-2"><a href="#cb447-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead<span class="sc">$</span>Totyrs[Ind3],lead<span class="sc">$</span>MAXFWT[Ind3],<span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] -0.09796866</code></pre>
<div class="sourceCode" id="cb449"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb449-1"><a href="#cb449-1" aria-hidden="true" tabindex="-1"></a>Ind4<span class="ot">&lt;-</span><span class="fu">between</span>(lead<span class="sc">$</span>Age,age.quartiles[<span class="dv">4</span>],age.quartiles[<span class="dv">5</span>])</span>
<span id="cb449-2"><a href="#cb449-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead<span class="sc">$</span>Totyrs[Ind4],lead<span class="sc">$</span>MAXFWT[Ind4],<span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] -0.06886756</code></pre>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="#cb451-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb451-2"><a href="#cb451-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[Ind1],lead<span class="sc">$</span>MAXFWT[Ind1],</span>
<span id="cb451-3"><a href="#cb451-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;age in first quartile&quot;</span>, </span>
<span id="cb451-4"><a href="#cb451-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)</span>
<span id="cb451-5"><a href="#cb451-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[Ind2],lead<span class="sc">$</span>MAXFWT[Ind2],</span>
<span id="cb451-6"><a href="#cb451-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;age in second quartile&quot;</span>, </span>
<span id="cb451-7"><a href="#cb451-7" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)</span>
<span id="cb451-8"><a href="#cb451-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[Ind3],lead<span class="sc">$</span>MAXFWT[Ind3],</span>
<span id="cb451-9"><a href="#cb451-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;age in third quartile&quot;</span>, </span>
<span id="cb451-10"><a href="#cb451-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)</span>
<span id="cb451-11"><a href="#cb451-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lead<span class="sc">$</span>Totyrs[Ind4],lead<span class="sc">$</span>MAXFWT[Ind4],</span>
<span id="cb451-12"><a href="#cb451-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;age in fourth quartile&quot;</span>, </span>
<span id="cb451-13"><a href="#cb451-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;number of year near smelter&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;MAXFWT (taps per 10 second)&quot;</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-136-1.png" width="672" /></p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="#cb452-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>This analysis illustrates that within age group (i.e. conditional on age), the correlation between <em>Totyrs</em> and MAXFWT is nearly zero.</p>
<p>Let us now look at the marginal correlation between <em>Totyrs</em> and MAXFWT.</p>
<div class="sourceCode" id="cb453"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb453-1"><a href="#cb453-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(lead<span class="sc">$</span>Totyrs,lead<span class="sc">$</span>MAXFWT,<span class="at">use=</span><span class="st">&quot;complete.obs&quot;</span>)</span></code></pre></div>
<pre><code>## [1] 0.3290015</code></pre>
<p>Thus marginally there is a (weak) possitive correlation.</p>
<p>If we consider age again as a continuous regressor, we also have a solution for looking at the conditional (in)dependence between <em>Totyrs</em> and MAXFWT: the additive linear regression model with MAXFWT as outcome, and <em>Totyrs</em> and age as regressors. In this model, the regression parameter for the main effect of <em>Totyrs</em> is interpreted as the effect of <em>Totyrs</em> on the mean MAXFWT score, conditional on age (i.e. given age).</p>
</div>
<div id="example-diabetes" class="section level2 unnumbered">
<h2>Example (Diabetes)</h2>
<p>This is an example of the causal graph in the left panel of Figure <a href="#fig:CausalLead">4.1</a>.</p>
<p>Consider a placebo-controlled clinical study that aims to test a new treatment for diabetes 2 patients. The drug aims to reduce the blood glucose levels. It is, however, not a completely randomised study. The clinician decided which patient to give the new drug and which patient to give placebo. In particular,</p>
<ul>
<li><p>patients with a poor general condition were more likely to get the new drug</p></li>
<li><p>patients with a good general condition were more likely to get the placebo</p></li>
</ul>
<p>The outcome of interest is the blood glucose level reduction after one week of treatment.</p>
<p>The next chunck of R code shows a data exploration and the data analysis with regression models. Here is the meaning of the variable names in the dataset:</p>
<ul>
<li><p><em>condition</em>: poor (1) or good (0) general condition</p></li>
<li><p><em>treatment</em>: placebo (0) or new drug (1)</p></li>
<li><p><em>glucose</em>: blood glucose level reduction (mmol / 100 ml)</p></li>
</ul>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="#cb455-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/Diabetes.RData&quot;</span>)</span>
<span id="cb455-2"><a href="#cb455-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(Diabetes<span class="sc">$</span>condition,Diabetes<span class="sc">$</span>treatment)</span></code></pre></div>
<pre><code>##    
##      0  1
##   0 45 12
##   1  3 26</code></pre>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb457-1"><a href="#cb457-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb457-2"><a href="#cb457-2" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(Diabetes<span class="sc">$</span>glucose<span class="sc">~</span>Diabetes<span class="sc">$</span>treatment, <span class="at">main=</span><span class="st">&quot;all data&quot;</span>,</span>
<span id="cb457-3"><a href="#cb457-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, </span>
<span id="cb457-4"><a href="#cb457-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb457-5"><a href="#cb457-5" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(Diabetes<span class="sc">$</span>glucose<span class="sc">~</span>Diabetes<span class="sc">$</span>condition, <span class="at">main=</span><span class="st">&quot;all data&quot;</span>,</span>
<span id="cb457-6"><a href="#cb457-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;condition&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, </span>
<span id="cb457-7"><a href="#cb457-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb457-8"><a href="#cb457-8" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(Diabetes<span class="sc">$</span>glucose[Diabetes<span class="sc">$</span>condition<span class="sc">==</span><span class="dv">0</span>]<span class="sc">~</span>Diabetes<span class="sc">$</span>treatment[Diabetes<span class="sc">$</span>condition<span class="sc">==</span><span class="dv">0</span>],</span>
<span id="cb457-9"><a href="#cb457-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="st">&quot;condition = 0&quot;</span>,</span>
<span id="cb457-10"><a href="#cb457-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, </span>
<span id="cb457-11"><a href="#cb457-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb457-12"><a href="#cb457-12" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(Diabetes<span class="sc">$</span>glucose[Diabetes<span class="sc">$</span>condition<span class="sc">==</span><span class="dv">1</span>]<span class="sc">~</span>Diabetes<span class="sc">$</span>treatment[Diabetes<span class="sc">$</span>condition<span class="sc">==</span><span class="dv">1</span>],</span>
<span id="cb457-13"><a href="#cb457-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">main=</span><span class="st">&quot;condition = 1&quot;</span>,</span>
<span id="cb457-14"><a href="#cb457-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;reduction of blood glucode (mmol / 100ml)&quot;</span>, </span>
<span id="cb457-15"><a href="#cb457-15" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-138-1.png" width="672" /></p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="#cb458-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb458-2"><a href="#cb458-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb458-3"><a href="#cb458-3" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>treatment,<span class="at">data=</span>Diabetes)</span>
<span id="cb458-4"><a href="#cb458-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glucose ~ treatment, data = Diabetes)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.0105 -0.6105 -0.1917  0.7895  2.6083 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.1917     0.1309   9.102 3.70e-14 ***
## treatment     0.9189     0.1970   4.665 1.15e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9071 on 84 degrees of freedom
## Multiple R-squared:  0.2058, Adjusted R-squared:  0.1963 
## F-statistic: 21.76 on 1 and 84 DF,  p-value: 1.152e-05</code></pre>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="#cb460-1" aria-hidden="true" tabindex="-1"></a>m2<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>treatment<span class="sc">+</span>condition,<span class="at">data=</span>Diabetes)</span>
<span id="cb460-2"><a href="#cb460-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glucose ~ treatment + condition, data = Diabetes)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.09902 -0.66999 -0.09499  0.60501  1.30501 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.09499    0.10445  10.484  &lt; 2e-16 ***
## treatment   -0.04285    0.20575  -0.208    0.836    
## condition    1.54688    0.21613   7.157 3.01e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7176 on 83 degrees of freedom
## Multiple R-squared:  0.5089, Adjusted R-squared:  0.4971 
## F-statistic:    43 on 2 and 83 DF,  p-value: 1.527e-13</code></pre>
<p>If we ignore the effect of <em>condition</em>, then we look at the first boxplot and the results of model <em>m1</em>. These results tell us that we have a strong positive effect of the new drug. It is estimated that the blood glucose levels reduce with on average an additional <span class="math inline">\(0.92\)</span> mmol / 100ml (SE <span class="math inline">\(0.20\)</span> mmol / 100 ml) as compared to placebo. This effect is significant at the 5% level of significance (<span class="math inline">\(p&lt;0.0001\)</span>).</p>
<p>However, when we account for <em>condition</em> (i.e. condition on condition) then the effect of treatment vanishes. It is now estimated as <span class="math inline">\(-0.04\)</span> mmol / 100ml (SE <span class="math inline">\(0.21\)</span> mmol / 100ml) and this is no longer significant at the 5% level of significance (<span class="math inline">\(p=0.863\)</span>). So there is no evidence at all for a treatment effect, within the condition groups. This can also be seen from the two boxplots for condition=0 and condition=1.</p>
<p>The variable <em>condition</em> acts as a confounder. The condition of the patient had an effect on the probability of receiving treatment, and the condition also affected the outcome of the patient, because the poor condition was a consequence of the severity of the diabetes (i.e. condition=1 patients were mostly patients with a large blood glucose level before the start of the treatment) and patients starting off with high blood glucose levels had more chance for showing a large reduction in the blood levels. Thus, <em>condition</em> is a confounder. However, within the <em>condition</em> groups there was no effect of treatment. There is thus no direct arrow from <em>treatment</em> to <em>glucose</em> in the causal diagram.</p>
<p>Suppose now that a similar study is set up, but this the patients are properly randomised (thus without first looking at their general condition). In this case the condition does not affect the treatment allocation, and hence in the causal diagram there is no arrow from <em>condition</em> into <em>treatment</em>. If this edge is missing, <em>condition</em> is no longer a confounder and there is no need for including <em>condition</em> into the model. Would you still be allowed to include it in the model? The answer to this question is given later in this chapter.</p>
<div id="causality-and-confounders" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Causality and confounders</h3>
<p>In the previous two examples, we identified a confounder and we argued that the model should include the confounder as a regressor.
Let <span class="math inline">\(Y\)</span> denote the outcome (as usual), <span class="math inline">\(X\)</span> the target regressor (i.e. we aim at estimating the causal effect of <span class="math inline">\(X\)</span>), and let <span class="math inline">\(Z\)</span> denote the confounder variable (it may also be a vector).
Without going into the more techical aspects of causality, as we did in Section <a href="#S:Causality">2.10</a>, we note here that a <strong>causal effect</strong> can be unbiasedly estimated if</p>
<ul>
<li><p>the confounder is included in the model</p></li>
<li><p>the linear model <span class="math inline">\(m(\mb{x})\)</span> is correctly specified and all other required model assumptions hold true</p></li>
<li><p><strong>consistency</strong> and <strong>mean exchangeability</strong>, as in Section <a href="#S:Causality">2.10</a>, still hold, but the latter must be replaced by <strong>conditional mean exchangeability</strong>,
<span class="math display">\[
\E{Y(a) \mid  X=0, Z=z} = \E{Y(a) \mid  X=1, Z=z} = \E{Y(a) \mid Z=z}
\]</span>
for <span class="math inline">\(a=0,1\)</span> and for all confounder outcomes <span class="math inline">\(z\)</span>. (this property is sometimes also referred to as <strong>ignorability</strong>)</p></li>
</ul>
<p>This all sounds very promissing, but there is a very important implication: what if not all confounders are observed or known? Confounders that are not in the dataset are known as <strong>unmeasured confounders</strong>. So we may only be sure that we can estimate a causal effect, if we are very confident that we have observed all confounders!</p>
</div>
<div id="colliders" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Colliders</h3>
</div>
</div>
<div id="example-study-time" class="section level2 unnumbered">
<h2>Example (Study time)</h2>
<p>Researchers suspect that the weekly number of hours a 15 year old child spends on working for school at home, is related to the level of the highest educational degree of its mother. They have a random sample of 56 fifteen year old school kids in England. The dataset has the following variables:</p>
<ul>
<li><p><em>study.time</em>: weekly number of hours of working for school at home</p></li>
<li><p><em>mother</em>: highest educational degree of the mother (1: secondary school; 2: bachelor; 3: master; 4: PhD)</p></li>
<li><p><em>school.result</em>: latest school result (as %)</p></li>
</ul>
<p>How should we analyse this dataset? Do we want to account for the school result? Let us explore the dataset and try some regression analyses.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="#cb462-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/School.RData&quot;</span>)</span>
<span id="cb462-2"><a href="#cb462-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb462-3"><a href="#cb462-3" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb462-4"><a href="#cb462-4" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(School<span class="sc">$</span>study.time<span class="sc">~</span>School<span class="sc">$</span>mother, <span class="at">main=</span><span class="st">&quot;all data&quot;</span>,</span>
<span id="cb462-5"><a href="#cb462-5" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;highest educational degree of mother&quot;</span>, </span>
<span id="cb462-6"><a href="#cb462-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;weekly study time (hours)&quot;</span>, </span>
<span id="cb462-7"><a href="#cb462-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb462-8"><a href="#cb462-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(School<span class="sc">$</span>study.time,School<span class="sc">$</span>school.result, <span class="at">main=</span><span class="st">&quot;all data&quot;</span>,</span>
<span id="cb462-9"><a href="#cb462-9" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;School result&quot;</span>, </span>
<span id="cb462-10"><a href="#cb462-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;weekly study time (hours)&quot;</span>, </span>
<span id="cb462-11"><a href="#cb462-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb462-12"><a href="#cb462-12" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(School<span class="sc">$</span>school.result<span class="sc">~</span>School<span class="sc">$</span>mother, <span class="at">main=</span><span class="st">&quot;all data&quot;</span>,</span>
<span id="cb462-13"><a href="#cb462-13" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;School result&quot;</span>, </span>
<span id="cb462-14"><a href="#cb462-14" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;highest educational degree of mother&quot;</span>, </span>
<span id="cb462-15"><a href="#cb462-15" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.3</span>,<span class="at">cex.lab=</span><span class="fl">1.3</span>)</span>
<span id="cb462-16"><a href="#cb462-16" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-139-1.png" width="672" /></p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="#cb463-1" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">lm</span>(study.time<span class="sc">~</span>mother,<span class="at">data=</span>School)</span>
<span id="cb463-2"><a href="#cb463-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = study.time ~ mother, data = School)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3436 -1.2936  0.0247  1.3089  3.5930 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.18872    0.69998  14.556   &lt;2e-16 ***
## mother      -0.08172    0.27569  -0.296    0.768    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.852 on 54 degrees of freedom
## Multiple R-squared:  0.001624,   Adjusted R-squared:  -0.01686 
## F-statistic: 0.08786 on 1 and 54 DF,  p-value: 0.7681</code></pre>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="#cb465-1" aria-hidden="true" tabindex="-1"></a>m2<span class="ot">&lt;-</span><span class="fu">lm</span>(study.time<span class="sc">~</span>mother<span class="sc">+</span>school.result,<span class="at">data=</span>School)</span>
<span id="cb465-2"><a href="#cb465-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = study.time ~ mother + school.result, data = School)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8066 -0.9524  0.1709  0.9350  2.8406 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    0.16846    1.64454   0.102  0.91880    
## mother        -0.72922    0.23150  -3.150  0.00268 ** 
## school.result  0.17597    0.02734   6.436 3.64e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.401 on 53 degrees of freedom
## Multiple R-squared:  0.4396, Adjusted R-squared:  0.4184 
## F-statistic: 20.79 on 2 and 53 DF,  p-value: 2.167e-07</code></pre>
<p>When not controlling for the school results, there seems to be no evidence of an effect of the educational level of the mother on the average weekly study time of the children. However, when controlling for school results, the effect of the educational level of the mother becomes important and statistically significant at the 5% level of significance (<span class="math inline">\(p=0.003\)</span>): among children with the same school results, we estimate that for each one step increase in the educational level of the mother, the children spend on average about <span class="math inline">\(0.73*60 \approx\)</span> 45 minutes per week less time working for school.</p>
<p>Which of the two regression models gives us the best estimate for the caual effect of the educational level of the mother? This time the answer is that we <em>may not</em> control for the school result, because it is a <em>collider</em> instead of a <em>confounder</em>. It is not the school result that affects the study time or the mother’s educational level, it is rather the opposite. This is also illustrated in the causal diagram in Figure <a href="#fig:CausalCollider">4.2</a>.</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="#cb467-1" aria-hidden="true" tabindex="-1"></a>g3 <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag {</span></span>
<span id="cb467-2"><a href="#cb467-2" aria-hidden="true" tabindex="-1"></a><span class="st">    mother [pos=&quot;0,1&quot;]</span></span>
<span id="cb467-3"><a href="#cb467-3" aria-hidden="true" tabindex="-1"></a><span class="st">    study_time [pos=&quot;2,1&quot;]</span></span>
<span id="cb467-4"><a href="#cb467-4" aria-hidden="true" tabindex="-1"></a><span class="st">    school_result [pos=&quot;1,0&quot;]</span></span>
<span id="cb467-5"><a href="#cb467-5" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb467-6"><a href="#cb467-6" aria-hidden="true" tabindex="-1"></a><span class="st">    mother -&gt; study_time</span></span>
<span id="cb467-7"><a href="#cb467-7" aria-hidden="true" tabindex="-1"></a><span class="st">    mother -&gt; school_result &lt;- study_time</span></span>
<span id="cb467-8"><a href="#cb467-8" aria-hidden="true" tabindex="-1"></a><span class="st">}&#39;</span>)</span>
<span id="cb467-9"><a href="#cb467-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb467-10"><a href="#cb467-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(g3)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CausalCollider"></span>
<img src="DASM2_files/figure-html/CausalCollider-1.png" alt="A causal diagram for the Study Time example. The school result acts a a collider." width="672" />
<p class="caption">
Figure 4.2: A causal diagram for the Study Time example. The school result acts a a collider.
</p>
</div>
<p>Controlling for a collider may introduce correlation between the outcome and the other regressor, which may thus result in the false conclusion that there is a causal effect of the mother’s educational level on the expected study time.</p>
<p>This is illustrated in the following simulation study. We start from the observed data on the mother’s eductional level, and for the parameters of the data generating model we make use of the following fitted regression model.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="#cb468-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(school.result<span class="sc">~</span>mother<span class="sc">+</span>study.time, <span class="at">data=</span>School)</span>
<span id="cb468-2"><a href="#cb468-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = school.result ~ mother + study.time, data = School)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -11.099  -3.809   0.264   2.822  11.296 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  31.5438     4.4209   7.135 2.73e-09 ***
## mother        3.8833     0.7854   4.945 8.07e-06 ***
## study.time    2.4928     0.3873   6.436 3.64e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.271 on 53 degrees of freedom
## Multiple R-squared:  0.5447, Adjusted R-squared:  0.5275 
## F-statistic:  31.7 on 2 and 53 DF,  p-value: 8.814e-10</code></pre>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="#cb470-1" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">mother=</span>School<span class="sc">$</span>mother)</span>
<span id="cb470-2"><a href="#cb470-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-3"><a href="#cb470-3" aria-hidden="true" tabindex="-1"></a>betas<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb470-4"><a href="#cb470-4" aria-hidden="true" tabindex="-1"></a>pvalues<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb470-5"><a href="#cb470-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-6"><a href="#cb470-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N1000) {</span>
<span id="cb470-7"><a href="#cb470-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># For the study time we permute the observed study times.</span></span>
<span id="cb470-8"><a href="#cb470-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#   This will assure that mother and study time are independent</span></span>
<span id="cb470-9"><a href="#cb470-9" aria-hidden="true" tabindex="-1"></a>  db<span class="sc">$</span>study.time<span class="ot">&lt;-</span><span class="fu">sample</span>(School<span class="sc">$</span>study.time,<span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb470-10"><a href="#cb470-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb470-11"><a href="#cb470-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Next we construct a collider (parameters from fitted model on real data)</span></span>
<span id="cb470-12"><a href="#cb470-12" aria-hidden="true" tabindex="-1"></a>  db<span class="sc">$</span>school.result<span class="ot">&lt;-</span><span class="fu">round</span>(<span class="dv">32</span><span class="sc">+</span><span class="dv">4</span><span class="sc">*</span>db<span class="sc">$</span>mother<span class="fl">+2.5</span><span class="sc">*</span>db<span class="sc">$</span>study.time,<span class="dv">0</span>)<span class="sc">+</span><span class="fu">rnorm</span>(<span class="dv">56</span>,<span class="at">mean=</span><span class="dv">0</span>,<span class="at">sd=</span><span class="dv">5</span>)</span>
<span id="cb470-13"><a href="#cb470-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb470-14"><a href="#cb470-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># now we fit the model that includes the collider</span></span>
<span id="cb470-15"><a href="#cb470-15" aria-hidden="true" tabindex="-1"></a>  m<span class="ot">&lt;-</span><span class="fu">lm</span>(study.time<span class="sc">~</span>mother<span class="sc">+</span>school.result, <span class="at">data=</span>db)</span>
<span id="cb470-16"><a href="#cb470-16" aria-hidden="true" tabindex="-1"></a>  betas<span class="ot">&lt;-</span><span class="fu">c</span>(betas,<span class="fu">coef</span>(m)[<span class="dv">2</span>]) <span class="co"># parameter estimate for effect of mother</span></span>
<span id="cb470-17"><a href="#cb470-17" aria-hidden="true" tabindex="-1"></a>  pvalues<span class="ot">&lt;-</span><span class="fu">c</span>(pvalues,<span class="fu">summary</span>(m)<span class="sc">$</span>coef[<span class="dv">2</span>,<span class="dv">4</span>])</span>
<span id="cb470-18"><a href="#cb470-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb470-19"><a href="#cb470-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb470-20"><a href="#cb470-20" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(betas, <span class="at">main=</span><span class="st">&quot;parameter estimates of effect of mother controlled for school result&quot;</span>)</span>
<span id="cb470-21"><a href="#cb470-21" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="fu">mean</span>(betas), <span class="at">col=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-141-1.png" width="672" /></p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="#cb471-1" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(pvalues, <span class="at">xlim=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),</span>
<span id="cb471-2"><a href="#cb471-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Two-sided p-values of effect of mother controlled for school result&quot;</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-141-2.png" width="672" /></p>
<p>These results very clearly illustrate that the causal effect cannot be estimated unbiasedly when the model controls for a collider. This results in spurious correlation between the outcome and the regressor of interest.</p>
</div>
<div id="collapsibility" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Collapsibility</h2>
<p>Another related, but different concept is <strong>collapsibility</strong>. We will see that linear models are often <strong>collapsible</strong> for the target effect (i.e. the effect of interest). In later courses (e.g. GLM) you will come accross models that do not have this collapsibility proporty and that will cause some interpration issues.</p>
</div>
<div id="example-blood-pressure-5" class="section level2 unnumbered">
<h2>Example (Blood pressure)</h2>
<p>Recall the blood pressure example, but this time we will simplify the problem by looking only at two doses: dose=0 (placebo) and the 2mg/day dose (active treatment group). These groups are coded in the dataset as <em>treatment=0</em> and <em>treatment=1</em>, respectively.
The research question is to test whether the treatment has an effect on the average blood pressure reduction. We also have the age of the patients.
Recall that this is a randomised study: 20 patients are randomised over the two treatment groups.</p>
<p>Let us think about a causal diagram for this study. Since it is a randomised study, the age cannot affect the treatment, and obviously the treatment cannot affect the age. However, it may be possible that the age affects how a patient responds to treatment, i.e. age may affect the blood pressure reduction (not the other way around). This is visualised in Figure <a href="#fig:CausalBP2">4.3</a>.</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="#cb472-1" aria-hidden="true" tabindex="-1"></a>g4 <span class="ot">&lt;-</span> <span class="fu">dagitty</span>(<span class="st">&#39;dag {</span></span>
<span id="cb472-2"><a href="#cb472-2" aria-hidden="true" tabindex="-1"></a><span class="st">    treatment [pos=&quot;0,1&quot;]</span></span>
<span id="cb472-3"><a href="#cb472-3" aria-hidden="true" tabindex="-1"></a><span class="st">    bp.reduction [pos=&quot;2,1&quot;]</span></span>
<span id="cb472-4"><a href="#cb472-4" aria-hidden="true" tabindex="-1"></a><span class="st">    age [pos=&quot;1,0&quot;]</span></span>
<span id="cb472-5"><a href="#cb472-5" aria-hidden="true" tabindex="-1"></a><span class="st">    </span></span>
<span id="cb472-6"><a href="#cb472-6" aria-hidden="true" tabindex="-1"></a><span class="st">    treatment -&gt; bp.reduction &lt;- age</span></span>
<span id="cb472-7"><a href="#cb472-7" aria-hidden="true" tabindex="-1"></a><span class="st">}&#39;</span>)</span>
<span id="cb472-8"><a href="#cb472-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb472-9"><a href="#cb472-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(g4)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:CausalBP2"></span>
<img src="DASM2_files/figure-html/CausalBP2-1.png" alt="The causal diagram for the blood pressure example." width="672" />
<p class="caption">
Figure 4.3: The causal diagram for the blood pressure example.
</p>
</div>
<p>With what model should the data be analysed for answering the research question? From the causal diagram we can already see that <em>age</em> is not a confounder, nor a collider. Since it is not a confounder, we are not obliged to include <em>age</em> in the model, and since it is not a collider this is no reason to not include <em>age</em> in the model.</p>
<p>The concept of <strong>collapsibility</strong> may help us to make a decision. We will introduce this concept with the following notation:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span>: outcome</p></li>
<li><p><span class="math inline">\(X\)</span>: target regressor</p></li>
<li><p><span class="math inline">\(Z\)</span>: covariate. Thus <span class="math inline">\(Z\)</span> is a potential regressor, which we refer to as a <em>covariate</em> (i.e. it co-varies with the outcome).</p></li>
<li><p><span class="math inline">\(f_{xy\mid z}\)</span>: density function of the distribution of <span class="math inline">\((Y,X) \mid Z\)</span></p></li>
<li><p><span class="math inline">\(f_{xy}\)</span>: density function of the distribution of <span class="math inline">\((Y,X)\)</span>.</p></li>
<li><p><span class="math inline">\(g(\cdot)\)</span> is a <strong>functional</strong>, i.e. a mapping from a density function to the real line <span class="math inline">\(\mathbb{R}\)</span>. The (conditional) expectation is an example of such a functional.</p></li>
</ul>
<p>We say that <span class="math inline">\(g(\cdot)\)</span> is collapsible on <span class="math inline">\(Z\)</span> if
<span class="math display">\[
  \Ef{Z}{g(f_{xy\mid z}(Y,X \mid Z))} = g(f_{xy}(Y,X)).
\]</span></p>
<p>If <span class="math inline">\(g(\cdot)\)</span> is the indentity function, the collapsibility condition is trivial:
<span class="math display">\[
  \Ef{Z}{f_{xy\mid z}(Y,X \mid Z)} = \int_{-\infty}^{+\infty} f_{xy\mid z}(Y,X \mid z) f_z(z) dz = \int_{-\infty}^{+\infty} f_{xyz}(Y,Z,z)  dz = f_{xy}(Y,X) = g(f_{xy}(Y,X)).
\]</span></p>
<p>We now apply the collapsibility definition to <span class="math inline">\(g(.)\)</span> being the target effect according to the two following statistical models
<span class="math display">\[
  {\cal{M}}_{xz}: \E{Y \mid X, Z} = \beta_0 + \beta_x X + \beta_z Z
\]</span>
and
<span class="math display">\[
  {\cal{M}}_{x}: \E{Y \mid X} = \alpha_0 + \alpha_x X.
\]</span></p>
<p>Consider, for some density function <span class="math inline">\(f_{xy}\)</span> of <span class="math inline">\((Y,X)\)</span>, the following functional:
<span class="math display">\[
  g(f_{xy}) = \E{Y \mid X=x+1} -  \E{Y \mid X=x}.
\]</span>
When Model <span class="math inline">\({\cal{M}}_{x}\)</span> is conisdered, this becomes
<span class="math display">\[
   g(f_{xy}) = \E{Y \mid X=x+1} -  \E{Y \mid X=x} = \alpha_x
\]</span>
which is the target effect size of <span class="math inline">\(X\)</span> in model <span class="math inline">\({\cal{M}}_{x}\)</span>.</p>
<p>When we then apply the same <span class="math inline">\(g(\cdot)\)</span> to the density function of <span class="math inline">\((Y,X)\)</span>, conditional on <span class="math inline">\(Z\)</span>, we get
<span class="math display">\[
  g(f_{xy\mid z}) = \E{Y \mid X=x+1, Z=z} -  \E{Y \mid X=x, Z=z}.
\]</span>
When applied to Model <span class="math inline">\({\cal{M}}_{xz}\)</span>, we find
<span class="math display">\[
  g(f_{xy\mid z}) = \E{Y \mid X=x+1, Z=z} -  \E{Y \mid X=x, Z=z} = \beta_x.
\]</span></p>
<p>Hence, the effect size in Model <span class="math inline">\({\cal{M}}_{xz}\)</span> is collapsible on variable <span class="math inline">\(Z\)</span> if
<span class="math display">\[
  \Ef{Z}{g(f_{xy\mid z}(Y,X \mid Z))} = g(f_{xy}(Y,X))
\]</span>
or, equivalently,
<span class="math display">\[
  \beta_x=\Ef{Z}{\E{Y \mid X=x+1, Z} -  \E{Y \mid X=x, Z}} = \E{Y \mid X=x+1} -  \E{Y \mid X=x} = \alpha_x.
\]</span>
This holds if,
<span class="math display">\[
  \Ef{Z}{\E{Y \mid X, Z}} = \E{Y \mid X}.
\]</span>
This equality will always hold if any of the following independences hold:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are conditionally independent, given <span class="math inline">\(X\)</span>, i.e,
<span class="math display">\[
 Y \ind Z \mid X,
\]</span></p></li>
<li><p><span class="math inline">\(Z\)</span> is independent of <span class="math inline">\(X\)</span>, i.e. 
<span class="math display">\[
Z \ind X.
\]</span></p></li>
</ul>
<p>This can be seen as follows:</p>
<ul>
<li><p>The condition <span class="math inline">\(Y \ind Z \mid X\)</span> says that if <span class="math inline">\(X\)</span> is known, <span class="math inline">\(Z\)</span> provides no additional information on the distribution of <span class="math inline">\(Z\)</span>. Hence <span class="math inline">\(\E{Y\mid X,Z} = \E{Y\mid X}\)</span></p></li>
<li><p>If <span class="math inline">\(Z \ind Z\)</span>, we can make the following calculations (the independence property is used when going from line one to line two):
<span class="math display">\[\begin{eqnarray*}
 \Ef{Z}{\E{Y \mid X,Z}}
 &amp;=&amp; \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y f(y\mid x,z) dy f(z) dz \\
 &amp;=&amp; \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y \frac{f(y, x,z)}{f(x) f(z)} dy f(z) dz \\
 &amp;=&amp; \int_{-\infty}^{+\infty} \frac{y}{f(x)} \int_{-\infty}^{+\infty} f(y, x,z) dz dy \\
 &amp;=&amp; \int_{-\infty}^{+\infty} \frac{y}{f(x)}  f(y, x) dy \\
 &amp;=&amp; \int_{-\infty}^{+\infty}y  f(y\mid x) dy \\
 &amp;=&amp; \E{Y\mid X}
\end{eqnarray*}\]</span></p></li>
</ul>
<p>When applied to the blood pressure example with <span class="math inline">\(Z\)</span> the age, <span class="math inline">\(X\)</span> the treatment and <span class="math inline">\(Y\)</span> the blood pressure reduction, we can immediately see that <span class="math inline">\(Z \ind X\)</span> (age is independent of treatment), which is guaranteed by the randomisation of the treatment. This can also be read from the causal diagram in Figure <a href="#fig:CausalBP2">4.3</a>.</p>
<p>What is collapsibility telling us for the blood pressure example?</p>
<ul>
<li><p>the research questions has nothing to do with the age, and the causal effect of interest is given by the parameter <span class="math inline">\(\alpha_x\)</span> in Model <span class="math inline">\({\cal{M}}_x\)</span>.</p></li>
<li><p>if we add age to the model, the (conditional) treatment effect is given by the parameter <span class="math inline">\(\beta_x\)</span> in Model <span class="math inline">\({\cal{M}}_{xz}\)</span>.</p></li>
<li><p>collapsibility tells us that <span class="math inline">\(\beta_x=\alpha_x\)</span>, and hence we can choose to work with either Model <span class="math inline">\({\cal{M}}_x\)</span> or Model <span class="math inline">\({\cal{M}}_{xz}\)</span>.</p></li>
</ul>
<p>On the one hand we have a convenient answer: both models are good and will provide us with unbiased estimators of the target causal effect. Can we find arguments to prefer one model over the other? We will come back to this question, but first we show the results of the two model fits.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="#cb473-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/Bloodpressure2.RData&quot;</span>)</span>
<span id="cb473-2"><a href="#cb473-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb473-3"><a href="#cb473-3" aria-hidden="true" tabindex="-1"></a>m.x<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>treatment, <span class="at">data=</span>BloodPressure2)</span>
<span id="cb473-4"><a href="#cb473-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.x)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ treatment, data = BloodPressure2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -7.10  -1.75   0.20   1.50   5.90 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   -0.900      1.168  -0.771  0.45083   
## treatment      5.400      1.651   3.270  0.00425 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.692 on 18 degrees of freedom
## Multiple R-squared:  0.3727, Adjusted R-squared:  0.3378 
## F-statistic: 10.69 on 1 and 18 DF,  p-value: 0.004252</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="#cb475-1" aria-hidden="true" tabindex="-1"></a>m.xz<span class="ot">&lt;-</span><span class="fu">lm</span>(bp.reduction<span class="sc">~</span>treatment<span class="sc">+</span>Age, <span class="at">data=</span>BloodPressure2)</span>
<span id="cb475-2"><a href="#cb475-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.xz)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bp.reduction ~ treatment + Age, data = BloodPressure2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0220 -1.3936  0.3129  1.3029  2.8220 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 33.69677    4.92470   6.842 2.86e-06 ***
## treatment    5.78441    0.85692   6.750 3.40e-06 ***
## Age         -0.64068    0.09051  -7.079 1.85e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.912 on 17 degrees of freedom
## Multiple R-squared:  0.8411, Adjusted R-squared:  0.8224 
## F-statistic: 44.99 on 2 and 17 DF,  p-value: 1.621e-07</code></pre>
<p>From model <em>m.x</em> we conclude that average blood pressure reduction in the treatment group is estimated to be <span class="math inline">\(5.4\)</span> mmHg (SE <span class="math inline">\(1.65\)</span> mmHg) larger than in the placebo group. This is significant at the 5% level of significance (<span class="math inline">\(p=0.0043\)</span>).</p>
<p>From model <em>m.xz</em> we conclude that among people of the same age, the average blood pressure reduction in the treatment group is estimated to be <span class="math inline">\(5.8\)</span> mmHg (SE <span class="math inline">\(0.86\)</span> mmHg) larger than in the placebo group. This is significant at the 5% level of significance (<span class="math inline">\(p&lt;0.0001\)</span>).</p>
<p>Discussion:</p>
<ul>
<li><p>Although in both models the effect size of the treatment is the same (<span class="math inline">\(\alpha_x = \beta_x\)</span>), the estimates are numerically different. Note that this is not a problem from a theoretical perspective. Both estimates result from unbiased estimators of the same causal effect.</p></li>
<li><p>The standard error (SE) of the causal effect estimate in the model with the age effect (<em>m.xz</em>) is smaller than in the model without the age effect. This is not true in general, but it a beneficial effect of adding the covariate.</p></li>
<li><p>The <span class="math inline">\(p\)</span>-value for the causal effect in the model with the age effect (<em>m.xz</em>) is smaller than in the model without the age effect. This is here a consequence of the smaller SE in the former model.</p></li>
<li><p>In model <em>m.xz</em> we have given the default interpretation of the effect of the treatment, i.e. the effect of the treatment controlled for age. However, now that we know that collapsibility gives <span class="math inline">\(\alpha_x=\beta_x\)</span>, we were not required to give the conditional interpretation The marginal interpretation, as in model <em>m.x</em>, is also correct.</p></li>
</ul>
<p>For this particular case study, the inclusion of age seems to be advantegous: it gives a smaller SE and hence a more narrow confidence interval for the causal effect size. A smaller SE also results in a smaller <span class="math inline">\(p\)</span>-value and hence in larger statistical power. Adding a covariate to the model, however, does not always result in a reduction of the SE. It only happens if the covariate is correlated with the outcome.</p>
<p>In this section, we have fitted two models to the dataset. This was for illustration purposes only. In a real setting you may not first fit the two models, and then select the model that gives you the <em>best</em> or desired results!</p>
<p>Finally, we show some exploratory graphs for the <em>BloodPressure2</em> dataset.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="#cb477-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(BloodPressure2<span class="sc">$</span>Age<span class="sc">~</span>BloodPressure2<span class="sc">$</span>treatment, </span>
<span id="cb477-2"><a href="#cb477-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;treatment group (0: placebo -- 1: active treatment)&quot;</span>,</span>
<span id="cb477-3"><a href="#cb477-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;age (years)&quot;</span>,</span>
<span id="cb477-4"><a href="#cb477-4" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-143-1.png" width="672" /></p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="#cb478-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(BloodPressure2<span class="sc">$</span>Age[BloodPressure2<span class="sc">$</span>treatment<span class="sc">==</span><span class="dv">0</span>])</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   48.00   49.50   52.50   54.00   59.25   61.00</code></pre>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="#cb480-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(BloodPressure2<span class="sc">$</span>Age[BloodPressure2<span class="sc">$</span>treatment<span class="sc">==</span><span class="dv">1</span>])</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   48.00   51.75   55.00   54.60   57.00   62.00</code></pre>
<p>These boxplots need to be interpreted with care, because each is based on only 10 observations. The results show that the age distributions in the two treatment groups are similar. This is of course a consequence of the randomisation which assures age <span class="math inline">\(\ind\)</span> treatment.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="#cb482-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(BloodPressure2<span class="sc">$</span>Age,BloodPressure2<span class="sc">$</span>bp.reduction,</span>
<span id="cb482-2"><a href="#cb482-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;age (years)&quot;</span>,</span>
<span id="cb482-3"><a href="#cb482-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Blood pressure reduction (mmHg)&quot;</span>,</span>
<span id="cb482-4"><a href="#cb482-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.axis=</span><span class="fl">1.5</span>, <span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-144-1.png" width="672" /></p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="#cb483-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor</span>(BloodPressure2<span class="sc">$</span>Age,BloodPressure2<span class="sc">$</span>bp.reduction)</span></code></pre></div>
<pre><code>## [1] -0.6443274</code></pre>
<p>These results show a negative correlation between the covariate <em>age</em> and the outcome variable.</p>
</div>
<div id="randomisation-restriction" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Randomisation restriction</h2>
<p>Consider the following (artificial) example.
The objective is to assess the effect of treatment (<em>treatment</em>=1) versus placebo (<em>treatment</em>=0) on the mean outcome. The researchers think that gender may have an effect on the outcome. For this reason they want to guarantee a balance between men and women in the study. For this reason they setup the following randomisation scheme: for each gender 10 subjects are randomly assigned to treatment and 10 subjects are assigned to placebo.</p>
<p>This design strongly resembles a randomised design, except that randomisation is performed within groups (gender). How should this dataset be analysed? In particular, should gender be included as a regressor in the model, or not?</p>
<p>In our example gender imposes a <strong>randomisation restriction</strong>. If it were a completely randomised design that does not make use of the gender, then with a total sample size of <span class="math inline">\(n\)</span>, the number of possible assignments of <span class="math inline">\(n\)</span> subjects in two groups of <span class="math inline">\(n_0=n_1\)</span> subjects, is given by <span class="math inline">\({n \choose n_1}\)</span>. For example, with <span class="math inline">\(n=40\)</span> and <span class="math inline">\(n_0=n_1=20\)</span>, <span class="math inline">\({n \choose n_1}={40 \choose 20}\)</span>=137846528820. On the other hand, when we randomise subjects to treatment within the two gender groups, with the restriction that for each gender we randomise 20 men or women to 10 active treatments and 10 placebos, the number of possible treatment assignments is given by <span class="math inline">\({20 choose 10}\times {20 choose 10}\)</span>=34134779536, which is approximately a factor 4 smaller as compared to the unrestricted randomisation.</p>
<p>In this context, the gender is referred to as a <strong>stratification factor</strong> or a <strong>blocking factor</strong>. Each gender (man or woman) is then referred to as a <strong>stratum</strong> or a <strong>block</strong>.</p>
<p>Can we think of <em>gender</em> as a confounder, collider or covariate?</p>
<ul>
<li><p>gender may affect the outcome (as for a confounder and a covariate, but not as for a collider)</p></li>
<li><p>despite the randomisation restriction implied by gender, gender is still stochastically independent of treatment (knowing the gender, does not change the probability of being treated or not). Hence, (1) gender is not a confounder, and (2) the effect size for treatment is collapsible on gender.</p></li>
</ul>
<p>So these arguments seem to suggest that we are free to either include <em>gender</em> as a covariate in the model, or not. This is, however, not entirely true. Since gender imposes a randomisation restriction, gender must be included in the model, otherwise no valid inference for the effect size of treatment can be guaranteed. The focus is not not on bias, but on the validity of the statistical inference (standard error, confidence intervals and hypothesis testing).</p>
<p>We will not prove that for <em>stratified randomised study</em> the <em>stratification factor</em> (here: <em>gender</em>) must be included in the model. Rather we will illustrate it in a simulation study.</p>
<p>In the next simulation study, we will randomise 20 men over two treatment groups in a balanced way, and, similarly, we will also randomise 20 women over two treatment groups in a balanced way. For each repeated experiment, we will analyse the data with two models: one with only the treatment as a 0/1 dummy regressor, and one with treatment and gender as 0/1 dummies. The outcomes are simulated under the null hypothesis of no treatment effect, but gender does have an effect on the mean outcome. Since data are simulated under the null hypothesis of no treatment effect, we expect the p-values to be uniformly distributed and we expect the empirical type I error rate to be close to the nominal level of <span class="math inline">\(\alpha=5\%\)</span>.
Over the repeated experiments, we keep track of the estimates of the treatment effect, the standard errors and the p-values for testing for no treatment effect.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="#cb485-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1786</span>)</span>
<span id="cb485-2"><a href="#cb485-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb485-3"><a href="#cb485-3" aria-hidden="true" tabindex="-1"></a>n.men<span class="ot">&lt;-</span><span class="dv">20</span></span>
<span id="cb485-4"><a href="#cb485-4" aria-hidden="true" tabindex="-1"></a>n.women<span class="ot">&lt;-</span><span class="dv">20</span></span>
<span id="cb485-5"><a href="#cb485-5" aria-hidden="true" tabindex="-1"></a>results1<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N10000,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb485-6"><a href="#cb485-6" aria-hidden="true" tabindex="-1"></a>results2<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>N10000,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb485-7"><a href="#cb485-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb485-8"><a href="#cb485-8" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">treatment=</span><span class="fu">rep</span>(<span class="cn">NA</span>,n.men<span class="sc">+</span>n.women),<span class="at">gender=</span><span class="cn">NA</span>,<span class="at">outcome=</span><span class="cn">NA</span>)</span>
<span id="cb485-9"><a href="#cb485-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb485-10"><a href="#cb485-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N10000) {</span>
<span id="cb485-11"><a href="#cb485-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># stratified random treatment assignment</span></span>
<span id="cb485-12"><a href="#cb485-12" aria-hidden="true" tabindex="-1"></a>  treatment.men<span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="fu">c</span>(n.men<span class="sc">/</span><span class="dv">2</span>,n.men<span class="sc">/</span><span class="dv">2</span>)), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb485-13"><a href="#cb485-13" aria-hidden="true" tabindex="-1"></a>  treatment.women<span class="ot">&lt;-</span><span class="fu">sample</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="fu">c</span>(n.women<span class="sc">/</span><span class="dv">2</span>,n.women<span class="sc">/</span><span class="dv">2</span>)), <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb485-14"><a href="#cb485-14" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb485-15"><a href="#cb485-15" aria-hidden="true" tabindex="-1"></a>  db<span class="sc">$</span>treatment<span class="ot">&lt;-</span><span class="fu">c</span>(treatment.men,treatment.women)</span>
<span id="cb485-16"><a href="#cb485-16" aria-hidden="true" tabindex="-1"></a>  db<span class="sc">$</span>gender<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>,n.men),<span class="fu">rep</span>(<span class="dv">1</span>,n.women))</span>
<span id="cb485-17"><a href="#cb485-17" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb485-18"><a href="#cb485-18" aria-hidden="true" tabindex="-1"></a>  <span class="co"># simulation of outcomes</span></span>
<span id="cb485-19"><a href="#cb485-19" aria-hidden="true" tabindex="-1"></a>  db<span class="sc">$</span>outcome<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n.men<span class="sc">+</span>n.women,<span class="at">mean=</span><span class="dv">10</span>,<span class="at">sd=</span><span class="dv">2</span>) <span class="co"># no treatment effect</span></span>
<span id="cb485-20"><a href="#cb485-20" aria-hidden="true" tabindex="-1"></a>  db<span class="sc">$</span>outcome[db<span class="sc">$</span>gender<span class="sc">==</span><span class="dv">0</span>]<span class="ot">&lt;-</span>db<span class="sc">$</span>outcome[db<span class="sc">$</span>gender<span class="sc">==</span><span class="dv">0</span>]<span class="sc">+</span><span class="dv">2</span> <span class="co"># gender effect</span></span>
<span id="cb485-21"><a href="#cb485-21" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb485-22"><a href="#cb485-22" aria-hidden="true" tabindex="-1"></a>  <span class="co"># analysis with the model with only treatment</span></span>
<span id="cb485-23"><a href="#cb485-23" aria-hidden="true" tabindex="-1"></a>  m1<span class="ot">&lt;-</span><span class="fu">lm</span>(outcome<span class="sc">~</span>treatment, <span class="at">data=</span>db)</span>
<span id="cb485-24"><a href="#cb485-24" aria-hidden="true" tabindex="-1"></a>  results1[i,]<span class="ot">&lt;-</span><span class="fu">summary</span>(m1)<span class="sc">$</span>coef[<span class="dv">2</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]</span>
<span id="cb485-25"><a href="#cb485-25" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb485-26"><a href="#cb485-26" aria-hidden="true" tabindex="-1"></a>  <span class="co"># analysis with the model with treatment and gender </span></span>
<span id="cb485-27"><a href="#cb485-27" aria-hidden="true" tabindex="-1"></a>  m2<span class="ot">&lt;-</span><span class="fu">lm</span>(outcome<span class="sc">~</span>treatment<span class="sc">+</span>gender, <span class="at">data=</span>db)</span>
<span id="cb485-28"><a href="#cb485-28" aria-hidden="true" tabindex="-1"></a>  results2[i,]<span class="ot">&lt;-</span><span class="fu">summary</span>(m2)<span class="sc">$</span>coef[<span class="dv">2</span>,<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]</span>
<span id="cb485-29"><a href="#cb485-29" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>First we look at the results of the analysis of the model with the treatment and gender effects included.</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="#cb486-1" aria-hidden="true" tabindex="-1"></a><span class="co"># approximation of the expectation of the target effect size</span></span>
<span id="cb486-2"><a href="#cb486-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(results2[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] -0.001960241</code></pre>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="#cb488-1" aria-hidden="true" tabindex="-1"></a><span class="co"># approximation of the variance of the estimator of the target effect size</span></span>
<span id="cb488-2"><a href="#cb488-2" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(results2[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.4047706</code></pre>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="#cb490-1" aria-hidden="true" tabindex="-1"></a><span class="co"># approximation of the expectation of the estimators of the variance </span></span>
<span id="cb490-2"><a href="#cb490-2" aria-hidden="true" tabindex="-1"></a><span class="co">#    of the estimated target effect size</span></span>
<span id="cb490-3"><a href="#cb490-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(results2[,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.399478</code></pre>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="#cb492-1" aria-hidden="true" tabindex="-1"></a><span class="co"># empirical type I eror rate for testing for no treatment effect size</span></span>
<span id="cb492-2"><a href="#cb492-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(results2[,<span class="dv">3</span>]<span class="sc">&lt;</span><span class="fl">0.05</span>)</span></code></pre></div>
<pre><code>## [1] 0.0501</code></pre>
<p>These simulation results demonstrate:</p>
<ul>
<li><p>the estimator of the treatment effect is unbiased</p></li>
<li><p>the estimator of the variance of the treatment effect estimator is unbiased</p></li>
<li><p>the type I error rate is controlled at the nominal significance level</p></li>
</ul>
<p>Next we look at the results of the analysis of the model with only the treatment effect (thus ignoring the randomisation restriction imposed by <em>gender</em>).</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="#cb494-1" aria-hidden="true" tabindex="-1"></a><span class="co"># approximation of the expectation of the target effect size</span></span>
<span id="cb494-2"><a href="#cb494-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(results1[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] -0.001960241</code></pre>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="#cb496-1" aria-hidden="true" tabindex="-1"></a><span class="co"># approximation of the variance of the estimator of the target effect size</span></span>
<span id="cb496-2"><a href="#cb496-2" aria-hidden="true" tabindex="-1"></a><span class="fu">var</span>(results1[,<span class="dv">1</span>])</span></code></pre></div>
<pre><code>## [1] 0.4047706</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="#cb498-1" aria-hidden="true" tabindex="-1"></a><span class="co"># approximation of the expectation of the estimators of the variance </span></span>
<span id="cb498-2"><a href="#cb498-2" aria-hidden="true" tabindex="-1"></a><span class="co">#    of the estimated target effect size</span></span>
<span id="cb498-3"><a href="#cb498-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(results1[,<span class="dv">2</span>]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.5055602</code></pre>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="#cb500-1" aria-hidden="true" tabindex="-1"></a><span class="co"># empirical type I eror rate for testing for no treatment effect size</span></span>
<span id="cb500-2"><a href="#cb500-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(results1[,<span class="dv">3</span>]<span class="sc">&lt;</span><span class="fl">0.05</span>)</span></code></pre></div>
<pre><code>## [1] 0.0276</code></pre>
<p>These simulation results demonstrate:</p>
<ul>
<li><p>the estimator of the treatment effect is unbiased</p></li>
<li><p>the estimator of the variance of the treatment effect estimator is biased</p></li>
<li><p>the type I error rate is not controlled at the nominal significance level</p></li>
</ul>
<p>Hence, we conclude that</p>
<ul>
<li><p>the factor causing the randomisation restriction should be included in the model (otherwise no valid inference can be guaranteed). Note that this requirement stands even if the effect of the stratification factor is non-signifcant in the data analysis.</p></li>
<li><p>the treatment effect size estimate from the correct model (including <em>gender</em>) also has the marginal treatment effect size interpretation (thanks to the collapsibility)</p></li>
</ul>
</div>
<div id="sample-size-and-power" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Sample size and power</h2>
<p>Consider the normal multiple linear regression model <a href="#eq:Mod5">(3.1)</a>,
<span class="math display">\[
 Y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_{p-1} x_{ip-1} + \eps_ i
\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>Without loss of generallity, suppose we are interested in the parameter <span class="math inline">\(\beta_1\)</span>. We will focus on the hypotheses
<span class="math display">\[
  H_0: \beta_1 =0 \;\;\text{ versus }\;\; H_1: \beta_1\neq 0
\]</span>
and we will be interested in the relationship between the sample size <span class="math inline">\(n\)</span> and the statistical power of the test. The power of a test is only defined for a very specific element of the alternative hypothesis, which is denoted by, for some <span class="math inline">\(\delta\neq 0\)</span>,
<span class="math display">\[
  H_1(\delta): \beta_1 = \delta.
\]</span></p>
<p>If <span class="math inline">\(T\)</span> is the test statistic and if <span class="math inline">\(R_\alpha\)</span> is the rejection region of the test at the <span class="math inline">\(\alpha\)</span> level of significance, then the statistical power is defined as
<span class="math display">\[
  \pi_\alpha(\delta) =\prob{T \in R_\alpha \mid \beta_1=\delta}.
\]</span>
Note that <span class="math inline">\(\pi_\alpha(0)=\alpha\)</span> by construction of the rejection region.</p>
<p>If all model assumptions hold true, then the sampling distribution of the LSE <span class="math inline">\(\hat\beta_1\)</span> is given by (see Theorem <a href="#thm:DistrMod4">2.4</a>)
<span class="math display">\[
   \hat\beta_1 \sim N(\beta_1, \sigma^2_{\beta_1})
\]</span>
with
<span class="math display">\[
  \sigma^2_{\beta_1} = \left[ (\mb{X}^t\mb{X})^{-1}\right]_{2,2} \sigma^2.
\]</span></p>
<p>This sampling distribution forms the basis of hypothesis testing. For simplicity we first assume that the residual variance <span class="math inline">\(\sigma^2\)</span> is known, then for testing
<span class="math display">\[
  H_0: \beta_1 =0 \;\;\text{ versus }\;\; H_1: \beta_1\neq 0
\]</span>
we have the following result for the test statistic
<span class="math display">\[
  T = \frac{\hat\beta_1}{\sigma_{\beta_1}} \stackrel{H_0}{\sim} N(0,1).
\]</span></p>
<p>For studying the power of the test, we also need to know the distrbution of the test statistic under the alternative <span class="math inline">\(H_1(\delta): \beta_1=\delta\)</span>. Under the given assumptions, this is very easy. First we write the test statistic as
<span class="math display">\[
  T=\frac{\hat\beta_1}{\sigma_{\beta_1}}=\frac{\hat\beta_1-\delta+\delta}{\sigma_{\beta_1}} = 
  \frac{\hat\beta_1-\delta}{\sigma_{\beta_1}}+\frac{\delta}{\sigma_{\beta_1}}.
\]</span>
Under the alternative <span class="math inline">\(H_1(\delta)\)</span>,
<span class="math display">\[
   \frac{\hat\beta_1-\delta}{\sigma_{\beta_1}} \sim N(0,1).
\]</span>
Hence, under <span class="math inline">\(H_1(\delta)\)</span>,
<span class="math display">\[
  T=\frac{\hat\beta_1-\delta}{\sigma_{\beta_1}}+\frac{\delta}{\sigma_{\beta_1}}
  \sim N\left(\frac{\delta}{\sigma_{\beta_1}}, 1\right) .
\]</span></p>
<p>We will now use this expression to find the power for the two sided t-test. For the two-sided test, we can use <span class="math inline">\(\mid T \mid\)</span> as a test statistic, which has rejection region
<span class="math display">\[
  R_\alpha=[z_{1-\alpha/2},+\infty[.
\]</span>
The power of this two-sided test can thus be written as
<span class="math display">\[\begin{eqnarray*}
  \pi_\alpha(\delta) 
    &amp;=&amp; \prob{\mid T \mid &gt;z_{1-\alpha/2} \mid \beta_1=\delta} \\
    &amp;=&amp; \prob{T &gt;z_{1-\alpha/2} \mid \beta_1=\delta} + \prob{T &lt; -z_{1-\alpha/2} \mid \beta_1=\delta} \\
    &amp;=&amp; \prob{T -\frac{\delta}{\sigma_{\beta_1}}&gt;z_{1-\alpha/2} -\frac{\delta}{\sigma_{\beta_1}}\mid \beta_1=\delta} + \prob{T -\frac{\delta}{\sigma_{\beta_1}}&lt; -z_{1-\alpha/2}-\frac{\delta}{\sigma_{\beta_1}} \mid \beta_1=\delta} \\
    &amp;=&amp; 1-\Phi\left(z_{1-\alpha/2}-\frac{\delta}{\sigma_{\beta_1}}\right) +  \Phi\left(-z_{1-\alpha/2}-\frac{\delta}{\sigma_{\beta_1}}\right).
\end{eqnarray*}\]</span></p>
<p>In this context, <span class="math inline">\(\frac{\delta}{\sigma_{\beta_1}}\)</span> is referred to as the <strong>noncentrality parameter</strong> (ncp). Note that if the ncp equals zero, the power reduces to the nominal significance level <span class="math inline">\(\alpha\)</span>.</p>
<p>This expression for the power <span class="math inline">\(\pi_\alpha(\delta)\)</span> can be used for sample size calculation, or, more generally, for designing studies so as to guarantee a preset power, say <span class="math inline">\(\pi_0\)</span>.</p>
<p>A very simple example: consider the intercept-only model, <span class="math inline">\(\E{Y\mid x}=\beta_0\)</span> and suppose the target parameter is the population mean <span class="math inline">\(\beta_0\)</span> (thus replace <span class="math inline">\(\beta_1\)</span> in the previous paragraphs with <span class="math inline">\(\beta_0\)</span>). The design matrix <span class="math inline">\(\mb{X}\)</span> for this model is the <span class="math inline">\(n\times 1\)</span> matrix with all elements equal to 1. Hence, <span class="math inline">\(\sigma_{\beta_0}^2=\sigma^2/n\)</span> and the ncp becomes <span class="math inline">\(\sqrt{n}\frac{\delta}{\sigma}\)</span>.</p>
<p>More generally, the ncp (and hence the power) is determined by the full design matrix <span class="math inline">\(\mb{X}\)</span>.</p>
<p>In the previous sections we assumed that the residual variance <span class="math inline">\(\sigma^2\)</span> was known. In realistic settings, this is unknown and typically estimated by MSE. The appropriate test statistic and null distribution is then
<span class="math display">\[
  T = \frac{\hat\beta_1}{\hat\sigma_{\beta_1}} \stackrel{H_0}{\sim} t_{n-p}.
\]</span>
The rejection region for the two-sided test based on <span class="math inline">\(\mid T \mid\)</span> at the <span class="math inline">\(\alpha\)</span> level of significance is thus <span class="math inline">\([t_{n-p; 1-\alpha/2}, +\infty[\)</span>.
It can be shown (here without proof) that under the alternative <span class="math inline">\(H_1(\delta)\)</span>,
<span class="math display">\[
   T \sim t_{n-p,\nu},
\]</span>
i.e. <span class="math inline">\(T\)</span> is distributed as a <strong>noncentral <span class="math inline">\(t\)</span>-distribution</strong> with <span class="math inline">\(n-p\)</span> degrees of freedom and noncentrality parameter <span class="math inline">\(\nu=\frac{\delta}{\sigma_{\beta_1}}\)</span>. The power is now given by
<span class="math display">\[
  \pi_\alpha(\delta) = \prob{\mid T \mid &gt;t_{n-p;1-\alpha/2} \mid \beta_1=\delta} =
  1-F_t\left(t_{n-p;1-\alpha/2}; n-p,\nu\right) +  F_t\left(-t_{n-p;1-\alpha/2}; n-p,\nu\right).
\]</span>
with <span class="math inline">\(F_t(\cdot;n-p,\nu)\)</span> the CDF of a noncentral <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-p\)</span> degrees of freedom and ncp <span class="math inline">\(\nu\)</span>.</p>
</div>
<div id="example-two-01-dummies" class="section level2 unnumbered">
<h2>Example (two 0/1 dummies)</h2>
<p>Suppose we need to design a study with two 0/1 dummy regressors. These two 0/1 dummies thus define <span class="math inline">\(2\times 2=4\)</span> groups. Suppose that one dummy codes for two treatment groups, and the other codes for two hospitals.</p>
<p>The study must be designed such that the power is <span class="math inline">\(\pi_0=80\%\)</span> for detecting a treatment effect of <span class="math inline">\(\beta_1=2\)</span> with a two-sided test at the <span class="math inline">\(\alpha=5\%\)</span> level of significance. This effect size of <span class="math inline">\(\beta_1=2\)</span> is considered to be the smallest clinically relevant effect size. If the true effect size is equal to 2 or larger, the study design should guarantee that it will be detected with a probability of <span class="math inline">\(80\%\)</span>.</p>
<p>From previous studies on the same outcome variable, we know that the variance of the outcome is typically not larger than <span class="math inline">\(\sigma^2=3\)</span>.</p>
<p>We don’t know whether the hospital will have an effect on the mean outcome. The final data analysis will include the 0/1 dummy for hospital as a regressor.</p>
<p>We will solve the following problems: restrict the attention to balanced designs (i.e. equal number of observations in each of the 4 groups): what sample size is needed? Does the effect of the hospital have an effect on the power/sample size?</p>
<p>First we write a function to construct the design matrix for a given sample size <span class="math inline">\(n\)</span>.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="#cb502-1" aria-hidden="true" tabindex="-1"></a>ConstructX<span class="ot">&lt;-</span><span class="cf">function</span>(n) {</span>
<span id="cb502-2"><a href="#cb502-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># n: total sample size. It must be a multiple of 4</span></span>
<span id="cb502-3"><a href="#cb502-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">#</span></span>
<span id="cb502-4"><a href="#cb502-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># This function constructs the design matrix for a given sample size and for </span></span>
<span id="cb502-5"><a href="#cb502-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#   a balanced design, for a model with main effects of two 0/1 dummies</span></span>
<span id="cb502-6"><a href="#cb502-6" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb502-7"><a href="#cb502-7" aria-hidden="true" tabindex="-1"></a>  m<span class="ot">&lt;-</span>n<span class="sc">/</span><span class="dv">4</span></span>
<span id="cb502-8"><a href="#cb502-8" aria-hidden="true" tabindex="-1"></a>  X<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="at">nrow=</span>n,<span class="at">ncol=</span><span class="dv">3</span>)</span>
<span id="cb502-9"><a href="#cb502-9" aria-hidden="true" tabindex="-1"></a>  X[,<span class="dv">1</span>]<span class="ot">&lt;-</span><span class="dv">1</span></span>
<span id="cb502-10"><a href="#cb502-10" aria-hidden="true" tabindex="-1"></a>  X[,<span class="dv">2</span>]<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>,<span class="dv">2</span><span class="sc">*</span>m),<span class="fu">rep</span>(<span class="dv">1</span>,<span class="dv">2</span><span class="sc">*</span>m))</span>
<span id="cb502-11"><a href="#cb502-11" aria-hidden="true" tabindex="-1"></a>  X[,<span class="dv">3</span>]<span class="ot">&lt;-</span><span class="fu">rep</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="fu">rep</span>(m,<span class="dv">4</span>))</span>
<span id="cb502-12"><a href="#cb502-12" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb502-13"><a href="#cb502-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(X)</span>
<span id="cb502-14"><a href="#cb502-14" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Next we write a function for the power.</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="#cb503-1" aria-hidden="true" tabindex="-1"></a>powerX<span class="ot">&lt;-</span><span class="cf">function</span>(n,delta,sigma2,<span class="at">alpha=</span><span class="fl">0.05</span>) {</span>
<span id="cb503-2"><a href="#cb503-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># n: total sample size. It must be a multiple of 4</span></span>
<span id="cb503-3"><a href="#cb503-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># delta: effect size</span></span>
<span id="cb503-4"><a href="#cb503-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># sigma2: varance of the error term</span></span>
<span id="cb503-5"><a href="#cb503-5" aria-hidden="true" tabindex="-1"></a>  <span class="co"># alpha: significance level</span></span>
<span id="cb503-6"><a href="#cb503-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">#</span></span>
<span id="cb503-7"><a href="#cb503-7" aria-hidden="true" tabindex="-1"></a>  <span class="co"># this functions computes the power for detecting the effect size at the alpha level </span></span>
<span id="cb503-8"><a href="#cb503-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#   for a balanced design</span></span>
<span id="cb503-9"><a href="#cb503-9" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb503-10"><a href="#cb503-10" aria-hidden="true" tabindex="-1"></a>  X<span class="ot">&lt;-</span><span class="fu">ConstructX</span>(n)</span>
<span id="cb503-11"><a href="#cb503-11" aria-hidden="true" tabindex="-1"></a>  sigma2Beta<span class="ot">&lt;-</span><span class="fu">solve</span>(<span class="fu">t</span>(X)<span class="sc">%*%</span>X)[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">*</span>sigma2</span>
<span id="cb503-12"><a href="#cb503-12" aria-hidden="true" tabindex="-1"></a>  ncp<span class="ot">&lt;-</span>delta<span class="sc">/</span><span class="fu">sqrt</span>(sigma2Beta)</span>
<span id="cb503-13"><a href="#cb503-13" aria-hidden="true" tabindex="-1"></a>  tcrit<span class="ot">&lt;-</span><span class="fu">qt</span>(<span class="dv">1</span><span class="sc">-</span>alpha<span class="sc">/</span><span class="dv">2</span>,<span class="at">df=</span>n<span class="dv">-3</span>)</span>
<span id="cb503-14"><a href="#cb503-14" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb503-15"><a href="#cb503-15" aria-hidden="true" tabindex="-1"></a>  pwr<span class="ot">&lt;-</span><span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(tcrit,<span class="at">df=</span>n<span class="dv">-3</span>,<span class="at">ncp =</span> ncp)<span class="sc">+</span><span class="fu">pt</span>(<span class="sc">-</span>tcrit,<span class="at">df=</span>n<span class="dv">-3</span>,<span class="at">ncp =</span> ncp)</span>
<span id="cb503-16"><a href="#cb503-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb503-17"><a href="#cb503-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(pwr)</span>
<span id="cb503-18"><a href="#cb503-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb503-19"><a href="#cb503-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb503-20"><a href="#cb503-20" aria-hidden="true" tabindex="-1"></a><span class="co"># check: power for delta=0 (which agrees with H0) should be alpha</span></span>
<span id="cb503-21"><a href="#cb503-21" aria-hidden="true" tabindex="-1"></a><span class="fu">powerX</span>(<span class="dv">20</span>,<span class="at">delta=</span><span class="dv">0</span>,<span class="at">sigma2=</span><span class="dv">3</span>)</span></code></pre></div>
<pre><code>## [1] 0.05</code></pre>
<p>Next we apply the power function for a sequence of sample sizes and make a plot of power versus sample size.</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="#cb505-1" aria-hidden="true" tabindex="-1"></a>nseq<span class="ot">&lt;-</span><span class="fu">seq</span>(<span class="dv">4</span>,<span class="dv">40</span>,<span class="at">by=</span><span class="dv">4</span>)</span>
<span id="cb505-2"><a href="#cb505-2" aria-hidden="true" tabindex="-1"></a>pwr<span class="ot">&lt;-</span><span class="fu">c</span>()</span>
<span id="cb505-3"><a href="#cb505-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb505-4"><a href="#cb505-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(n <span class="cf">in</span> nseq) {</span>
<span id="cb505-5"><a href="#cb505-5" aria-hidden="true" tabindex="-1"></a>  pwr<span class="ot">&lt;-</span><span class="fu">c</span>(pwr,<span class="fu">powerX</span>(n,<span class="at">delta=</span><span class="dv">2</span>,<span class="at">sigma2=</span><span class="dv">3</span>))</span>
<span id="cb505-6"><a href="#cb505-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb505-7"><a href="#cb505-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb505-8"><a href="#cb505-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(nseq,pwr,</span>
<span id="cb505-9"><a href="#cb505-9" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;sample size&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;power&quot;</span>,</span>
<span id="cb505-10"><a href="#cb505-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">cex.lab=</span><span class="fl">1.5</span>, <span class="at">cex.axis=</span><span class="fl">1.5</span>)</span>
<span id="cb505-11"><a href="#cb505-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fl">0.8</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-150-1.png" width="672" /></p>
<p>From this graph we read that we need a sample size of <span class="math inline">\(n=28\)</span> to guarantee a power of <span class="math inline">\(80\%\)</span>.</p>

</div>
</div>
<div id="Ch:ANOVA" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Analysis of Variance</h1>
<p>Linear regression models are basically models for which the conditional mean of the outcome is modeled as a function of <em>continuous regressors</em> and which is linear in the parameters. Although in general the regressors are continuous variables, we have seen a special case in which the regressors are defined as 0/1 binary dummy variables. This construction can be used for modelling the conditional outcome mean for two populations (as referred to by the 0 and 1 value of the regressor). Classical examples include <em>gender</em> and <em>treatment</em>.</p>
<p>In many applications, however, there is a need to model the conditional outcome mean for more than two populations. For example, there may be more than two treatments involved, or the mean outcome of more than two countries need to be compared. In such settings we refer to <em>treatment</em> or <em>country</em> as the <strong>factor variable</strong>, and the treatments or the countries as the <strong>levels</strong> of the factor. Thus, in the previous chapters we could have said that <em>gender</em> and <em>treatment</em> were factor variables, each with only two levels.</p>
<p>In this chapter we will allow for factor variables with more than two levels. Again we will define dummy regressors and this will allow us to use the theory of the linear regression models.</p>
<div id="example-pwd-post-weaning-diarrhea" class="section level2 unnumbered">
<h2>Example (PWD: Post-weaning diarrhea)</h2>
<p>This example is taken from the following paper: Vangroenweghe and Thas (2020). Application of High Energy and Protein Diets in Combination with a Live Avirulent <em>Escherichia coli</em> F4 Vaccine Against Post-Weaning Diarrhea. <em>Vaccine Research</em>, 7(1), 1-9.</p>
<p>The data presented here may slightly deviate from the data presented in the paper.</p>
<p>Post-weaning diarrhea (PWD) is a worldwide economically important disease in pigs in piggeries. The disease is characterised by increased mortality, weight loss, retarded growth, increased treatment costs and higher use of antibiotics. Enterotoxigenic <em>Escherichia coli</em> is considered to be the most important cause of the disease.</p>
<p>Currently the disease is often controlled by using antimicrobials, but the emergence of antimicrobial resistence in <em>E. coli</em> urges the need for alternative control strategies. For example, inclusion of additional dietary fiber and reduction of crude protein levels, but also the addition of zinc oxide (ZnO) has been demonstrated to have beneficial effects. However, by 2022 this zinc may no longer be used (EU legislation).</p>
<p>Another strategy is to vaccinate the piglets. In this study <strong>we are interested in the effect of vaccination</strong> as compared to the addidition of ZnO and nutraceuticals (e.g. fibers) to the feed. In particular the following treatments are considered (in the dataset this variables is names <em>Treatment</em>):</p>
<ul>
<li><p>A: normal feed + ZnO</p></li>
<li><p>B: normal feed + nutraceuticals</p></li>
<li><p>C: vaccination + high energy/protein in phases 2 and 3 (time periods)</p></li>
<li><p>D: vaccination + high energy/protein in phases 1, 2 and 3</p></li>
<li><p>E: vaccination + high energy/protein in phases 1, 2 and 3 + nutraceutics</p></li>
</ul>
<p>We are interested in the following outcomes:</p>
<ul>
<li><p><em>ADWG0021</em>, <em>ADWG2150</em>, <em>ADWG0050</em>: <strong>average daily weight gain</strong> (g/day) in the period between 0 and 21 days post-weaning, between day 21 and day 50 post-weaning and in the period between 0 and 50 days post-weaning, repectively.</p></li>
<li><p><em>FCR0021</em>, <em>FCR2150</em>, <em>FCR0050</em>: <strong>food conversion rate</strong> (kg growth / kg feed) in the period between 0 and 21 days post-weaning, between day 21 and day 50 post-weaning and in the period between 0 and 50 days post-weaning, repectively.</p></li>
</ul>
<p>A few details about the <strong>design of the study</strong>:</p>
<ul>
<li><p>Piglets live in pens (<span class="math inline">\(\approx\)</span> cages) (16 piglets in one pen).</p></li>
<li><p>We only have the total weights of the piglets living together in a pen.</p></li>
<li><p>In this study each treatment group consists of 128 piglets (thus 8 pens of 16 piglets).</p></li>
<li><p>The five treatments were randomised over the the <span class="math inline">\(5\times 8 = 40\)</span> pens.</p></li>
</ul>
<p>First we read the data and explore the dataset (only the ADWG variables).</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="#cb506-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/PWD.RData&quot;</span>)</span>
<span id="cb506-2"><a href="#cb506-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(PWD)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    40 obs. of  9 variables:
##  $ Pen      : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Treatment: Factor w/ 5 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 1 1 3 3 5 5 2 2 4 4 ...
##  $ Feeder   : num  1 1 2 2 3 3 4 4 5 5 ...
##  $ Sex      : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ W0       : num  110 111 108 99 103 ...
##  $ P0       : num  16 16 16 16 16 16 16 16 16 16 ...
##  $ ADWG0021 : num  167 152 131 152 134 ...
##  $ ADWG2150 : num  526 472 608 569 502 ...
##  $ ADWG0050 : num  375 338 408 394 348 ...</code></pre>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="#cb508-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(PWD)</span></code></pre></div>
<pre><code>##   Pen Treatment Feeder Sex    W0 P0 ADWG0021 ADWG2150 ADWG0050
## 1   1         A      1   1 110.0 16 166.6667 525.8621 375.0000
## 2   2         A      1   1 111.0 16 151.7857 471.9828 337.5000
## 3   3         C      2   1 108.5 16 130.9524 608.1178 407.7083
## 4   4         C      2   1  99.0 16 151.7857 568.9655 393.7500
## 5   5         E      3   1 103.0 16 133.9286 502.1552 347.5000
## 6   6         E      3   1 104.5 16 147.3214 500.0000 351.8750</code></pre>
<p>The dataset contains more variables than those decribed earlier. We may need them later, but for the moment you may ignore them.</p>
<p>Note that the <em>Treatment</em> variable is recognised by R as a character variable. In this chapter we will consider <em>Treatment</em> as a factor variable. For this reason, we will make this explicit in R:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="#cb510-1" aria-hidden="true" tabindex="-1"></a>PWD<span class="sc">$</span>Treatment<span class="ot">&lt;-</span><span class="fu">as.factor</span>(PWD<span class="sc">$</span>Treatment)</span>
<span id="cb510-2"><a href="#cb510-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(PWD)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    40 obs. of  9 variables:
##  $ Pen      : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Treatment: Factor w/ 5 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 1 1 3 3 5 5 2 2 4 4 ...
##  $ Feeder   : num  1 1 2 2 3 3 4 4 5 5 ...
##  $ Sex      : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ W0       : num  110 111 108 99 103 ...
##  $ P0       : num  16 16 16 16 16 16 16 16 16 16 ...
##  $ ADWG0021 : num  167 152 131 152 134 ...
##  $ ADWG2150 : num  526 472 608 569 502 ...
##  $ ADWG0050 : num  375 338 408 394 348 ...</code></pre>
<p>We will start with studying the ADWG after 21 days (<em>ADWG0021</em>) and after 50 days (<em>ADWG0050</em>).</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="#cb512-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(PWD<span class="sc">$</span>ADWG0021<span class="sc">~</span>PWD<span class="sc">$</span>Treatment, <span class="at">xlab=</span><span class="st">&quot;Treatment&quot;</span>, </span>
<span id="cb512-2"><a href="#cb512-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;ADWG between 0 and 21 days post-weaning (g/day&quot;</span>,</span>
<span id="cb512-3"><a href="#cb512-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-153-1.png" width="672" /></p>
<p>While interpreting the boxplots, we have to keep in mind that each boxplot is based on only 8 observations. The plot suggests that the ADWG is the largest in treatment group A (ZnO addition), but from the graph is it hard to see whether this will be siginificantly higher than the ADWG of the vaccination groups (C, D and E). From these plots, we can already see that the variances are more less equal.</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="#cb513-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(PWD<span class="sc">$</span>ADWG0050<span class="sc">~</span>PWD<span class="sc">$</span>Treatment, <span class="at">xlab=</span><span class="st">&quot;Treatment&quot;</span>, </span>
<span id="cb513-2"><a href="#cb513-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">ylab=</span><span class="st">&quot;ADWG between 0 and 50 days post-weaning (g/day&quot;</span>,</span>
<span id="cb513-3"><a href="#cb513-3" aria-hidden="true" tabindex="-1"></a>        <span class="at">cex.axis=</span><span class="fl">1.5</span>,<span class="at">cex.lab=</span><span class="fl">1.5</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-154-1.png" width="672" /></p>
<p>The boxplots for <em>ADWG0050</em> suggest that the three vaccination treatment have larger ADWG as compared to the other two treatments. Again, formal statistical analysis is needed to check whether this effect can be attributed by chance or whether it is caused by the vaccination.</p>
</div>
<div id="S:ANOVA1" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> One-Way ANOVA</h2>
<div id="S:ANOVA1Mu3" class="section level3" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Comparison of Three Means</h3>
<p>Let us start with the simplest extension of the setting of the two-sample <span class="math inline">\(t\)</span>-test (i.e. the <em>two-sample problem</em>): comparison of three means. One way of formulating the problem is by means of hypotheses:
<span class="math display">\[
  H_0: \mu_1=\mu_2=\mu_3 \text{ versus } H_1: \text{ not } H_0.
\]</span>
Of course we are also interested in estimating the <strong>effect sizes</strong>: <span class="math inline">\(\mu_1-\mu_2\)</span>, <span class="math inline">\(\mu_1-\mu_3\)</span> and <span class="math inline">\(\mu_2-\mu_3\)</span> (and their confidence intervals).</p>
<p>To illustrate this simple problem, we will focus on the three vaccination treatment groups of the PWD example.</p>
<p>Let <span class="math inline">\(Y_i\)</span> denote the outcome of observation <span class="math inline">\(i\)</span> (<span class="math inline">\(i=1,\ldots, n\)</span>). Consider the linear regression model</p>
<p><span class="math display" id="eq:Regmu3">\[\begin{equation}
  \tag{5.1}
  Y_i = \beta_0+\beta_1 x_{i1} +\beta_2 x_{i2} +\eps_i
\end{equation}\]</span></p>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span> and with the dummy regressors</p>
<p><span class="math display">\[\begin{eqnarray*}
  x_{i1} 
   &amp;=&amp; 1 \text{ if observation i belongs to treatment C group} \\
   &amp;=&amp; 0 \text{ if observation i belongs to another treatment group}
\end{eqnarray*}\]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{eqnarray*}
  x_{i2} 
   &amp;=&amp; 1 \text{ if observation i belongs to treatment D group} \\
   &amp;=&amp; 0 \text{ if observation i belongs to another treatment group}
\end{eqnarray*}\]</span></p>
<p>The treatment group with <span class="math inline">\(x_{i1}=x_{i2}=0\)</span> is referred to as the <strong>reference group</strong> and in the (reduced) PWD example this corresponds to the treatment E group.</p>
<p>For an observation in the reference group E the regression model becomes
<span class="math display">\[
   Y_i = \beta_0+\eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>For an observation in treatment C group the model becomes
<span class="math display">\[
   Y_i = \beta_0+\beta_1 + \eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>For an observation in treatment D group the model becomes
<span class="math display">\[
   Y_i = \beta_0+\beta_2 + \eps_i
\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>The interpretation of the <span class="math inline">\(\beta\)</span>-parameters is thus straightforward:</p>
<p><span class="math display">\[\begin{eqnarray*}
   \beta_0 &amp;=&amp;  \E{Y \mid \text{treatment E}} \\
   \beta_1 &amp;=&amp;  (\beta_0+\beta_1)-\beta_0 = \E{Y \mid \text{treatment C}} - \E{Y \mid \text{treatment E}} \\
   \beta_2 &amp;=&amp;  (\beta_0+\beta_2)-\beta_0 = \E{Y \mid \text{treatment D}}-\E{Y \mid \text{treatment E}} .
 \end{eqnarray*}\]</span>
In other words: parameter <span class="math inline">\(\beta_0\)</span> is the mean outcome in the reference group (here: treatment E). Parameter <span class="math inline">\(\beta_1\)</span> is the effect of treatment C relative to the reference treatment E. Parameter <span class="math inline">\(\beta_2\)</span> is the effect of treatment D relative to the reference treatment E.</p>
<p>We now reformulate the models, making use of the <span class="math inline">\(\mu\)</span>-notation. Models in terms of the obvious <span class="math inline">\(\mu\)</span> notation are known as the <strong>cell means models</strong>.</p>
<p><span class="math display">\[\begin{eqnarray*}
 Y_i &amp;=&amp; \beta_0+\eps_i = \mu_1+\eps_i \;\;\text{ for the reference group = treatment E}\\
 Y_i &amp;=&amp; \beta_0+\beta_1+ \eps_i = \mu_2+\eps_i \;\;\text{ for treatment C} \\ 
 Y_i &amp;=&amp; \beta_0+\beta_2 + \eps_i = \mu_3+\eps_i \;\;\text{ for treatment D} ,
\end{eqnarray*}\]</span></p>
<p>with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span> en with <span class="math inline">\(\mu_j = \E{Y \mid \text{treatment group } j}\)</span>.</p>
<p>The original null hypothesis <span class="math inline">\(H_0:\mu_1=\mu_2=\mu_3\)</span> can now equivalently be formulated as
<span class="math display">\[
  H_0: \beta_1=\beta_2=0.
\]</span></p>
<p>Since Model <a href="#eq:Regmu3">(5.1)</a> is a linear regression model, we can use the methods from Chapter <a href="#Ch:Reg2">3</a> for estimating the parameters, their variances (standard errors) and for constructing confidence intervals.</p>
<p>On the other hand, testing <span class="math inline">\(H_0: \beta_1=\beta_2=0\)</span> cannot be done with the <span class="math inline">\(t\)</span>-tests from Chapter <a href="#Ch:Reg2">3</a>, but the F-test will be do the job. This will be introduced in this chapter.</p>
</div>
<div id="example-pwd" class="section level3 unnumbered">
<h3>Example (PWD)</h3>
<p>We analyse the PWD data (only treatments C,D and E) by explicitly coding the dummies.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="#cb514-1" aria-hidden="true" tabindex="-1"></a><span class="co"># subset of the data with only treatments C, D, and E</span></span>
<span id="cb514-2"><a href="#cb514-2" aria-hidden="true" tabindex="-1"></a>PWD3 <span class="ot">&lt;-</span> PWD <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Treatment<span class="sc">%in%</span><span class="fu">c</span>(<span class="st">&quot;C&quot;</span>,<span class="st">&quot;D&quot;</span>,<span class="st">&quot;E&quot;</span>)) <span class="sc">%&gt;%</span> <span class="fu">droplevels</span>()</span>
<span id="cb514-3"><a href="#cb514-3" aria-hidden="true" tabindex="-1"></a><span class="co"># the droplevels function resets the levels of the new Treatment factor variable</span></span>
<span id="cb514-4"><a href="#cb514-4" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(PWD3)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    24 obs. of  9 variables:
##  $ Pen      : num  3 4 5 6 9 10 13 14 15 16 ...
##  $ Treatment: Factor w/ 3 levels &quot;C&quot;,&quot;D&quot;,&quot;E&quot;: 1 1 3 3 2 2 1 1 3 3 ...
##  $ Feeder   : num  2 2 3 3 5 5 7 7 8 8 ...
##  $ Sex      : num  1 1 1 1 1 1 2 2 2 2 ...
##  $ W0       : num  108 99 103 104 102 ...
##  $ P0       : num  16 16 16 16 16 16 16 16 16 16 ...
##  $ ADWG0021 : num  131 152 134 147 159 ...
##  $ ADWG2150 : num  608 569 502 500 585 ...
##  $ ADWG0050 : num  408 394 348 352 406 ...</code></pre>
<p>Next we manually construct the dummies.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="#cb516-1" aria-hidden="true" tabindex="-1"></a>xC<span class="ot">&lt;-</span><span class="fu">ifelse</span>(PWD3<span class="sc">$</span>Treatment<span class="sc">==</span><span class="st">&quot;C&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb516-2"><a href="#cb516-2" aria-hidden="true" tabindex="-1"></a>xD<span class="ot">&lt;-</span><span class="fu">ifelse</span>(PWD3<span class="sc">$</span>Treatment<span class="sc">==</span><span class="st">&quot;D&quot;</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb516-3"><a href="#cb516-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb516-4"><a href="#cb516-4" aria-hidden="true" tabindex="-1"></a><span class="co"># let&#39;s check:</span></span>
<span id="cb516-5"><a href="#cb516-5" aria-hidden="true" tabindex="-1"></a>PWD3<span class="sc">$</span>Treatment</span></code></pre></div>
<pre><code>##  [1] C C E E D D C C E E D D C C E E D D C C E E D D
## Levels: C D E</code></pre>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="#cb518-1" aria-hidden="true" tabindex="-1"></a>xC</span></code></pre></div>
<pre><code>##  [1] 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0</code></pre>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="#cb520-1" aria-hidden="true" tabindex="-1"></a>xD</span></code></pre></div>
<pre><code>##  [1] 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1</code></pre>
<p>Now we fit the regression model.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="#cb522-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(PWD3<span class="sc">$</span>ADWG0021<span class="sc">~</span>xC<span class="sc">+</span>xD)</span>
<span id="cb522-2"><a href="#cb522-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = PWD3$ADWG0021 ~ xC + xD)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -23.065 -11.068  -2.604  11.440  34.970 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  148.438      5.925  25.054   &lt;2e-16 ***
## xC            -8.929      8.379  -1.066    0.299    
## xD            -4.836      8.379  -0.577    0.570    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 16.76 on 21 degrees of freedom
## Multiple R-squared:  0.05141,    Adjusted R-squared:  -0.03893 
## F-statistic: 0.5691 on 2 and 21 DF,  p-value: 0.5745</code></pre>
<p>From this analysis we conclude that</p>
<ul>
<li><p>the ADWG in the treatment C group is estimated to be <span class="math inline">\(8.9\)</span> g/day (SE <span class="math inline">\(8.4\)</span> g/day) less than in the reference treatment group E. This effect is not significant at the 5% level of significance (<span class="math inline">\(p=0.299\)</span>).</p></li>
<li><p>the ADWG in the treatment D group is estimated to be <span class="math inline">\(4.8\)</span> g/day (SE <span class="math inline">\(8.4\)</span> g/day) less than in the reference treatment group E. This effect is not significant at the 5% level of significance (<span class="math inline">\(p=0.570\)</span>).</p></li>
<li><p>the ADWG in the reference treatment group E is estimated to be <span class="math inline">\(148.4\)</span> g/day (SE <span class="math inline">\(5.9\)</span> g/day).</p></li>
</ul>
<p>Fortunately, the dummy coding does not need to be done manually in R. If the factor variable is correctly defined as a factor variable in the R data set, R will recognise it as such in the <em>lm</em> function.</p>
<p>Let’s try the analysis with the <em>lm</em> function and with <em>Treatment</em> as factor.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="#cb524-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0021<span class="sc">~</span>Treatment,<span class="at">data=</span>PWD3)</span>
<span id="cb524-2"><a href="#cb524-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ADWG0021 ~ Treatment, data = PWD3)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -23.065 -11.068  -2.604  11.440  34.970 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  139.509      5.925  23.547   &lt;2e-16 ***
## TreatmentD     4.092      8.379   0.488    0.630    
## TreatmentE     8.929      8.379   1.066    0.299    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 16.76 on 21 degrees of freedom
## Multiple R-squared:  0.05141,    Adjusted R-squared:  -0.03893 
## F-statistic: 0.5691 on 2 and 21 DF,  p-value: 0.5745</code></pre>
<p>The <em>summary</em> function shows us the parameter estimates of the dummy-coded regression model. R has constructed the dummies <em>TreatmentD</em> and <em>TreatmentE</em>. However, these are coded differently from what we have done (<em>xC</em> and <em>xD</em>). The difference is caused by the choice of the reference group. We have chosen for the treatment E group, whereas R has chosen for treatment C group. The default choice of R is the factor level which is ranked first in alphabetical order (here C, among the levels C, D, and E). Later we will see how the user can choose the reference group.</p>
<p>The parameter estimates with this other dummy coding have also a straightforward interpretation</p>
<ul>
<li><p>The estimate of the intercept (<span class="math inline">\(\hat\beta_0=139.5\)</span>) gives us an estimate of the ADWG of <span class="math inline">\(139.5\)</span> g/day in the reference treatment C group.</p></li>
<li><p>From <span class="math inline">\(\hat\beta_1=4.092\)</span> we conclude that piglets treated with D (vaccination and high protein diet in all three phases) show on average <span class="math inline">\(4.1\)</span> g/day more weight gain as compared to animals treated with C (vaccination and high protein diet only in phases 2 and 3). This comes with a SE of <span class="math inline">\(8.4\)</span> g/day. The <span class="math inline">\(p\)</span>-value for testing equality of means in treatment groups C and D is given by <span class="math inline">\(p=0.63\)</span>, and hence there is no evidence for any effect of D as compared to C (i.e. the effect of having also high protein diet in phase 1 among the vaccinated piglets is neglectable).</p></li>
<li><p>A similar conclusion holds for the effect of treatment E (vaccination and high protein diet in all three phases and with nutraceuticals) relative to treatmetn C. The effect size is estimated as <span class="math inline">\(8.9\)</span> g/day in favor of treatment E (SE = <span class="math inline">\(8.4\)</span> g/day). This effect is not significant at the 5% level of significance (<span class="math inline">\(p=0.299\)</span>).</p></li>
</ul>
</div>
<div id="the-anova-model" class="section level3" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> The ANOVA model</h3>
<p>Although the model formation of the previous section is correct, this is not the conventional model formulation in the analysis of variance (ANOVA). ANOVA comes with its own terminology and model notation.</p>
<p>A <strong>factor</strong> (or factor variable) refers to different populations or groups. These groups are often referred to as <strong>treatments</strong>, even if their meaning has nothing to do with treatments (again this is conventional terminology).</p>
<p>The groups or treatments are referred to as the <strong>levels</strong> of the factor. A level actually refers to a population (e.g treatment level D refers so to the population of all pens of 16 piglets vaccinated and treated with a high protein diet in the three phases).</p>
<p>In general, consider a study with a factor variable with <span class="math inline">\(t\geq 2\)</span> levels. The outcomes are denoted as <span class="math inline">\(Y_{ij}\)</span> for observation <span class="math inline">\(j=1,\ldots, n_i\)</span> from group <span class="math inline">\(i=1,\ldots, t\)</span>, with <span class="math inline">\(n_i\)</span> the number of observations (<strong>replicates</strong>) in group <span class="math inline">\(i\)</span>. Observation <span class="math inline">\(Y_{ij}\)</span> is sometimes referred to as the <span class="math inline">\(j\)</span>th replicate from treatment <span class="math inline">\(i\)</span>.</p>
<p>The ANOVA model is typically written in its <strong>factor effect model</strong> formulation,</p>
<p><span class="math display" id="eq:ModAnova1">\[\begin{equation}
 \tag{5.2}
 Y_{ij} = \mu + \tau_i +\eps_{ij} \;\;\; i=1,\ldots, t; j=1,\ldots, n_i
\end{equation}\]</span>
with</p>
<ul>
<li><p><strong>error term</strong> <span class="math inline">\(\eps_{ij} \iid N(0,\sigma^2)\)</span></p></li>
<li><p><strong>intercept</strong> <span class="math inline">\(\mu\)</span></p></li>
<li><p><strong>effect</strong> <span class="math inline">\(\tau_i\)</span> of treatment <span class="math inline">\(i\)</span>.</p></li>
</ul>
<p>Just as for linear regression models, we can equivalently write the model as
<span class="math display">\[
   Y \mid \text{treatment }i \sim N(\mu_i,\sigma^2) \;\;\;\text{ with }\;\;\; \mu_i=\E{Y \mid \text{treatment }i}=\mu+\tau_i.
 \]</span></p>
<p>Figure <a href="#fig:AnovaModel">5.1</a> illustrates how the model specifies the conditional distributions of the outcomes in the <span class="math inline">\(t\)</span> treatment groups. Note that the model also implies that the (residual) variance is constant across the <span class="math inline">\(t\)</span> treatment groups. This assumption is referred to as <strong>homoscedasticity</strong>. Model <a href="#eq:ModAnova1">(5.2)</a> actually specifies the conditional distributions of the outcomes in the <span class="math inline">\(t\)</span> treatment groups.</p>
<p>Observe that the model actually parameterises the <span class="math inline">\(t\)</span> population means <span class="math inline">\(\mu_i\)</span> as <span class="math inline">\(\mu_i=\mu+\tau_i\)</span> and that this parameterisation counts <span class="math inline">\(t+1\)</span> parameters (1 <span class="math inline">\(\mu\)</span> parameter and <span class="math inline">\(t\)</span> <span class="math inline">\(\tau_i\)</span>-parameters).
Hence, the model is overparameterised and the <span class="math inline">\(t+1\)</span> parameters are not uniquely specified (and they are not uniquely estimable).
A solution exists in imposing a restriction on the <span class="math inline">\(\tau\)</span>-parameters. At this time we only consider the restriction <span class="math inline">\(\tau_1=0\)</span>. This restriction corresponds to the dummy coding from Section <a href="#S:ANOVA1Mu3">5.1.1</a>. In particular, consider the regression model</p>
<p><span class="math display" id="eq:Regmup">\[\begin{equation}
   \tag{5.3}
   Y_i = \beta_0 + \sum_{j=1}^{t-1} \beta_j x_{ij} + \eps_i
 \end{equation}\]</span>
with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>.</p>
<p>Define the dummies as (<span class="math inline">\(k=1,\ldots, t-1\)</span>)</p>
<p><span class="math display">\[\begin{eqnarray*}
  x_{ik} 
   &amp;=&amp; 1 \text{ if observation i belongs to treatment group k+1} \\
   &amp;=&amp; 0 \text{ if observation i belongs to another treatment group}
\end{eqnarray*}\]</span></p>
<p>(note that in this coding the index <span class="math inline">\(i\)</span> refers to the outcome notation <span class="math inline">\(Y_i\)</span> of the regression model and not to the index <span class="math inline">\(i\)</span> of the outcome notation <span class="math inline">\(Y_{ij}\)</span> of the ANOVA model).</p>
<p>The treatment with observations for which <span class="math inline">\(x_{i1}=x_{i2}=\cdots = x_{it-1}=0\)</span> is then referred to as the <strong>reference treatment</strong>, <strong>reference group</strong> or <strong>reference level</strong>.</p>
<p>With these dummy definitions we become</p>
<p><span class="math display">\[\begin{eqnarray*}
 Y_i &amp;=&amp; \mu_1+\eps_i = \mu + \eps_i = \beta_0 + \eps_i \\
 Y_i &amp;=&amp; \mu_2+\eps_i = \mu+ \tau_2 +\eps_i = \beta_0 + \beta_1+\eps_i \\ 
 \vdots  &amp;=&amp;  \vdots \\
 Y_i &amp;=&amp; \mu_t+\eps_i = \mu+ \tau_t +\eps_i =\beta_0 + \beta_{t-1} +\eps_i,
\end{eqnarray*}\]</span></p>
<p>always with <span class="math inline">\(\eps_i \iid N(0,\sigma^2)\)</span>. The equivalence of the <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\beta\)</span>-parameters is immediate.</p>
<div class="figure"><span style="display:block;" id="fig:AnovaModel"></span>
<img src="DASM2_files/figure-html/AnovaModel-1.png" alt="Illustration of the ANOVA model \@ref(eq:ModAnova1). The black horizontal lines show the contional means. The individual points represent sample observations. The density functions are also shown." width="672" />
<p class="caption">
Figure 5.1: Illustration of the ANOVA model <a href="#eq:ModAnova1">(5.2)</a>. The black horizontal lines show the contional means. The individual points represent sample observations. The density functions are also shown.
</p>
</div>
</div>
<div id="parameter-estimators-1" class="section level3" number="5.1.3">
<h3><span class="header-section-number">5.1.3</span> Parameter Estimators</h3>
<p>The parameters of the ANOVA model <a href="#eq:ModAnova1">(5.2)</a> are estimated via the <span class="math inline">\(\beta\)</span>-parameters of the equivalent regression model formulation, but the final results are presented in terms of the factor effects ANOVA model <span class="math inline">\(\mu\)</span> en <span class="math inline">\(\tau\)</span>-parameters. From the direct relationship between the two sets of parameters we find
<span class="math display">\[
  \hat\mu = \hat\beta_0 = \bar{Y}_1
\]</span>
with <span class="math inline">\(\bar{Y}_1\)</span> the sample mean of the outcomes in the first treatment group (reference group). Further, <span class="math inline">\(\hat\tau_1=\tau_1=0\)</span> (imposed restriction) and for <span class="math inline">\(i=2,\ldots, t\)</span>,
<span class="math display">\[
  \hat\tau_i = \hat\beta_{i-1} = \bar{Y}_i-\bar{Y}_1
\]</span>
with <span class="math inline">\(\bar{Y}_i\)</span> the sample mean of the outcomes in treatment group <span class="math inline">\(i\)</span>.</p>
<p>We write the <em>predictions</em> as (<span class="math inline">\(i=1,\ldots, t\)</span>)
<span class="math display">\[
  \hat{Y}_{ij} = \hat\mu+\hat\tau_i = \bar{Y}_i,
\]</span>
but in the first place they are to be interpreted as the estimates of the group means (conditional means) <span class="math inline">\(\mu_i\)</span>. So we also write <span class="math inline">\(\hat\mu_i = \hat\mu+\hat\tau_i\)</span>.</p>
<p>The sampling distributions of the parameter estimators also follows immediately from the theory of the regression analysis of the previous chapters. All parameter estimators are unbiased and normally distributed. We denote the variances as <span class="math inline">\(\var{\hat\mu}=\sigma_\mu^2\)</span> and <span class="math inline">\(\var{\hat\tau_i}=\sigma_{\tau_i}^2\)</span>, and the corresponding unbiased estimators of their variances are denoted as <span class="math inline">\(S_\mu^2\)</span> or <span class="math inline">\(\hat\sigma^2_\mu\)</span> and <span class="math inline">\(S_{\tau_i}^2\)</span> or <span class="math inline">\(\hat\sigma^2_{\tau_i}\)</span>. With these estimators confidence intervals of the parameters and hypothesis tests about the parameters can be constructed.</p>
</div>
<div id="S:ANOVA1SS" class="section level3" number="5.1.4">
<h3><span class="header-section-number">5.1.4</span> Sum of Squares</h3>
<p>Because of the equivalence of the ANOVA and regression models, the sum of squares are the same for both models. For hystorical reasons and because of the specific notation and meaning of the ANOVA models, the sum of squares are formulated differently (though numerically equal).</p>
<p>The <strong>total sum of squares</strong> (SSTot) remains unchanged, but in ANOVA notation this becomes
<span class="math display">\[
 \SSTot = \sum_{i=1}^t \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y})^2.
\]</span>
So it measures the variability of the sample outcomes without accounting for the model (treatment groups); it is related to the marginal distribution of the outcomes.
The number of degrees of freedom is <span class="math inline">\(n-1\)</span> and <span class="math inline">\(\SSTot/(n-1)\)</span> is the sample variance of the outcome. It is an estimator of the variance of the marginal outcome distribution.</p>
<p>The <strong>residual sum of squares</strong> is written as
<span class="math display">\[
  \SSE= \sum_{i=1}^t \sum_{j=1}^{n_i} (Y_{ij} - \hat{Y}_{ij})^2 =\sum_{i=1}^t \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2,
\]</span>
which we can also write as
<span class="math display">\[
  \SSE= \sum_{i=1}^t (n_i-1) \left(\frac{1}{n_i-1} \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2\right)
  =  \sum_{i=1}^t (n_i-1) S_i^2
\]</span>
with <span class="math inline">\(S_i^2\)</span> the sample variance in group <span class="math inline">\(i\)</span>. <span class="math inline">\(\MSE=\SSE/(n-t)\)</span> is thus the weighted average of the sample variances <span class="math inline">\(S_i^2\)</span> and since all treatment-specific variances are assumed to coincide, MSE is the <strong>pooled</strong> variance estimator of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>In the context of ANOVA the <strong>sum of squares of the regression</strong> (SSR) is renamed to SST, the <strong>sum of squares of the treatment</strong> or the <strong>treatment sum of squares</strong>. Sometimes it is also named the <strong>between sum of squares</strong> (SSBetween); this refers to the sum of squares as a measure of the variability between the groups.
We write
<span class="math display">\[
  \SST = \sum_{i=1}^t \sum_{j=1}^{n_i} (\hat{Y}_{ij}-\bar{Y})^2 = \sum_{i=1}^t \sum_{j=1}^{n_i} (\bar{Y}_{i}-\bar{Y})^2 = \sum_{i=1}^t n_i (\bar{Y}_i - \bar{Y})^2.
\]</span>
This expression shows that it is a measure for the variability of the treatment-specific sample means <span class="math inline">\(\bar{Y}_i\)</span> about the overall sample mean <span class="math inline">\(\bar{Y}\)</span>. If SST is small, then all sample means are close to one another and thus also close to the overall sample mean. If SST is large, then at least one treatment-specific sample mean is far away from the overall sample mean and this gives evidence that <span class="math inline">\(H_0: \tau_1=\cdots =\tau_t=0\)</span> is not true.</p>
<p>SST has <span class="math inline">\(t-1\)</span> degrees of freedom and the corresponding mean sum of squares thus becomes <span class="math inline">\(\MST=\SST/(t-1)\)</span>.</p>
<p>The decomposition of the total sum of squares still holds and is now written as
<span class="math display">\[
  \SSTot = \SST + \SSE.
\]</span></p>
</div>
</div>
<div id="S:FTest" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> The <span class="math inline">\(F\)</span>-test</h2>
<div id="introduction-2" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Introduction</h3>
<p>In Section <a href="#S:ANOVA1Mu3">5.1.1</a> we have translated the research question into the hypotheses
<span class="math display">\[
  H_0: \mu_1=\mu_2=\mu_3 \text{ versus } H_1: \text{ not } H_0.
\]</span></p>
<p>Based on Model <a href="#eq:Regmu3">(5.1)</a> this null hypothesis can be reformulated as
<span class="math display">\[
  H_0: \beta_1=\beta_2=0.
\]</span>
From the previous chapters we know that each <span class="math inline">\(\beta\)</span>-parameter can be separately tested (e.g. <span class="math inline">\(H_0: \beta_1=0\)</span>) via a <span class="math inline">\(t\)</span>-test, but in this section we will develop a method that allows us to test hypotheses that involve multiple <span class="math inline">\(\beta\)</span>-parameters with only a single test: the <span class="math inline">\(F\)</span>-test.</p>
</div>
<div id="the-general-linear-hypothesis" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> The General Linear Hypothesis</h3>
<p>The <span class="math inline">\(F\)</span>-test that we will develop here, is more general, i.e. it can also be used for testing hypotheses other than those of the form <span class="math inline">\(H_0: \beta_1=\beta_2=0\)</span>.</p>
<p>In this section the <span class="math inline">\(\beta\)</span>-parameters refer to the parameters in the multiple linear regression model.</p>
<p>Let</p>
<ul>
<li><p><span class="math inline">\(\mb{A}\)</span> denote an <span class="math inline">\(m\times p\)</span> matrix, with <span class="math inline">\(1\leq m \leq p\)</span> and rank(<span class="math inline">\(\mb{A}\)</span>)=<span class="math inline">\(m\)</span></p></li>
<li><p><span class="math inline">\(\mb{a}\)</span> an <span class="math inline">\(m\times 1\)</span> constant vector.</p></li>
</ul>
<p>Consider now the hypotheses
<span class="math display" id="eq:ALH">\[\begin{equation}
  \tag{5.4}
  H_0: \mb{A}\mb\beta = \mb{a} \text{ versus } H_1: \mb{A}\mb\beta \neq \mb{a}.
\end{equation}\]</span></p>
<p>This null hypothesis is known as the <strong>general linear hypothesis</strong>.</p>
<p>With this formulation the null hypothesis <span class="math inline">\(H_0:\beta_1=\beta_2=0\)</span> can be rewritten as
<span class="math display">\[
  \mb{A} = \begin{pmatrix}
    0 &amp; 1 &amp; 0 \\
    0 &amp; 0 &amp; 1
  \end{pmatrix}
\]</span>
and <span class="math inline">\(\mb{a}^t=(0,0)\)</span>.</p>
<p>In the PWD example with only the vaccination groups C, D and E, we could. e.g. be interested in comparing the limited high protein diet (C) with the full high protein diets (D and E). With the dummy coding with C as the reference group, the null hypothesis can be written as <a href="#eq:ALH">(5.4)</a> with
<span class="math display">\[
  \mb{A} = \begin{pmatrix}
    1 &amp; -1/2 &amp; -1/2 
  \end{pmatrix}
\]</span>
and <span class="math inline">\(\mb{a}=0\)</span>.</p>
<p>More complex examples will be given later, but first we introduce the <span class="math inline">\(F\)</span>-test for the general linear hypothesis.
We will do so in the following steps:</p>
<ol style="list-style-type: decimal">
<li><p>provide an expression for the <strong>restricted LSE</strong> of <span class="math inline">\(\mb\beta\)</span> under the general linear hypothesis (i.e. a LSE of <span class="math inline">\(\mb\beta\)</span> that satisfies the general linear hypothesis)</p></li>
<li><p>provide an expression for the <strong>restricted SSE</strong> (i.e. the SSE obtained under the general linear hypothesis)</p></li>
<li><p>provide the <span class="math inline">\(F\)</span>-test statistic and its null distribution.</p></li>
</ol>
<p><strong>Step 1</strong>: The restricted LSE is given by (without proof)
<span class="math display">\[
  \hat{\mb\beta}_0=\hat{\mb\beta}+(\mb{X}^t\mb{X})^{-1}\mb{A}^t[\mb{A}(\mb{X}^t\mb{X})^{-1}\mb{A}^t]^{-1}(\mb{a}-\mb{A}\hat{\mb\beta}). 
\]</span>
Thus for this estimator it holds that
<span class="math display">\[
  \mb{A}\hat{\mb\beta}_0 = \mb{a}. 
\]</span></p>
<p><strong>Step 2</strong>: The restricted SSE is given by (without proof)
<span class="math display">\[
  \SSE_0 = \Vert \mb{Y}-\mb{X}\hat{\mb\beta}_0\Vert^2 = \SSE + (\mb{A}\hat{\mb\beta}-\mb{a})^t[\mb{A}(\mb{X}^t\mb{X})^{-1}\mb{A}^t]^{-1}(\mb{A}\hat{\mb\beta}-\mb{a}).
\]</span></p>
<p><strong>Step 3</strong>: The <span class="math inline">\(F\)</span>-test statistic for testing <span class="math inline">\(H_0: \mb{A\beta}=\mb{a}\)</span> is given by</p>
<p><span class="math display" id="eq:FStat">\[\begin{equation}
   F = \frac{(\SSE_0-\SSE)/m}{\MSE}. 
   \tag{5.5}
\end{equation}\]</span></p>
<p>It can be shown (here without proof) that under the general linear hypothesis,
<span class="math display">\[
F \sim F_{m,n-p}
\]</span>
with <span class="math inline">\(p\)</span> the number of <span class="math inline">\(\beta\)</span>-parameters in the linear model.</p>
<p>In the special case of the one-way ANOVA, the general linear hypothesis for
<span class="math display">\[
  H_0: \beta_1=\cdots = \beta_{t-1}=0
\]</span>
the <span class="math inline">\(F\)</span>-test statistic reduces to
<span class="math display">\[
  F = \frac{\SST/(t-1)}{\MSE}
\]</span>
with null distribution <span class="math inline">\(F_{t-1,n-t}\)</span>.</p>
<p>It can be shown that for <span class="math inline">\(t=2\)</span> (i.e. the comparison of two means, <span class="math inline">\(\beta_1=0\)</span>), the <span class="math inline">\(F\)</span>-test statistic is the square of the <span class="math inline">\(t\)</span>-test statistic for testing <span class="math inline">\(\beta_1=0\)</span>. The <span class="math inline">\(p\)</span>-values of the <span class="math inline">\(F\)</span>-test and the two-sided <span class="math inline">\(t\)</span>-tests coincide in this case.</p>
</div>
</div>
<div id="example-pwd-1" class="section level2 unnumbered">
<h2>Example (PWD)</h2>
<p>Consider again the PWD example with only treatment groups C, D and E. Suppose we want to assess whether the three ADWG means are equal, then we know that this can be tested with the <span class="math inline">\(F\)</span>-statistic
<span class="math display">\[
  F = \frac{\SST/(t-1)}{\MSE}.
\]</span></p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="#cb526-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0021<span class="sc">~</span>Treatment, <span class="at">data=</span>PWD3)</span>
<span id="cb526-2"><a href="#cb526-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: ADWG0021
##           Df Sum Sq Mean Sq F value Pr(&gt;F)
## Treatment  2  319.6  159.81  0.5691 0.5745
## Residuals 21 5897.0  280.81</code></pre>
<p>The <span class="math inline">\(F\)</span>-statistic is equal to <span class="math inline">\(0.5691\)</span>, with <span class="math inline">\(p=0.5745\)</span>. Hence, at the <span class="math inline">\(5\%\)</span> level of signicance we conclude that there is no evidence against the null hypothesis of equal ADWG among the three vaccination treatment groups.</p>
<p>We now analyse the full PWD dataset with all of its five treatment groups. The null hypothesis of interest is that the ADWG is the same for all 5 treatments.</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="#cb528-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0021<span class="sc">~</span>Treatment, <span class="at">data=</span>PWD)</span>
<span id="cb528-2"><a href="#cb528-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: ADWG0021
##           Df Sum Sq Mean Sq F value Pr(&gt;F)
## Treatment  4   2771  692.75  2.1048 0.1011
## Residuals 35  11520  329.13</code></pre>
<p>Now the <span class="math inline">\(F\)</span>-statistic equals <span class="math inline">\(2.1048\)</span> with a <span class="math inline">\(p\)</span>-value of <span class="math inline">\(0.1011\)</span>. This <span class="math inline">\(p\)</span>-value still does not reach statistical significance at the 5% level of significance, but the <span class="math inline">\(p\)</span>-value is rather small than large.</p>
<p>A similar analysis on the ADWG 50 days post weaning gives the following results.</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="#cb530-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment, <span class="at">data=</span>PWD)</span>
<span id="cb530-2"><a href="#cb530-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: ADWG0050
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Treatment  4  14235  3558.7  3.9011 0.01009 *
## Residuals 35  31928   912.2                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From this analysis, at the 5% level of significance, we can conclude that at least two treatment groups show different ADWGs (<span class="math inline">\(p=0.01\)</span>). The <span class="math inline">\(F\)</span>-test, however, does not tell us what groups show different mean outcomes. Later, in Section <a href="#S:Multiplicity">5.6</a>, we will see methods to formally identify these groups. A first indication can be given by looking at the individual parameter estimates.</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="#cb532-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -78.307 -17.500   2.547  16.453  53.568 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   337.58      10.68  31.613   &lt;2e-16 ***
## TreatmentB    -17.77      15.10  -1.177   0.2472    
## TreatmentC     25.73      15.10   1.704   0.0973 .  
## TreatmentD     26.41      15.10   1.749   0.0891 .  
## TreatmentE     30.94      15.10   2.049   0.0481 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 30.2 on 35 degrees of freedom
## Multiple R-squared:  0.3084, Adjusted R-squared:  0.2293 
## F-statistic: 3.901 on 4 and 35 DF,  p-value: 0.01009</code></pre>
<p>This output shows that R has chosen treatment group A as the reference group. The default choice of R is the factor level that is ranked first in alphabetical order. Thus, the estimate of the intercept, <span class="math inline">\(\hat\mu=\hat\beta_0=337.58\)</span>, is the estimate of the ADWG of piglets that receive normal feed with the addition of ZnO. The other parameter esimates represent the difference in ADWG of the corresponding treatment as compared to the treatment A.</p>
</div>
<div id="exercise-pwd---fcr" class="section level2 unnumbered">
<h2>Exercise: PWD - FCR</h2>
<p>For this exercise we consider the PWD example with the food conversion rates as outcomes. The data is given in the <em>PWD2</em> dataset.</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;Data/PWD2.RData&quot;</span>)</span>
<span id="cb534-2"><a href="#cb534-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(PWD2)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    20 obs. of  7 variables:
##  $ Feeder : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Feed   : chr  &quot;A&quot; &quot;C&quot; &quot;E&quot; &quot;B&quot; ...
##  $ Sex    : num  1 1 1 1 1 2 2 2 2 2 ...
##  $ W0     : num  221 208 208 214 212 ...
##  $ FCR0021: num  1.85 2.1 1.94 2.4 2.03 ...
##  $ FCR2150: num  1.48 1.23 1.32 1.45 1.3 ...
##  $ FCR0050: num  1.55 1.37 1.43 1.6 1.42 ...</code></pre>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="#cb536-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(PWD2)</span></code></pre></div>
<pre><code>##   Feeder Feed Sex    W0  FCR0021  FCR2150  FCR0050
## 1      1    A   1 221.0 1.850701 1.479482 1.549167
## 2      2    C   1 207.5 2.100263 1.233141 1.367305
## 3      3    E   1 207.5 1.942063 1.322581 1.427212
## 4      4    B   1 214.0 2.396541 1.450472 1.599851
## 5      5    D   1 212.5 2.025250 1.304099 1.419576
## 6      6    A   2 205.5 1.757930 1.478463 1.536878</code></pre>
<p>The variable <em>Feed</em> is here the treatment variable. We will first make it a factor variable in R.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="#cb538-1" aria-hidden="true" tabindex="-1"></a>PWD2<span class="sc">$</span>Feed<span class="ot">&lt;-</span><span class="fu">as.factor</span>(PWD2<span class="sc">$</span>Feed)</span>
<span id="cb538-2"><a href="#cb538-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(PWD2)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    20 obs. of  7 variables:
##  $ Feeder : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Feed   : Factor w/ 5 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 1 3 5 2 4 1 3 5 2 4 ...
##  $ Sex    : num  1 1 1 1 1 2 2 2 2 2 ...
##  $ W0     : num  221 208 208 214 212 ...
##  $ FCR0021: num  1.85 2.1 1.94 2.4 2.03 ...
##  $ FCR2150: num  1.48 1.23 1.32 1.45 1.3 ...
##  $ FCR0050: num  1.55 1.37 1.43 1.6 1.42 ...</code></pre>
<p>Analyse the <em>FCR0050</em> outcome. Does treatment has an effect on the average food conversion rate 50 days post-weaning? Interpret the parameter that is associated with the effect of treatment (feed) D and give a 95% confidence interval of this effect parameter</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="#cb540-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(FCR0050<span class="sc">~</span>Feed,<span class="at">data=</span>PWD2)</span>
<span id="cb540-2"><a href="#cb540-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: FCR0050
##           Df   Sum Sq   Mean Sq F value  Pr(&gt;F)  
## Feed       4 0.083947 0.0209867  4.7114 0.01157 *
## Residuals 15 0.066817 0.0044545                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="#cb542-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FCR0050 ~ Feed, data = PWD2)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.105678 -0.022160 -0.003611  0.015090  0.190740 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.54970    0.03337  46.439   &lt;2e-16 ***
## FeedB        0.03589    0.04719   0.760   0.4588    
## FeedC       -0.07672    0.04719  -1.626   0.1248    
## FeedD       -0.12935    0.04719  -2.741   0.0152 *  
## FeedE       -0.11605    0.04719  -2.459   0.0266 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.06674 on 15 degrees of freedom
## Multiple R-squared:  0.5568, Adjusted R-squared:  0.4386 
## F-statistic: 4.711 on 4 and 15 DF,  p-value: 0.01157</code></pre>
<p>From the ANOVA table we read the results of the <span class="math inline">\(F\)</span>-test for testing no-treatment effect on the average <em>FCR0050</em>. The <span class="math inline">\(p\)</span>-value equals <span class="math inline">\(0.0116\)</span>. Hence, at the 5% level of significance we conclude that not all treatments give the same average <em>FCR0050</em>.</p>
<p>From the <em>summary</em> function we can read the parameter estimate that corresponds to treatment D. The parameter is estimated as <span class="math inline">\(-0.129\)</span>. Since treatment A acts as the reference group, the parameter estimate is interpreted as follows: the FCR 50 days post-weaning is estimated to be on average <span class="math inline">\(0.129\)</span> kg growth / kg feed smaller among piglets in group D (vaccination, with high protein/energy feedi in all three phases) than among piglets in group A (normal feed with ZnO). This comes with a standard error of <span class="math inline">\(0.047\)</span> kg growth / kg feed. At the 5% level of signifcance this effect is significantly different from zero (<span class="math inline">\(p=0.0152\)</span>).</p>
<p>With the standard error, can calculate the 95% CI of the effect parameter:
<span class="math display">\[
  [-0.12935- 0.04719  t_{15;0.975}, -0.12935+ 0.04719  t_{15;0.975}] = [-0.2299331,-0.0287669].
\]</span>
Hence, we with a probability of 95% we expect that in treatment group D the average FCR 50 days post-weaning is <span class="math inline">\(29\)</span> to <span class="math inline">\(230\)</span> g growth / kg feed smaller than in treatment group A.</p>
</details>
</div>
<div id="contrasts" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Contrasts</h2>
<div id="setting-the-reference-level" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Setting the reference level</h3>
<p>Sometimes the reference level chosen by R is not the best choice in the sense that the parameters in the model do not have the most informative interpretation in terms of the research question.
In the PWD example, we can consider treatment B (normal feed) as the standard feed and hence comparison of the other treatments with B makes more sense than comparing all the treatments with group A (which is the default reference group in R). The next R code shows how the dummy coding can be changed.</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="#cb544-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(PWD<span class="sc">$</span>Treatment)</span></code></pre></div>
<pre><code>##   B C D E
## A 0 0 0 0
## B 1 0 0 0
## C 0 1 0 0
## D 0 0 1 0
## E 0 0 0 1</code></pre>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="#cb546-1" aria-hidden="true" tabindex="-1"></a>contr.NRef<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb546-2"><a href="#cb546-2" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb546-3"><a href="#cb546-3" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,</span>
<span id="cb546-4"><a href="#cb546-4" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,</span>
<span id="cb546-5"><a href="#cb546-5" aria-hidden="true" tabindex="-1"></a>                    <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>),<span class="at">nrow=</span><span class="dv">5</span>,<span class="at">byrow=</span>T)</span>
<span id="cb546-6"><a href="#cb546-6" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(contr.NRef)<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;C&quot;</span>,<span class="st">&quot;D&quot;</span>,<span class="st">&quot;E&quot;</span>)</span>
<span id="cb546-7"><a href="#cb546-7" aria-hidden="true" tabindex="-1"></a>contr.NRef</span></code></pre></div>
<pre><code>##      A C D E
## [1,] 1 0 0 0
## [2,] 0 0 0 0
## [3,] 0 1 0 0
## [4,] 0 0 1 0
## [5,] 0 0 0 1</code></pre>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="#cb548-1" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(PWD<span class="sc">$</span>Treatment)<span class="ot">&lt;-</span>contr.NRef</span>
<span id="cb548-2"><a href="#cb548-2" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment, <span class="at">data=</span>PWD)</span>
<span id="cb548-3"><a href="#cb548-3" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: ADWG0050
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)  
## Treatment  4  14235  3558.7  3.9011 0.01009 *
## Residuals 35  31928   912.2                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="#cb550-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -78.307 -17.500   2.547  16.453  53.568 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   319.81      10.68  29.949  &lt; 2e-16 ***
## TreatmentA     17.77      15.10   1.177  0.24723    
## TreatmentC     43.50      15.10   2.881  0.00674 ** 
## TreatmentD     44.18      15.10   2.925  0.00600 ** 
## TreatmentE     48.71      15.10   3.225  0.00273 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 30.2 on 35 degrees of freedom
## Multiple R-squared:  0.3084, Adjusted R-squared:  0.2293 
## F-statistic: 3.901 on 4 and 35 DF,  p-value: 0.01009</code></pre>
<p>Every column of the contrast matrix <em>contr.NRef</em> represents a dummy regressor. The matrix indicates for which factor level the dummy is set to 1. The order of the lines of the matrix is the alphabetical order of the factor levels.</p>
<p>Note that the <span class="math inline">\(F\)</span>-test in the ANOVA table is the same as for the original dummy coding. This is because the null hypotheses for both dummy codings imply the same model: <span class="math inline">\(\E{Y\mid \text{treatment } j} = \mu\)</span>, for all <span class="math inline">\(j=1,2,\ldots, 5\)</span>.</p>
<p>From the output of the <em>summary</em> function we read that the ADWG is the smallest in the reference group B (normal feed). All other treatments show on average a larger ADWG. Each of the <span class="math inline">\(p\)</span>-values that correspond to testing the hypotheses <span class="math inline">\(H_0:\tau_i=0\)</span> versus <span class="math inline">\(H_0:\tau_i\neq 0\)</span> for the vaccination groups C, D and E, is smaller than <span class="math inline">\(\alpha=0.05\)</span> and hence we are tempted to conclude that the vaccinated piglets show a larger ADWG 50 days post-weaning than piglets that were not vaccinated and received a normal diet. However, in Section <a href="#S:Multiplicity">5.6</a> we will argue that this procedure is not allowed.</p>
</div>
<div id="examples-of-other-contrasts" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Examples of other contrasts</h3>
<p>The contrasts in the matrix <span class="math inline">\(\mb{A}\)</span> that make up the general linear hypothesis can also be used for estimating and testing other interesting effects.</p>
<p>In the PWD example, we could be interested in comparing the ADWG of vaccinated piglets (groups C, D and E) with the ADWG of the piglets that were not vaccinated (groups A and B). So we are interested in the contrast
<span class="math display">\[
  \frac{1}{3}(\mu_3+\mu_4+\mu_5) - \frac{1}{2}(\mu_1+\mu_2).
\]</span>
In terms of the <span class="math inline">\(\tau\)</span>-parameters, and with (the default) reference group A, this contrast can be written as
<span class="math display">\[
  \frac{1}{3}((\mu+\tau_3)+(\mu+\tau_4)+(\mu+\tau_5)) - \frac{1}{2}((\mu+0)+(\mu+\tau_2))
\]</span>
which simplifies to
<span class="math display">\[
  \frac{1}{3}(\tau_3+\tau_4+\tau_5) - \frac{1}{2}(\tau_2).
\]</span>
In terms of the <span class="math inline">\(\beta\)</span>-parameters this becomes
<span class="math display">\[
  \frac{1}{3}(\beta_2+\beta_3+\beta_4) - \frac{1}{2}(\beta_1).
\]</span>
Hence, this contrast is obtained as <span class="math inline">\(\mb{A}\mb\beta\)</span> with
<span class="math display">\[
\mb{A}=(0, -1/2, 1/3, 1/3, 1/3).
\]</span></p>
<p>The null hypothesis of interest can be formulated as a general linear hypothesis, <span class="math inline">\(H_0: \mb{A}\mb\beta=\mb{a}\)</span> with <span class="math inline">\(\mb{a}=0\)</span>. This can be tested with the <span class="math inline">\(F\)</span>-test from Section <a href="#S:FTest">5.2</a>. Moreover, since <span class="math inline">\(m=1\)</span> in this case (i.e. <span class="math inline">\(\mb{A}\)</span> has only one row), the null hypothesis can also be tested with a <span class="math inline">\(t\)</span>-test. This is demonstrated in the next chunck of R code.</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="#cb552-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reload the data so that the dummy-defining constrasts are set back to default</span></span>
<span id="cb552-2"><a href="#cb552-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="at">file=</span><span class="st">&quot;Data/PWD.RData&quot;</span>) </span>
<span id="cb552-3"><a href="#cb552-3" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment,<span class="at">data=</span>PWD)</span>
<span id="cb552-4"><a href="#cb552-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb552-5"><a href="#cb552-5" aria-hidden="true" tabindex="-1"></a>A<span class="ot">&lt;-</span><span class="fu">rbind</span>(<span class="st">&quot;vaccination - no vaccination&quot;</span><span class="ot">=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>))</span>
<span id="cb552-6"><a href="#cb552-6" aria-hidden="true" tabindex="-1"></a>vaccination<span class="ot">&lt;-</span><span class="fu">glht</span>(m,<span class="at">linfct=</span>A)</span>
<span id="cb552-7"><a href="#cb552-7" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(vaccination,<span class="at">test=</span><span class="fu">univariate</span>())</span></code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Linear Hypotheses:
##                                   Estimate Std. Error t value Pr(&gt;|t|)    
## vaccination - no vaccination == 0   36.576      9.748   3.752 0.000635 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Univariate p values reported)</code></pre>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="#cb554-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(vaccination,<span class="at">calpha =</span> <span class="fu">univariate_calpha</span>())</span></code></pre></div>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Fit: lm(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Quantile = 2.0301
## 95% confidence level
##  
## 
## Linear Hypotheses:
##                                   Estimate lwr     upr    
## vaccination - no vaccination == 0 36.5764  16.7870 56.3658</code></pre>
<p>Conclusion: we estimate that vaccinated piglets show on average an ADWG that is <span class="math inline">\(36.6\)</span> g/day (SE <span class="math inline">\(9.7\)</span> g/day) larger than piglets that were not vaccinated (50 days post-weaning). The 95% confidence interval of this effect goes from <span class="math inline">\(16.8\)</span> g/day to <span class="math inline">\(56.4\)</span> g/day. This effect is signicant at the 5% level of significance (<span class="math inline">\(p=0.0006\)</span>).</p>
<p>This analysis made use of the <em>glht</em> function from the <em>multcomp</em> R package. Some of the arguments of the functions make look weird, because the function was also developed for multiple hypotheses testing; in Section <a href="#S:Multiplicity">5.6</a> we will use the function again.</p>
<p>We now redo the analysis, but without the use of the <em>glht</em> function.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="#cb556-1" aria-hidden="true" tabindex="-1"></a>A<span class="ot">&lt;-</span><span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>,<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="sc">/</span><span class="dv">3</span>),<span class="at">nrow=</span><span class="dv">1</span>)</span>
<span id="cb556-2"><a href="#cb556-2" aria-hidden="true" tabindex="-1"></a>beta.hat<span class="ot">&lt;-</span><span class="fu">coef</span>(m)</span>
<span id="cb556-3"><a href="#cb556-3" aria-hidden="true" tabindex="-1"></a>vaccination<span class="ot">&lt;-</span>A<span class="sc">%*%</span>beta.hat</span>
<span id="cb556-4"><a href="#cb556-4" aria-hidden="true" tabindex="-1"></a>vaccination</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 36.57639</code></pre>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="#cb558-1" aria-hidden="true" tabindex="-1"></a>Sigma.beta<span class="ot">&lt;-</span><span class="fu">summary</span>(m)<span class="sc">$</span>cov<span class="sc">*</span><span class="fu">summary</span>(m)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb558-2"><a href="#cb558-2" aria-hidden="true" tabindex="-1"></a>SE<span class="ot">&lt;-</span><span class="fu">sqrt</span>(A<span class="sc">%*%</span>Sigma.beta<span class="sc">%*%</span><span class="fu">t</span>(A))</span>
<span id="cb558-3"><a href="#cb558-3" aria-hidden="true" tabindex="-1"></a>SE</span></code></pre></div>
<pre><code>##         [,1]
## [1,] 9.74796</code></pre>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="#cb560-1" aria-hidden="true" tabindex="-1"></a><span class="co"># lower limit 95% CI</span></span>
<span id="cb560-2"><a href="#cb560-2" aria-hidden="true" tabindex="-1"></a>vaccination <span class="sc">-</span> SE<span class="sc">*</span><span class="fu">qt</span>(<span class="fl">0.975</span>,<span class="at">df=</span><span class="dv">35</span>)</span></code></pre></div>
<pre><code>##          [,1]
## [1,] 16.78698</code></pre>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="#cb562-1" aria-hidden="true" tabindex="-1"></a><span class="co"># upper limit 95% CI</span></span>
<span id="cb562-2"><a href="#cb562-2" aria-hidden="true" tabindex="-1"></a>vaccination <span class="sc">+</span> SE<span class="sc">*</span><span class="fu">qt</span>(<span class="fl">0.975</span>,<span class="at">df=</span><span class="dv">35</span>)</span></code></pre></div>
<pre><code>##         [,1]
## [1,] 56.3658</code></pre>
</div>
</div>
<div id="S:ANOVA2" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> The Two-way ANOVA</h2>
<div id="the-additive-model" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> The additive model</h3>
<p>The mean outcome may sometimes depend on more than just a single factor. In the PWD example, we may also be interested to assess the effect of the gender of the piglets on the ADWG, or assess whether or not the effect of vaccination is moderated by the gender (interaction effect).</p>
<p>In this section we will use T and A to refer to the two factor variables. Factor <span class="math inline">\(T\)</span> has <span class="math inline">\(t\)</span> levels, and factor <span class="math inline">\(A\)</span> has <span class="math inline">\(a\)</span> levels.</p>
<p>The <strong>additive two-way ANOVA model</strong> is then given by
<span class="math display">\[
  Y_{ijk} = \mu + \tau_i + \alpha_j + \eps_{ijk} \;\;i=1,\ldots, t; j=1,\ldots, a; k=1,\ldots, n_{ij}
\]</span>
with</p>
<ul>
<li><p><strong>error term</strong> <span class="math inline">\(\eps_{ijk} \iid N(0,\sigma^2)\)</span></p></li>
<li><p><span class="math inline">\(\tau_i\)</span> the <strong>main effect</strong> of treatment <span class="math inline">\(i\)</span> of factor T</p></li>
<li><p><span class="math inline">\(\alpha_j\)</span> the <strong>main effect</strong> of treatment <span class="math inline">\(j\)</span> of factor A</p></li>
<li><p><span class="math inline">\(n_{ij}\)</span> the number of replicates in treatment group <span class="math inline">\((i,j)\)</span>.</p></li>
</ul>
<p>Just as for the one-way ANOVA, we will need restrictions on the parameters. For example,
<span class="math display">\[
  \tau_1=\alpha_1=0.
\]</span>
Thus the first level of T and the first level of A act as the reference groups. Other restrictions are possible, but we will always use this restriction.</p>
</div>
<div id="exercise-interpretation-of-the-parameters" class="section level3 unnumbered">
<h3>Exercise: interpretation of the parameters</h3>
<p>Demonstrate that the additive two-way ANOVA model implies that the effects of factor T are the same in all levels of factor A and the other way around.
Make use of the notation <span class="math inline">\(\E{Y \mid T=i, A=j}\)</span> for the mean outcome in the group defined by level <span class="math inline">\(i\)</span> of factor T and level <span class="math inline">\(j\)</span> of factor A.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<p><strong>Main effects of factor T</strong>:</p>
<p>The effect of treatment <span class="math inline">\(i\)</span> of factor T as compared to the mean outcome in the reference group of factor T, within treatment group <span class="math inline">\(j\)</span> of factor A, is given by
<span class="math display">\[
  \E{Y \mid T=i, A=j} - \E{Y \mid T=1, A=j} = (\mu+\tau_i+\alpha_j) - (\mu+\alpha_j) =\tau_i.
\]</span>
This gives immediately the interpretation of parameter <span class="math inline">\(\tau_i\)</span> and it demonstrates that the effects of factor T do not depend on the level of factor A.</p>
<p><strong>Main effects of factor A</strong>:</p>
<p>The effect of treatment <span class="math inline">\(j\)</span> of factor A as compared to the mean outcome in the reference group of factor A, within treatment group <span class="math inline">\(i\)</span> of factor T, is given by
<span class="math display">\[
  \E{Y \mid T=i, A=j} - \E{Y \mid T=i, A=1} = (\mu+\tau_i+\alpha_j) - (\mu+\tau_i) =\alpha_j.
\]</span></p>
<p></detalils></p>
<p>Just as with the one-way ANOVA, the parameters can be estimated by first reformulating the model as a linear regression model. This can be accomplished by the appropriate coding of dummies. For each treatment, a set of dummies can be coded as before. When all these dummies are entered as regressors in a linear regression model, the parameters can be estimated as in Chapter <a href="#Ch:Reg2">3</a>.</p>
</div>
<div id="sum-of-squares" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Sum of Squares</h3>
<p>For the one-sample ANOVA the sum of squares were given in Section <a href="#S:ANOVA1SS">5.1.4</a>. They were essentially the same as for the linear regression model, except that we use different notation (ANOVA model notation with multiple indices) and we often use the notation SST instead of SSR.</p>
<p>Now that we have two factors, we can replace SST by a sum of squares for the (maineffect of) factor T and a sum of squares for the (maineffect of) factor A.</p>
<p>SSTot and SSE remain as before, except that their expressions now involve three summations (over indices <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span>). The degrees of freedom of SSTot is still <span class="math inline">\(n-1\)</span>, and the degrees of freedom of SSE is <span class="math inline">\(n-a-t+1\)</span> (number of observations minus number of <span class="math inline">\(\beta\)</span>-parameters in the regression model).</p>
<p>Here are the new sum of squares:</p>
<ul>
<li><strong>sum of squares for the (main effect of) factor T</strong>:
<span class="math display">\[
\SST = \sum_{i=1}^t \sum_{j=1}^a \sum_{k=1}^{n_{ij}} (\hat{Y}_{i\cdot} - \bar{Y})^2 = \sum_{i=1}^t \sum_{j=1}^a  n_{ij} (\hat{Y}_{i\cdot} - \bar{Y})^2,
\]</span>
in which <span class="math inline">\(\bar{Y}_{i\cdot}\)</span> is the sample mean of all outcomes in treatment group <span class="math inline">\(i\)</span> of factor T.</li>
</ul>
<p>SST measures the variability in the outcomes that can be attributed to the effect of factor T. It comes with <span class="math inline">\(t-1\)</span> degrees of freedom. We also define <span class="math inline">\(\MST=\SST / (t-1)\)</span>.</p>
<ul>
<li><strong>sum of squares for the (main effect of) factor A</strong>:
<span class="math display">\[
\SSA = \sum_{i=1}^t \sum_{j=1}^a \sum_{k=1}^{n_{ij}} (\hat{Y}_{\cdot j} - \bar{Y})^2 = \sum_{i=1}^t \sum_{j=1}^a  n_{ij} (\hat{Y}_{\cdot j} - \bar{Y})^2,
\]</span>
in which <span class="math inline">\(\bar{Y}_{\cdot j}\)</span> is the sample mean of all outcomes in treatment group <span class="math inline">\(j\)</span> of factor A.</li>
</ul>
<p>SSA measures the variability in the outcomes that can be attributed to the effect of factor A. It comes with <span class="math inline">\(a-1\)</span> degrees of freedom. We also define <span class="math inline">\(\MSA=\SSA / (a-1)\)</span>.</p>
<p>Note: in general it does <strong>not</strong> hold that SSTot=SST+SSA+SSE!</p>
</div>
<div id="S:Ftest2" class="section level3" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> <span class="math inline">\(F\)</span>-tests</h3>
<p>The sum of squares turn out to be very useful for the construction of hypothesis tests (<span class="math inline">\(F\)</span>-tests) for testing for the absence of treatment effects (separately for factors T and A).</p>
<p>Some intuition:</p>
<ul>
<li><p>If factor T has no effect on the mean outcome, then we would expect that all sample means <span class="math inline">\(\bar{Y}_{i \cdot}\)</span> are approximately equal. If they would be equal, they would be equal to the overal sample mean <span class="math inline">\(\bar{Y}\)</span>. In this setting, <span class="math inline">\(\SST=\sum_{i=1}^t \sum_{j=1}^a n_{ij} (\hat{Y}_{i\cdot} - \bar{Y})^2\)</span> will be close to zero.</p></li>
<li><p>On the other hand, when factor T has an effect on the mean outcome, then we expect that not all sample means <span class="math inline">\(\bar{Y}_{i \cdot}\)</span> are close to one another, and hence SST will be (substantially) larger than zero.</p></li>
<li><p>In general we may expect that <span class="math inline">\(\MST=\SST/(t-1)\)</span> is proportional to the residual variance <span class="math inline">\(\sigma^2\)</span>.</p></li>
</ul>
<p>From these arguments it comes to no suprise that the following statistic makes sense as a test statistic for testing for no-treatment effect for factor T:
<span class="math display">\[
  F = \frac{\MST}{\MSE}.
\]</span></p>
<p>More formally: consider the hypotheses
<span class="math display">\[
  H_0: \tau_1=\cdots =\tau_t=0 \;\text{ versus }\; H_1: \text{ not } H_0
\]</span>
and assume that the model assumptions hold true (including the normality assumption), then
<span class="math display">\[
  F =\frac{\MST}{\MSE} \stackrel{H_0}{\sim} F_{t-1,n-a-t+1}. 
\]</span></p>
<p>Similarly, consider the hypotheses
<span class="math display">\[
  H_0: \alpha_1=\cdots =\alpha_a=0 \;\text{ versus }\; H_1: \text{ not } H_0
\]</span>
and assume that the model assumptions hold true (including the normality assumption), then
<span class="math display">\[
  F =\frac{\MSA}{\MSE} \stackrel{H_0}{\sim} F_{a-1,n-a-t+1}. 
\]</span></p>
</div>
<div id="example-pwd-2" class="section level3 unnumbered">
<h3>Example (PWD)</h3>
<p>In the PWD example we may be interested in the effect of treatment on the ADWG 50 days post-weaning, but also in the effect of gender. We therefore include both factors in an (additive) ANOVA model.</p>
<div class="sourceCode" id="cb564"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb564-1"><a href="#cb564-1" aria-hidden="true" tabindex="-1"></a><span class="co"># first check if sex is defined as a factor variable</span></span>
<span id="cb564-2"><a href="#cb564-2" aria-hidden="true" tabindex="-1"></a><span class="fu">is.factor</span>(PWD<span class="sc">$</span>Sex)</span></code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<div class="sourceCode" id="cb566"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb566-1"><a href="#cb566-1" aria-hidden="true" tabindex="-1"></a><span class="co"># no, so we make it a factor variable</span></span>
<span id="cb566-2"><a href="#cb566-2" aria-hidden="true" tabindex="-1"></a>PWD<span class="sc">$</span>Sex<span class="ot">&lt;-</span><span class="fu">as.factor</span>(PWD<span class="sc">$</span>Sex)</span>
<span id="cb566-3"><a href="#cb566-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb566-4"><a href="#cb566-4" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment<span class="sc">+</span>Sex, <span class="at">data=</span>PWD)</span>
<span id="cb566-5"><a href="#cb566-5" aria-hidden="true" tabindex="-1"></a><span class="fu">Anova</span>(m, <span class="at">type=</span><span class="st">&quot;III&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: ADWG0050
##             Sum Sq Df  F value    Pr(&gt;F)    
## (Intercept) 795072  1 915.8353 &lt; 2.2e-16 ***
## Treatment    14235  4   4.0992  0.008115 ** 
## Sex           2411  1   2.7771  0.104810    
## Residuals    29517 34                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Note that we made use of the <em>Anova</em> function, which is part of the <em>car</em> R package. This function may give different results than the <em>anova</em> function of R base. The <em>Anova</em> function gives type III sum of squares. This will be explained in Section <a href="#S:TypesSS">5.5.3</a>. You may ignore the line of <em>Intercept</em>.</p>
<p>The result of the <em>Anova</em> function shows an ANOVA table. Actually the table is not complete. There should be an extra line for the total sum of squares. A complete table is shown next.</p>
<table>
<thead>
<tr class="header">
<th>source</th>
<th align="right">df</th>
<th align="right">SS</th>
<th align="right">MS</th>
<th align="right">F</th>
<th align="right">p</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Treatment</td>
<td align="right">4</td>
<td align="right">14235</td>
<td align="right">3558.7</td>
<td align="right">4.0992</td>
<td align="right">0.0081</td>
</tr>
<tr class="even">
<td>Sex</td>
<td align="right">1</td>
<td align="right">2411</td>
<td align="right">2410.9</td>
<td align="right">2.7771</td>
<td align="right">0.1048</td>
</tr>
<tr class="odd">
<td>Residual</td>
<td align="right">34</td>
<td align="right">29517</td>
<td align="right">868.1</td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td>Total</td>
<td align="right">39</td>
<td align="right">46163</td>
<td align="right">1183.65</td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>From this output we conclude:</p>
<ul>
<li><p>Given the gender of the piglets, the treatment has a significant effect on the ADWG at the 5% level of significance (<span class="math inline">\(p=0.008\)</span>), i.e. at least two treatments have a different ADWG (50 days post-weaning).</p></li>
<li><p>Given a particular treatment, we cannot establish a significant gender effect at the 5% level of significance (<span class="math inline">\(p=0.1048\)</span>).</p></li>
</ul>
<p>Note that for this particular data analysis, we do find a decomposition of SSTot:<br />
<span class="math display">\[
\SSTot = \SST + \SSA + SSE = 14235+2411 +29517 = 46163 . 
\]</span>
It can be proven that this only holds for <strong>orthogonal designs</strong>. The next chunck of R code shows how orthogonality can be seen for the PWD dataset.</p>
<div class="sourceCode" id="cb568"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb568-1"><a href="#cb568-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment<span class="sc">+</span>Sex, <span class="at">x=</span>T, <span class="at">data=</span>PWD)</span>
<span id="cb568-2"><a href="#cb568-2" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span>m<span class="sc">$</span>x</span>
<span id="cb568-3"><a href="#cb568-3" aria-hidden="true" tabindex="-1"></a>X<span class="ot">&lt;-</span><span class="fu">scale</span>(X,<span class="at">center =</span> T, <span class="at">scale =</span> F)</span>
<span id="cb568-4"><a href="#cb568-4" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>(X)<span class="sc">%*%</span>X</span></code></pre></div>
<pre><code>##             (Intercept)    TreatmentB    TreatmentC TreatmentD    TreatmentE
## (Intercept)           0  0.000000e+00  0.000000e+00        0.0  0.000000e+00
## TreatmentB            0  6.400000e+00 -1.600000e+00       -1.6 -1.600000e+00
## TreatmentC            0 -1.600000e+00  6.400000e+00       -1.6 -1.600000e+00
## TreatmentD            0 -1.600000e+00 -1.600000e+00        6.4 -1.600000e+00
## TreatmentE            0 -1.600000e+00 -1.600000e+00       -1.6  6.400000e+00
## Sex2                  0  5.551115e-17 -8.326673e-17        0.0  2.775558e-17
##                      Sex2
## (Intercept)  0.000000e+00
## TreatmentB   5.551115e-17
## TreatmentC  -8.326673e-17
## TreatmentD   0.000000e+00
## TreatmentE   2.775558e-17
## Sex2         1.000000e+01</code></pre>
<p>This matrix shows zeroes for all combinations of the dummy for Sex (<em>Sex2</em>) and the dummies for Treatment.
This orthogonality actually follows from the balanced design: the same number of males and females in each treatment group.</p>
<div class="sourceCode" id="cb570"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb570-1"><a href="#cb570-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(PWD<span class="sc">$</span>Sex,PWD<span class="sc">$</span>Treatment)</span></code></pre></div>
<pre><code>##    
##     A B C D E
##   1 4 4 4 4 4
##   2 4 4 4 4 4</code></pre>
</div>
<div id="the-non-additive-anova-model" class="section level3" number="5.4.4">
<h3><span class="header-section-number">5.4.4</span> The non-additive ANOVA model</h3>
<p>The additive ANOVA model implies that the effect of one factor is the same within each level of the other factor. If this is not the case, we have an <strong>interaction effect</strong> of the two factors on the mean response.</p>
<p>The additive model is now extended to
<span class="math display">\[
  Y_{ijk} = \mu + \tau_i + \alpha_j + (\tau\alpha)_{ij} + \eps_{ijk} \;\;i=1,\ldots, t; j=1,\ldots, a; k=1,\ldots, n_{ij}
\]</span>
with</p>
<ul>
<li><p><strong>error term</strong> <span class="math inline">\(\eps_{ijk} \iid N(0,\sigma^2)\)</span></p></li>
<li><p><span class="math inline">\(\tau_i\)</span> the <strong>main effect</strong> of treatment <span class="math inline">\(i\)</span> of factor T</p></li>
<li><p><span class="math inline">\(\alpha_j\)</span> the <strong>main effect</strong> of treatment <span class="math inline">\(j\)</span> of factor A</p></li>
<li><p><span class="math inline">\((\tau\alpha)_{ij}\)</span> the <strong>interaction effect</strong> of treatment <span class="math inline">\(i\)</span> of factor T and treatment <span class="math inline">\(j\)</span> of factor A</p></li>
<li><p><span class="math inline">\(n_{ij}\)</span> the number of replicates in treatment group <span class="math inline">\((i,j)\)</span>.</p></li>
</ul>
<p>Just as for the one-way model, restrictions are required. On the main effect parameters, the same restrictions apply as in the additive model. For the interaction effects we impose the restrictions
<span class="math display">\[
  (\tau\alpha)_{ij}=0 \;\;\text{ for } i=1 \text{ and for } j=1. 
\]</span>
This restriction leaves only <span class="math inline">\((t-1)(a-1)\)</span> interaction effect parameters to be estimated.</p>
<p>This model can again be formulated as a regression model by defining the appropriate dummies. In particular:</p>
<ul>
<li><p>construct the <span class="math inline">\(t-1\)</span> dummies for factor T. Let <span class="math inline">\(X_i^T\)</span> be the generic notation for the dummy for the level <span class="math inline">\(i\)</span> of factor T (<span class="math inline">\(i=2,\ldots, t\)</span>).</p></li>
<li><p>construct the <span class="math inline">\(a-1\)</span> dummies for factor A. Let <span class="math inline">\(X_j^A\)</span> be the generic notation for the dummy for the level <span class="math inline">\(j\)</span> of factor A (<span class="math inline">\(j=2,\ldots, a\)</span>).</p></li>
<li><p>with the dummies defined for the main effects of factors T and A, the dummies for the interaction effects are simply constructed by multiplying the main effects dummies (as we did for multiple linear regression models), <span class="math inline">\(i=2, \ldots, t\)</span>, <span class="math inline">\(j=2, \ldots, a\)</span>,
<span class="math display">\[
X^{TA}_{ij} = X^T_i X^A_j.
\]</span></p></li>
</ul>
<p>Once the model is reformulated as a linear regression model with the dummies as regressors, we can again rely on the results of the previous chapters for parameter estimation and statistical inference.</p>
<p>When interaction is present, the main effect parameters have no longer a clear interpretation (just like for regression models with interaction effects).</p>
</div>
<div id="exercise-interpretation-of-the-parameters-1" class="section level3 unnumbered">
<h3>Exercise: interpretation of the parameters</h3>
<p>Demonstrate that the non-additive two-way ANOVA model implies that the effect of factor T may depend on the levels of factor A, and the other way around.
Make use of the notation <span class="math inline">\(\E{Y \mid T=i, A=j}\)</span> for the mean outcome in the group defined by level <span class="math inline">\(i\)</span> of factor T and level <span class="math inline">\(j\)</span> of factor A.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<p><strong>Effects of factor T</strong>:</p>
<p>The effect of treatment <span class="math inline">\(i\)</span> of factor T as compared to the mean outcome in the reference group of factor T, within treatment group <span class="math inline">\(j\)</span> of factor A, is given by
<span class="math display">\[
  \E{Y \mid T=i, A=j} - \E{Y \mid T=1, A=j} = (\mu+\tau_i+\alpha_j + (\tau\alpha)_{ij}) - (\mu+\alpha_j) =\tau_i + (\tau\alpha)_{ij}.
\]</span>
This demonstrates that the effect of treatment <span class="math inline">\(i\)</span> of factor T depends on the level of factor A.</p>
<p><strong>Effects of factor A</strong>:</p>
The effect of treatment <span class="math inline">\(j\)</span> of factor A as compared to the mean outcome in the reference group of factor A, within treatment group <span class="math inline">\(i\)</span> of factor T, is given by
<span class="math display">\[
  \E{Y \mid T=i, A=j} - \E{Y \mid T=i, A=1} = (\mu+\tau_i+\alpha_j + (\tau\alpha)_{ij}) - (\mu+\tau_i) =\alpha_i + (\tau\alpha)_{ij}.
\]</span>
This demonstrates that the effect of treatment <span class="math inline">\(j\)</span> of factor A depends on the level of factor T.
</details>
</div>
<div id="S:FInteraction" class="section level3" number="5.4.5">
<h3><span class="header-section-number">5.4.5</span> Sum of squares and <span class="math inline">\(F\)</span>-test for interaction</h3>
<p>The definitions of the sum of squares SST, SSA, SSTot and SSE remain as before. We now introduce a new sum of squares for the interaction effect,</p>
<p><span class="math display">\[\begin{eqnarray*}
 \SSTA 
    &amp;=&amp; \sum_{i=1}^t \sum_{j=1}^a \sum_{k=1}^{n_{ij}} \left[ \bar{Y}_{ij} - \left(\bar{Y}+(\bar{Y}_{i\cdot}-\bar{Y}) + (\bar{Y}_{\cdot j}-\bar{Y})\right) \right]^2 \\
    &amp;=&amp; \sum_{i=1}^t \sum_{j=1}^a n_{ij} \left[ \bar{Y}_{ij} - \left(\bar{Y}+(\bar{Y}_{i\cdot}-\bar{Y}) + (\bar{Y}_{\cdot j}-\bar{Y})\right) \right]^2.
\end{eqnarray*}\]</span></p>
<p>In this expression, <span class="math inline">\(\bar{Y}+(\bar{Y}_{i\cdot}-\bar{Y}) + (\bar{Y}_{\cdot j}-\bar{Y})\)</span> is equal to <span class="math inline">\(\hat\mu+\hat\tau_i+\hat\alpha_j\)</span> in which all estimates are estimated under the additive model. Hence, SSTA measures the deviation from the additive model, or, equivalently, it measures the overall interaction effect in the sample data.</p>
<p>This SSTA comes with <span class="math inline">\((t-1)(a-1)\)</span> degrees of freedom. The corresponding mean sum of squares is then given by <span class="math inline">\(\MSTA=\SSTA/[(t-1)(a-1)]\)</span>.</p>
<p>The null hypothesis of no-interaction,
<span class="math display">\[
  H_0: (\tau\alpha)_{ij}=0 \;\;\text{ for all } i=2,\ldots, t; j=2, \ldots, a
\]</span>
versus the alternative <span class="math inline">\(H_1: \text{ not } H_0\)</span>, can be tested with the <span class="math inline">\(F\)</span>-test statistic
<span class="math display">\[
  F= \frac{\MSTA}{\MSE},
\]</span>
and the null distribution (if all model assumptions hold true)
<span class="math display">\[
  F= \frac{\MSTA}{\MSE} \stackrel{H_0}{\sim} F_{(t-1)(a-1), n-ta}.
\]</span></p>
</div>
<div id="exercise-degrees-of-freedom" class="section level3 unnumbered">
<h3>Exercise: degrees of freedom</h3>
<p>The null distribution of the <span class="math inline">\(F\)</span>-test for the interaction effects is an <span class="math inline">\(F\)</span>-distribution. Its denominator degrees of freedom equals <span class="math inline">\(n-ta\)</span>. Explain where this degrees of freedom comes from.</p>
<details>
<summary>
Try to make this exercise yourself. If you are ready you can expand this page and look at a solution
</summary>
<p>The denominator degrees of freedom refer to the degrees of freedom of MSE. In general the degrees of freedom of MSE (or SSE) equals the number of sample observations minus the number of <span class="math inline">\(\beta\)</span>-parameters that need to be estimated (this refers to the regression model formulation of the ANOVA model).</p>
<p>In the ANOVA model with interaction, we have one intercepte <span class="math inline">\(\mu\)</span> parameter, <span class="math inline">\(t-1\)</span> main effect <span class="math inline">\(\tau\)</span>-parameters, <span class="math inline">\(a-1\)</span> main effect <span class="math inline">\(\alpha\)</span>-parameters and <span class="math inline">\((t-1)(a-1)\)</span> interaction <span class="math inline">\((\tau\alpha)_{ij}\)</span>-parameters.</p>
Hence, the residual degrees of freedom equals
<span class="math display">\[
  n - 1- (t-1) - (a-1) - (t-1)(a-1) = n- at. 
\]</span>
</details>
</div>
<div id="example-pwd-3" class="section level3 unnumbered">
<h3>Example (PWD)</h3>
<p>Earlier we have analysed the <em>ADWG0050</em> outcome with an additive model including the main effects of treatment and gender. However, just as for linear regression models, we can only correctly interpret main effects if there is no interaction effect. So we have to check if there is an interaction effect of treatment and gender on the mean ADWG outcome.</p>
<div class="sourceCode" id="cb572"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb572-1"><a href="#cb572-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment<span class="sc">*</span>Sex, <span class="at">data=</span>PWD)</span>
<span id="cb572-2"><a href="#cb572-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Anova</span>(m, <span class="at">type=</span><span class="st">&quot;III&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: ADWG0050
##               Sum Sq Df  F value Pr(&gt;F)    
## (Intercept)   473516  1 564.6088 &lt;2e-16 ***
## Treatment       6662  4   1.9860 0.1221    
## Sex              336  1   0.4011 0.5313    
## Treatment:Sex   4357  4   1.2988 0.2929    
## Residuals      25160 30                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>In this anova table, we only look at the interaction effect (cfr. hierarchical modelling). The <span class="math inline">\(F\)</span>-test gives a <span class="math inline">\(p\)</span>-value of <span class="math inline">\(0.293\)</span>. So at the 5% level of signicance, there seems to be not much evidence in favor of interaction effects. As a consequence, we may remove the interaction terms from the model, and analyse the data with the additive model (which we have done before).</p>
<p>When analysing an outcome as a function of two factor variables, <strong>interaction</strong> plots are often used for data visualisation / exploration.</p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="#cb574-1" aria-hidden="true" tabindex="-1"></a><span class="fu">interaction.plot</span>(PWD<span class="sc">$</span>Treatment,PWD<span class="sc">$</span>Sex,<span class="at">response=</span>PWD<span class="sc">$</span>ADWG0050,</span>
<span id="cb574-2"><a href="#cb574-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;ADWG (g/day) 50 days post-weaning&quot;</span>,</span>
<span id="cb574-3"><a href="#cb574-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trace.label =</span> <span class="st">&quot;gender&quot;</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-173-1.png" width="672" /></p>
<p>If the lines in the interaction plot are approximately parallel, then this suggests that there is no interaction effect. If the lines are not parellel, then this may be an indication for the presence of interaction.</p>
<p>In the interaction plot for <em>ADWG0050</em> we see that the lines are almost parallel, except for the lines between treatments D and E. So this could indicate an interaction effect (gender effect in the treatment E group is oposite from the gender effect in the other treatment groups). However, the <span class="math inline">\(F\)</span>-test for the interaction effects was not significant.</p>
</div>
</div>
<div id="extra-sum-of-squares" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Extra sum of squares</h2>
<div id="the-f-test-in-a-one-way-anova" class="section level3" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> The <span class="math inline">\(F\)</span>-test in a one-way ANOVA</h3>
<p>In this section we provide another way of constructing the <span class="math inline">\(F\)</span>-test for testing no-treatment effect, i.e. for testing <span class="math inline">\(\tau_1=\cdots=\tau_t=0\)</span>. Note that the <span class="math inline">\(F\)</span>-test from Section <a href="#S:FTest">5.2</a> is more general and can be used for testing general linear hypotheses.</p>
<p>Consider again the one-way ANOVA model for the mean outcome, <span class="math inline">\(j=1,2,\ldots, t\)</span>,
<span class="math display">\[
  \text{Model (1): } \E{Y \mid \text{treatment }j} = \mu+\tau_j.
\]</span>
The model fit comes with a sum of squared errors (SSE), which will here be denoted as <span class="math inline">\(\SSE_1\)</span>. The corresponding degrees of freedom is denoted as <span class="math inline">\(d_1\)</span>. We will refer to <span class="math inline">\(\SSE_1\)</span> as the <strong>unrestricted</strong> SSE. The notation <span class="math inline">\(\SST_1\)</span> is used for the SST for this model.</p>
<p>Under the null hypothesis of no-treatment effect, the model reduces to, <span class="math inline">\(j=1,2,\ldots, t\)</span>,
<span class="math display">\[
  \text{Model (0): }\E{Y \mid \text{treatment }j} = \mu.
\]</span>
This model fit also comes with a SSE, which is denoted by <span class="math inline">\(\SSE_0\)</span>, and the corresponding degrees of freedom as <span class="math inline">\(d_0\)</span>. We will refer to <span class="math inline">\(\SSE_0\)</span> as the <strong>restricted</strong> SSE (restricted under the null hypothesis). The notation <span class="math inline">\(\SST_0\)</span> is used for the SST for this model.</p>
<p><span class="math inline">\(\SSE_0\)</span> is thus the minimised least squares criterion under the restriction that all treatment groups have the same mean. Hence, <span class="math inline">\(\SSE_0\geq \SSE_1\)</span> (i.e. the restriction cannot result in a better fit of the model).</p>
<p>In this setting, the <strong>extra sum of squares</strong> is defined as
<span class="math display">\[
  \SSR_{1\mid 0} = \SSE_0-\SSE_1.
\]</span>
It is the increase in SSE when moving from model (1) (model with main effects of the factor) to model (0) (model without the main effects).</p>
<p>Intuitive interpretation: if the factor has no effect on the mean outcome, we expect both models (1) and (0) to fit equally well, and hence we expect the extra sum of squares to be close to zero. The larger the effects of the factor (i.e. the larger the differences in the mean outcome between the treatment groups), the larger we expect the extra sum of squares. So, the extra sum of squares seems to make sense for the construction of a test statistic for testing for no-treatment effect.</p>
<p>Since both models have the same SSTot and the decomposition of SSTot applies to both models, we have
<span class="math display">\[
  \SSTot = \SST_1+\SSE_1 = \SST_0+\SSE_0.
\]</span>
Hence, the extra sum of squares can also be written as
<span class="math display">\[
  \SSR_{1\mid 0} = \SST_1-\SST_0.
\]</span></p>
<p>The extra sum of squares also comes with degrees of freedom, which is given by the difference in degrees of freedom between <span class="math inline">\(\SSE_1\)</span> and <span class="math inline">\(\SSE_0\)</span> (or, equivalently, between <span class="math inline">\(\SST_1\)</span> and <span class="math inline">\(\SST_0\)</span>).</p>
<p>In this very simple setting, note that
<span class="math display">\[
  \SST_0 = \sum_{i=1}^t \sum_{j=1}^{n_i} (\bar{Y}_i-\bar{Y})^2 = \sum_{i=1}^t \sum_{j=1}^{n_i} (\bar{Y}-\bar{Y})^2  =0
\]</span>
(because in model (0) all treatment means are restricted to coincide with the overall mean).
Hence, the <span class="math inline">\(F\)</span>-test statistic from Section <a href="#S:FTest">5.2</a> can be written as
<span class="math display">\[
  F = \frac{\MST_1}{\MSE_1}=\frac{\SST_1 / (t-1)}{\MSE_1} = \frac{(\SST_1-\SST_0) / (t-1)}{\MSE_1} = \frac{\SSR_{1 \mid 0} / (t-1)}{\MSE_1}.
\]</span></p>
</div>
<div id="an-f-test-for-nested-models" class="section level3" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> An <span class="math inline">\(F\)</span>-test for nested models</h3>
<p>The extra sum of squares, and a corresponding <span class="math inline">\(F\)</span>-test, can be constructed for <strong>nested models</strong>.
We will introduce the concept for linear regression models, but these also include ANOVA models.</p>
<p>Let <span class="math inline">\(\mb{x}_0\)</span> and <span class="math inline">\(\mb{x}_1\)</span> denote two vectors of regressors (they may also include 0/1 dummy regressors that code for factor variables). Similarly, let <span class="math inline">\(\mb\beta_0\)</span> and <span class="math inline">\(\mb\beta_1\)</span> denote two parameter vectors.</p>
<p>Consider two models:
<span class="math display">\[
  \text{Model (0): } \E{Y \mid \mb{x}_0} = \mu+m_0(\mb{x}_0;\mb\beta_0)
\]</span>
and
<span class="math display">\[
  \text{Model (1): } \E{Y \mid \mb{x}_0, \mb{x}_1} = \mu + m_0(\mb{x}_0;\mb\beta_0) + m_1(\mb{x}_1; \mb\beta_1)
\]</span>
with <span class="math inline">\(m_0\)</span> and <span class="math inline">\(m_1\)</span> representing linear models</p>
<ul>
<li><p>with no intercepts</p></li>
<li><p>such that <span class="math inline">\(m_0(\mb{x}_0;\mb\beta_0)=0\)</span> if <span class="math inline">\(\mb\beta_0=\mb{0}\)</span></p></li>
<li><p>such that <span class="math inline">\(m_1(\mb{x}_1;\mb\beta_1)=0\)</span> if <span class="math inline">\(\mb\beta_1=\mb{0}\)</span></p></li>
<li><p>that come with sum of squares <span class="math inline">\(\SST_1\)</span> and <span class="math inline">\(\SSE_1\)</span> for model (1) and <span class="math inline">\(\SST_0\)</span> and <span class="math inline">\(\SSE_0\)</span> for model (0). The residual degrees of freedom are denoted by <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_0\)</span>.</p></li>
</ul>
<p>Now it can be seen that model (0) <strong>is nested within</strong> model (1): if <span class="math inline">\(\mb\beta_1=\mb{0}\)</span> then model (1) reduces to model (0).</p>
<p>The <strong>extra sum of squares</strong> is now defined as
<span class="math display">\[
  \SSR_{1\mid 0} = \SSE_0-\SSE_1=\SST_1-\SST_0.
\]</span>
It is the extra variablity of the outcome that can be explained by adding <span class="math inline">\(m_1\)</span> to the model that already includes <span class="math inline">\(m_0\)</span>. Its degrees of freedom, which will be denoted by <span class="math inline">\(d\)</span>, is given by the difference in residual degrees of freedom of the two models, i.e. <span class="math inline">\(d=d_0-d_1\)</span>.</p>
<p>We can now construct a test for testing
<span class="math display">\[
   H_0: \mb\beta_1=\mb{0} \;\;\text{ versus }\;\; H_1: \mb\beta_0\neq \mb{0}.
\]</span>
Consider the <span class="math inline">\(F\)</span>-test statistic
<span class="math display">\[
  F = \frac{\SSR_{1\mid 0} / d}{\MSE_1}.
\]</span>
Then, under the assumption that the model assumptions for model (1) hold true, it can be shown that
<span class="math display">\[
  F \stackrel{H_0}{\sim} F_{d;d_1}.
\]</span></p>
<p>Note that the <span class="math inline">\(F\)</span>-test statistic is <em>standardised</em> by dividing by <span class="math inline">\(\MSE_1\)</span> (and not by <span class="math inline">\(\MSE_0\)</span>). This agrees with e.g. the <span class="math inline">\(F\)</span>-test from Section <a href="#S:Ftest2">5.4.3</a> for testing in the two-way ANOVA. However, one could argue that the null distribution must (by definition) only hold under the null hypothesis, and under the null hypothesis also <span class="math inline">\(\MSE_0\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>. Here are some arguments in favor of <span class="math inline">\(\MSE_1\)</span>:</p>
<ul>
<li><p><span class="math inline">\(\MSE_1\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> even if the null hypothesis is not true .</p></li>
<li><p>If the null hypothesis is not true, we expect <span class="math inline">\(\MSE_1&lt;\MSE_0\)</span>, making the <span class="math inline">\(F\)</span>-test statistic larger. This increases the power of the test.</p></li>
</ul>
</div>
<div id="example-pwd-4" class="section level3 unnumbered">
<h3>Example (PWD)</h3>
<p>Consider again the <em>ADWG0050</em> outcome variable. Suppose that we are interested in testing whether the gender has no effect at all on the mean outcome. This implies: no interaction effect of gender and treatment and no main effect of gender.</p>
<p>Then it makes sense to consider the following two nested models:
<span class="math display">\[
  \text{Model (0): } \E{Y \mid \text{treatment } i \text{ and gender } j} = \mu + \tau_i
\]</span>
and
<span class="math display">\[
  \text{Model (1): } \E{Y \mid \text{treatment } i \text{ and gender } j} = \mu + \tau_i + \alpha_j + (\tau\alpha)_{ij}.
\]</span></p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="#cb575-1" aria-hidden="true" tabindex="-1"></a>m0<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment, <span class="at">data=</span>PWD)</span>
<span id="cb575-2"><a href="#cb575-2" aria-hidden="true" tabindex="-1"></a>a0<span class="ot">&lt;-</span><span class="fu">Anova</span>(m0, <span class="at">type=</span><span class="st">&quot;III&quot;</span>)</span>
<span id="cb575-3"><a href="#cb575-3" aria-hidden="true" tabindex="-1"></a>a0</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: ADWG0050
##             Sum Sq Df  F value  Pr(&gt;F)    
## (Intercept) 911672  1 999.4013 &lt; 2e-16 ***
## Treatment    14235  4   3.9011 0.01009 *  
## Residuals    31928 35                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="#cb577-1" aria-hidden="true" tabindex="-1"></a>m1<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Treatment<span class="sc">*</span>Sex, <span class="at">data=</span>PWD)</span>
<span id="cb577-2"><a href="#cb577-2" aria-hidden="true" tabindex="-1"></a>a1<span class="ot">&lt;-</span><span class="fu">Anova</span>(m1, <span class="at">type=</span><span class="st">&quot;III&quot;</span>)</span>
<span id="cb577-3"><a href="#cb577-3" aria-hidden="true" tabindex="-1"></a>a1</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: ADWG0050
##               Sum Sq Df  F value Pr(&gt;F)    
## (Intercept)   473516  1 564.6088 &lt;2e-16 ***
## Treatment       6662  4   1.9860 0.1221    
## Sex              336  1   0.4011 0.5313    
## Treatment:Sex   4357  4   1.2988 0.2929    
## Residuals      25160 30                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="#cb579-1" aria-hidden="true" tabindex="-1"></a>SSR<span class="ot">&lt;-</span>a0[<span class="dv">3</span>,<span class="dv">1</span>]<span class="sc">-</span>a1[<span class="dv">5</span>,<span class="dv">1</span>]</span>
<span id="cb579-2"><a href="#cb579-2" aria-hidden="true" tabindex="-1"></a>d<span class="ot">&lt;-</span>a0[<span class="dv">3</span>,<span class="dv">2</span>]<span class="sc">-</span>a1[<span class="dv">5</span>,<span class="dv">2</span>]</span>
<span id="cb579-3"><a href="#cb579-3" aria-hidden="true" tabindex="-1"></a>FStat<span class="ot">&lt;-</span>(SSR<span class="sc">/</span>d)<span class="sc">/</span>(a1[<span class="dv">5</span>,<span class="dv">1</span>]<span class="sc">/</span>a1[<span class="dv">5</span>,<span class="dv">2</span>])</span>
<span id="cb579-4"><a href="#cb579-4" aria-hidden="true" tabindex="-1"></a>FStat</span></code></pre></div>
<pre><code>## [1] 1.613943</code></pre>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="#cb581-1" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">1</span><span class="sc">-</span><span class="fu">pf</span>(FStat,<span class="at">df1=</span>d,<span class="at">df2=</span>a1[<span class="dv">5</span>,<span class="dv">2</span>])</span>
<span id="cb581-2"><a href="#cb581-2" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<pre><code>## [1] 0.1866552</code></pre>
<p>Hence, at the 5% level of significance, we conclude that there is no evidence for any gender effect on the mean outcome within the treatment groups (<span class="math inline">\(p=0.187\)</span>).</p>
<p>Here is R code that can do this much faster.</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="#cb583-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m0,m1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: ADWG0050 ~ Treatment
## Model 2: ADWG0050 ~ Treatment * Sex
##   Res.Df   RSS Df Sum of Sq      F Pr(&gt;F)
## 1     35 31928                           
## 2     30 25160  5    6767.8 1.6139 0.1867</code></pre>
</div>
<div id="S:TypesSS" class="section level3" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> types of SS</h3>
<p>The SST and SSR sum of squares that we have seen so far are <strong>type III</strong> sum of squares. However, there are also <strong>Type I</strong> and <strong>Type II</strong> sum of squares.</p>
<p>We will explain the different types of SS in the setting with three nested models. Generalisation to more nested models is trivial (by recursion).</p>
<p>Consider three nested models (similar notation as before) with new indexing system:
<span class="math display">\[
  \text{Model (0): } \E{Y \mid \mb{x}_0} = \mu+m_0(\mb{x}_0;\mb\beta_0)
\]</span>
and
<span class="math display">\[
  \text{Model (0,1): } \E{Y \mid \mb{x}_0, \mb{x}_1} = \mu + m_0(\mb{x}_0;\mb\beta_0) + m_1(\mb{x}_1; \mb\beta_1)
\]</span>
and
<span class="math display">\[
  \text{Model (0,1,2): } \E{Y \mid \mb{x}_0, \mb{x}_1, \mb{x}_2} = \mu + m_0(\mb{x}_0;\mb\beta_0) + m_1(\mb{x}_1; \mb\beta_1) + m_2(\mb{x}_2; \mb\beta_2)
\]</span>
such that</p>
<ul>
<li><p><span class="math inline">\(m_0(\mb{x}_0;\mb\beta_0)=0\)</span> if <span class="math inline">\(\mb\beta_0=\mb{0}\)</span></p></li>
<li><p><span class="math inline">\(m_1(\mb{x}_1;\mb\beta_1)=0\)</span> if <span class="math inline">\(\mb\beta_1=\mb{0}\)</span></p></li>
<li><p><span class="math inline">\(m_2(\mb{x}_2;\mb\beta_2)=0\)</span> if <span class="math inline">\(\mb\beta_2=\mb{0}\)</span></p></li>
</ul>
<p>Other models, with the same terms <span class="math inline">\(m_0\)</span>, <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span>:
<span class="math display">\[
  \text{Model (0,2): } \E{Y \mid \mb{x}_0, \mb{x}_2} = \mu + m_0(\mb{x}_0;\mb\beta_0) + m_2(\mb{x}_2; \mb\beta_2)
\]</span>
and
<span class="math display">\[
  \text{Model (1,2): } \E{Y \mid \mb{x}_1, \mb{x}_2} = \mu +  m_1(\mb{x}_1; \mb\beta_1) +m_2(\mb{x}_2;\mb\beta_2).
\]</span>
With these models we can calculate the following extra sum of squares:</p>
<ul>
<li><p><span class="math inline">\(\SSR_{1\mid 0} = \SSE_0-\SSE_{01} = \SST_{01}-\SST_0\)</span></p></li>
<li><p><span class="math inline">\(\SSR_{2\mid 0,1} = \SSE_{01}-\SSE_{012} = \SST_{012}-\SST_{01}\)</span></p></li>
<li><p><span class="math inline">\(\SSR_{2\mid 0} = \SSE_0-\SSE_{02} = \SST_{02}-\SST_0\)</span></p></li>
<li><p><span class="math inline">\(\SSR_{1\mid 0,2}= \SSE_{02}-\SSE_{012} = \SST_{012}-\SST_{02}\)</span></p></li>
<li><p><span class="math inline">\(\SSR_{0\mid 1,2}= \SSE_{12}-\SSE_{012} = \SST_{012}-\SST_{12}\)</span></p></li>
</ul>
<p>Now we are ready to introduce the three types of sum of squares. All sum of squares relate to model (0,1,2).</p>
<p><strong>Type I Sum of Squares</strong></p>
<p>Type I sum of squares are also known as <strong>sequential sum of squares</strong>. They depend on the order in which the terms are added to the model. For example,</p>
<ul>
<li><p>start with a model with only <span class="math inline">\(m_0(\mb{x}_0)\)</span>: model (0)</p></li>
<li><p>proceed by adding the term <span class="math inline">\(m_1(\mb{x}_1)\)</span> to the model: model (0,1)</p></li>
<li><p>proceed by adding the term <span class="math inline">\(m_2(\mb{x}_2)\)</span> to the model: model (0,1,2).</p></li>
</ul>
<p>This gives the following type I sum of squares (they can be denoted by SST or SSR, depending on whether we work in a regression or ANOVA context):</p>
<ul>
<li><p>for <span class="math inline">\(m_0(\mb{x}_0)\)</span>: <span class="math inline">\(\SSR(m_0)=\SST(m_0)=\SST_0=\SSR_0\)</span> (the treatment of regression sum or squares as we have seen them before)</p></li>
<li><p>for <span class="math inline">\(m_1(\mb{x}_1)\)</span>: <span class="math inline">\(\SSR(m_1)=\SST(m_1)=\SSR_{1\mid 0}\)</span>. This measures the extra variability of the outcome that is explained by adding <span class="math inline">\(m_1\)</span> to a model that already includes <span class="math inline">\(m_0\)</span>.</p></li>
<li><p>for <span class="math inline">\(m_2(\mb{x}_2)\)</span>: <span class="math inline">\(\SSR(m_2)=\SST(m_2)=\SSR_{2\mid 0,1}\)</span>. This measures the extra variability of the outcome that is explained by adding <span class="math inline">\(m_2\)</span> to a model that already includes <span class="math inline">\(m_0\)</span> and <span class="math inline">\(m_1\)</span>.</p></li>
</ul>
<p>Type I sum of squares have the following property:
<span class="math display">\[
  \SSR(m_0)+\SSR(m_1)+\SSR(m_2) = \SSR_0 + (\SSR_{01}-\SSR_0) + (\SSR_{012}-\SSR_{01}) = \SSR_{012},
\]</span>
which is the sum of squares of the full model (<span class="math inline">\(m_0+m_1+m_2\)</span>).</p>
</div>
</div>
<div id="example-diabetes-1" class="section level2 unnumbered">
<h2>Example (Diabetes)</h2>
<p>Consider again the Diabetes example for which there were two factor variables (<em>condition</em> and <em>treatment</em>), each with two levels. Previously we manually coded these factors to dummies, but now we will make them factor variables and let R do the dummy coding. In other words, we will consider it a two-way ANOVA.</p>
<div class="sourceCode" id="cb585"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb585-1"><a href="#cb585-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(Diabetes)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    86 obs. of  3 variables:
##  $ condition: int  0 1 0 0 0 0 0 1 0 1 ...
##  $ treatment: int  0 1 0 1 1 0 0 1 0 1 ...
##  $ glucose  : num  2.4 1.8 0.4 1.6 2 1.7 0.8 2.9 1.4 3.7 ...</code></pre>
<div class="sourceCode" id="cb587"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb587-1"><a href="#cb587-1" aria-hidden="true" tabindex="-1"></a>Diabetes<span class="sc">$</span>condition<span class="ot">&lt;-</span><span class="fu">as.factor</span>(Diabetes<span class="sc">$</span>condition)</span>
<span id="cb587-2"><a href="#cb587-2" aria-hidden="true" tabindex="-1"></a>Diabetes<span class="sc">$</span>treatment<span class="ot">&lt;-</span><span class="fu">as.factor</span>(Diabetes<span class="sc">$</span>treatment)</span></code></pre></div>
<p>We analyse the data with type I sum of squares. These sum of squares are computed by the <em>anova</em> function of R base.</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="#cb588-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>condition<span class="sc">+</span>treatment<span class="sc">+</span>condition<span class="sc">:</span>treatment, <span class="at">data=</span>Diabetes)</span>
<span id="cb588-2"><a href="#cb588-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: glucose
##                     Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## condition            1 44.261  44.261 86.0354 2.038e-14 ***
## treatment            1  0.022   0.022  0.0434    0.8355    
## condition:treatment  1  0.551   0.551  1.0711    0.3037    
## Residuals           82 42.185   0.514                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Conclusions:</p>
<ul>
<li><p>at the line of <em>condition</em> we read the results of the <span class="math inline">\(F\)</span>-test for testing no-condition effect in a model that only contains the <em>condition</em> factor. So we are testing the null hypothesis that the mean blood glucose levels are the same in the two condition groups. The <span class="math inline">\(p\)</span>-value is very small (<span class="math inline">\(p&lt;0.0001\)</span>), and hence the null hypothesis is strongly rejected at the 5% level of significance and we conclude that the mean blood glucose level of the patients with a good condition is different from the mean blood glucose level of the patients with a poor condition.</p></li>
<li><p>at the line of <em>treatment</em> we read the results of the <span class="math inline">\(F\)</span>-test for testing no-treatment effect in a model that already contains the main effect of <em>condition</em>. So the null hypothesis is no-treatment effect for patients within the same condition group. With <span class="math inline">\(p=0.8355\)</span> we conclude that is no evidence to reject this null hypothesis.</p></li>
<li><p>at the line of <em>condition:treatment</em> we read the results of the <span class="math inline">\(F\)</span>-test for testing no interaction effect of treatment and condition, in the model that already contains the main effects of treatment and condition. With <span class="math inline">\(p=0.3037\)</span> we conclude that there is insufficient evidence for such an interaction effect.</p></li>
</ul>
<p>Some notes:</p>
<ul>
<li><p>All hypothesis tests make sense (the hierarchy of the effects was respected).</p></li>
<li><p>If we keep the original research question in mind, and if we recall that condition is a confounder and should be in the model anyway, we can see that with this single ANOVA table we can make all required hypothesis tests:</p>
<ul>
<li><p>start with testing for no-interaction effect</p></li>
<li><p>since we have concluded that there is no interaction effect, we may now look at the line of <em>treatment</em>, because this shows the test for no-treatment effect while controlling for confounder condition (and without the interaction in the model)</p></li>
<li><p>we must not necessarily look at the line of <em>condition</em>, because condition, as a confounder, should remain in the model anyway.</p></li>
</ul></li>
<li><p>In the previous point we have argued that we can do valid statistical inference by looking only at a single ANOVA table. There is, however, one important remark to make. Even though at the line of <em>treatment</em> the correct extra sum of squares is used for the calculation of the <span class="math inline">\(F\)</span>-test statistic, you may disagree with the MSE that is used in this calculation! All <span class="math inline">\(F\)</span>-tests presented in the ANOVA table, make use of the same MSE: the MSE of the maximal model, i.e. the model that includes all main effects and the interaction effect. Earlier we have said that when the interaction effect is nonsignificant, the interaction term should be removed from the model and the model should be fit again to the data. That would result in another MSE (a larger MSE than the MSE that is used now).</p></li>
</ul>
<p>Finally, to demonstrate that the <span class="math inline">\(p\)</span>-values depend on the order in which the terms are added to the model, we redo the analysis with another ordering of the terms.</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb590-1"><a href="#cb590-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>treatment<span class="sc">+</span>condition<span class="sc">+</span>condition<span class="sc">:</span>treatment, <span class="at">data=</span>Diabetes)</span>
<span id="cb590-2"><a href="#cb590-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: glucose
##                     Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## treatment            1 17.907 17.9071 34.8081 7.855e-08 ***
## condition            1 26.376 26.3764 51.2708 3.129e-10 ***
## treatment:condition  1  0.551  0.5510  1.0711    0.3037    
## Residuals           82 42.185  0.5145                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This ANOVA table shows a very significant result for treatment. However, the test for no-treatment effect is now the first test in the table and hence it tests for treatment effect in the model without any other terms in it. So the analysis for treatment is not controlled for the confounder <em>condition</em>.</p>
<p>Note that the result for the interaction effect is exactly the same as before. This is because in both analyses the interaction term was added to the model as the last term.</p>
<p><strong>Type III Sum of Squares</strong></p>
<p>Here are the type III sum of squares (they can be denoted by SST or SSR, depending on whether we work in a regression or ANOVA context):</p>
<ul>
<li><p>for <span class="math inline">\(m_0(\mb{x}_0)\)</span>: <span class="math inline">\(\SSR(m_0)=\SST_(m_0)=\SSR_{0 \mid 1,2}\)</span>. This measures the extra variability of the outcome that is explained by adding <span class="math inline">\(m_0\)</span> to a model that already includes <span class="math inline">\(m_1\)</span> and <span class="math inline">\(m_2\)</span>.</p></li>
<li><p>for <span class="math inline">\(m_1(\mb{x}_1)\)</span>: <span class="math inline">\(\SSR(m_1)=\SST(m_1)=\SSR_{1\mid 0,2}\)</span>. This measures the extra variability of the outcome that is explained by adding <span class="math inline">\(m_1\)</span> to a model that already includes <span class="math inline">\(m_0\)</span> and <span class="math inline">\(m_2\)</span>.</p></li>
<li><p>for <span class="math inline">\(m_2(\mb{x}_2)\)</span>: <span class="math inline">\(\SSR(m_2)=\SST(m_2)=\SSR_{2\mid 0,1}\)</span>. This measures the extra variability of the outcome that is explained by adding <span class="math inline">\(m_2\)</span> to a model that already includes <span class="math inline">\(m_0\)</span> and <span class="math inline">\(m_1\)</span>.</p></li>
</ul>
<p>Thus each sum of squares measures the reduction of outcome variability when adding a term to the model that already contains all the other terms. Hence, these sum of squares do not depend on the order the terms are added to the model.
The sum of squares that we have seen earlier in Section <a href="#S:ANOVA2">5.4</a> for the two-way ANOVA are examples of type III sum of squares.</p>
</div>
<div id="example-diabetes-2" class="section level2 unnumbered">
<h2>Example (Diabetes)</h2>
<p>We analyse the data with type III sum of squares. These sum of squares are computed by the <em>Anova</em> function of the <em>car</em> R package.</p>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb592-1"><a href="#cb592-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>condition<span class="sc">+</span>treatment<span class="sc">+</span>condition<span class="sc">:</span>treatment, <span class="at">data=</span>Diabetes)</span>
<span id="cb592-2"><a href="#cb592-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Anova</span>(m, <span class="at">type=</span><span class="st">&quot;III&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: glucose
##                     Sum Sq Df  F value    Pr(&gt;F)    
## (Intercept)         51.628  1 100.3544 6.888e-16 ***
## condition           10.464  1  20.3405 2.139e-05 ***
## treatment            0.047  1   0.0917    0.7628    
## condition:treatment  0.551  1   1.0711    0.3037    
## Residuals           42.185 82                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>You may ignore the first line (<em>Intercept</em>).</p>
<p>This analysis has been presented before in Section <a href="#S:FInteraction">5.4.5</a> and so we will not give a detailed interpretaion here.</p>
<p>A few remarks:</p>
<ul>
<li><p>The test result for the interaction is the same as for the type I analysis, because for this model the type I and type III sum of squares for this interaction effect coincide.</p></li>
<li><p>The <span class="math inline">\(F\)</span>-tests for the main effects cannot be properly interpreted, because these <span class="math inline">\(F\)</span>-tests compare the following nested models:</p>
<ul>
<li><p>unrestricted model: model with all main effects and the interaction effect</p></li>
<li><p>restricted model: model with all effects of the unrestricted model, except for the main effect for which the test is performed. This restricted model has no proper interpretation.</p></li>
</ul></li>
</ul>
<p>Since the interaction is not significant, we will remove the interaction effect from the model and redo the analysis (with type III sum of squares).</p>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb594-1"><a href="#cb594-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>condition<span class="sc">+</span>treatment, <span class="at">data=</span>Diabetes)</span>
<span id="cb594-2"><a href="#cb594-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Anova</span>(m, <span class="at">type=</span><span class="st">&quot;III&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type III tests)
## 
## Response: glucose
##             Sum Sq Df  F value    Pr(&gt;F)    
## (Intercept) 56.589  1 109.9049 &lt; 2.2e-16 ***
## condition   26.376  1  51.2269 3.011e-10 ***
## treatment    0.022  1   0.0434    0.8355    
## Residuals   42.736 83                       
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>This output allows us to test for the effect of condition within treatment groups, and to test for the effect for treatment effect within condition groups.</p>
<p><strong>Type II Sum of Squares</strong></p>
<p>The type II sum of squares can be seen as a special case of the type III sum of squares in the sense that type II account the hierarchical modelling.
In particular,</p>
<ul>
<li><p>for the interaction effect the type II and type III sum of squares coincide</p></li>
<li><p>for the main effects the type II sum of squares agree with the type III sum of squares of the model without the interaction effects. The <span class="math inline">\(F\)</span>-tests for the main effects, on the basis of type II sum of squares, make use of the MSE of the model without the interaction effect.</p></li>
</ul>
</div>
<div id="example-diabetes-3" class="section level2 unnumbered">
<h2>Example (Diabetes)</h2>
<p>We reanalyse the Diabetes data with <span class="math inline">\(F\)</span>-tests based on type II sum of squares.</p>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="#cb596-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(glucose<span class="sc">~</span>condition<span class="sc">+</span>treatment<span class="sc">+</span>condition<span class="sc">:</span>treatment, <span class="at">data=</span>Diabetes)</span>
<span id="cb596-2"><a href="#cb596-2" aria-hidden="true" tabindex="-1"></a><span class="fu">Anova</span>(m, <span class="at">type=</span><span class="st">&quot;II&quot;</span>)</span></code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: glucose
##                     Sum Sq Df F value    Pr(&gt;F)    
## condition           26.376  1 51.2708 3.129e-10 ***
## treatment            0.022  1  0.0434    0.8355    
## condition:treatment  0.551  1  1.0711    0.3037    
## Residuals           42.185 82                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Some remarks:</p>
<ul>
<li><p>the results for the interaction effect are the same as with type III sum of squares</p></li>
<li><p>the results for the main effects are the same as for the analysis with type III sum of squares in a model without the interaction effect.</p></li>
</ul>
</div>
<div id="S:Multiplicity" class="section level2" number="5.6">
<h2><span class="header-section-number">5.6</span> Multiple comparisons of means</h2>
<p>Let us go back to the PWD example of one-way ANOVA in Section <a href="#S:ANOVA1">5.1</a>. For the <em>ADWG0050</em> outcome variable we have established a significant treatment effect. This conclusion, however, only tells us that not all treatments give on average the same outcome, but it does not tell us which treatments have significantly different mean outcomes.</p>
<div id="formulation-of-the-multiplicity-problem" class="section level3" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Formulation of the multiplicity problem</h3>
<p>In Section <a href="#S:ANOVA1">5.1</a> we have introduced the <span class="math inline">\(F\)</span>-test for testing
<span class="math display">\[
  H_0: \mu_1=\cdots = \mu_t \text{ versus } H_1: \text{not } H_0.
\]</span>
If the null hypothesis is rejected, we conclude that at least two means are different, but the <span class="math inline">\(F\)</span>-test does not tell us which means are different from one another.</p>
<p>A naive solution could exist in splitting <span class="math inline">\(H_0\)</span> into <strong>partial null hypotheses</strong>,
<span class="math display">\[
  H_{0ij}: \mu_i=\mu_j \text{ versus } H_{1ij}: \mu_i \neq \mu_j
\]</span>
and subsequently test all these partial null hypotheses with two-sample <span class="math inline">\(t\)</span>-tests.</p>
<p>For the comparison of treatment <span class="math inline">\(i\)</span> with treatment <span class="math inline">\(j\)</span> the classical two-sample <span class="math inline">\(t\)</span>-test statistic is given by (under the assumption of constant variance)
<span class="math display">\[
  T_{ij} = \frac{\bar{Y}_i-\bar{Y}_j}{S_p\sqrt{\frac{1}{n_i}+\frac{1}{n_j}}} \HSim t_{n-2}
\]</span>
in which <span class="math inline">\(S_p^2\)</span> is the <strong>pooled variance estimator</strong>
<span class="math display">\[
  S_p^2 = \frac{(n_i-1)S_i^2 + (n_j-1)S_j^2}{n_i+n_j-2}
\]</span>
with <span class="math inline">\(S_i^2\)</span> and <span class="math inline">\(S_j^2\)</span> the sample variance of the outcomes in groups <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, respectively,</p>
<p>If the one-way ANOVA model assumptions hold, then the variances in all <span class="math inline">\(t\)</span> groups coincide. This allows us to use all data (from all <span class="math inline">\(t\)</span> groups) for estimating the common variance <span class="math inline">\(\sigma^2\)</span>. We know already that MSE from the one-way ANOVA is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span> under the one-way ANOVA model assumptions. Using MSE for all partial hypotheses is a better approach than using a different <span class="math inline">\(S_p^2\)</span> for each partial hypothesis test. Since MSE makes use of more observations than the <span class="math inline">\(S_p^2\)</span>, it will be a more <em>efficient</em> estimator (i.e. showing less sampling variability). This will in turn increase the power of the statistical test.</p>
<p>Recall that MSE for a one-way ANOVA is given by
<span class="math display">\[
  \MSE= \sum_{i=1}^t \frac{(n_i-1)S_i^2}{n-t}.
\]</span>
With this MSE the two-sample <span class="math inline">\(t\)</span>-test statistics for the partial null hypotheses are given by
<span class="math display">\[
  T_{ij} = \frac{\bar{Y}_i-\bar{Y}_j}{\sqrt{\MSE\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}} \HSim t_{n-t}.
\]</span></p>
<p>Consider now the following procedure for testing the <strong>overall null hypothesis</strong>
<span class="math display">\[
  H_0: \mu_1=\cdots = \mu_t \text{ versus } H_1: \text{not } H_0.
\]</span></p>
<ol style="list-style-type: decimal">
<li><p>split <span class="math inline">\(H_0\)</span> into partial null hypotheses <span class="math inline">\(H_{0ij}\)</span></p></li>
<li><p>test each partial null hypothesis with the two-sample <span class="math inline">\(t\)</span>-test (using MSE as estimator of <span class="math inline">\(\sigma^2\)</span>), at the <span class="math inline">\(\alpha\)</span> level of significance</p></li>
<li><p>reject the overall null hypothesis as soon as one two-sample <span class="math inline">\(t\)</span>-test is significant at the <span class="math inline">\(\alpha\)</span> level of significance.</p></li>
</ol>
<p>First we will demonstrate that this naive procedure based on <span class="math inline">\(t\)</span>-tests, each applied at the <span class="math inline">\(\alpha\)</span> level of significance, does not control the type I error rate for testing the overall null hypothesis at the <span class="math inline">\(\alpha\)</span> level. This will eventually bring us to a more general definition of the type I error rate.</p>
<p>In the next simulation study we compute the empirical probability of rejecting the overall null hypothesis (empirical type I error rate for testing the overall null hypothesis) with this procedure.</p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="#cb598-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17651141</span>)</span>
<span id="cb598-2"><a href="#cb598-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000</span>
<span id="cb598-3"><a href="#cb598-3" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">3</span> <span class="co"># number of treatments or levels (t=3)</span></span>
<span id="cb598-4"><a href="#cb598-4" aria-hidden="true" tabindex="-1"></a>ni<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of replicates within each level</span></span>
<span id="cb598-5"><a href="#cb598-5" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span>p<span class="sc">*</span>ni <span class="co"># total sample size</span></span>
<span id="cb598-6"><a href="#cb598-6" aria-hidden="true" tabindex="-1"></a>alpha<span class="ot">&lt;-</span><span class="fl">0.05</span> <span class="co"># significance level of individual t-tests</span></span>
<span id="cb598-7"><a href="#cb598-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb598-8"><a href="#cb598-8" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">rnorm</span>(n),<span class="at">trt=</span><span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>p,ni)))</span>
<span id="cb598-9"><a href="#cb598-9" aria-hidden="true" tabindex="-1"></a>cnt<span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb598-10"><a href="#cb598-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb598-11"><a href="#cb598-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb598-12"><a href="#cb598-12" aria-hidden="true" tabindex="-1"></a>   db<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb598-13"><a href="#cb598-13" aria-hidden="true" tabindex="-1"></a>   tests<span class="ot">&lt;-</span><span class="fu">pairwise.t.test</span>(db<span class="sc">$</span>y,db<span class="sc">$</span>trt,<span class="st">&quot;none&quot;</span>)   </span>
<span id="cb598-14"><a href="#cb598-14" aria-hidden="true" tabindex="-1"></a>   reject<span class="ot">&lt;-</span><span class="fu">min</span>(tests<span class="sc">$</span>p.value,<span class="at">na.rm=</span>T)<span class="sc">&lt;</span>alpha</span>
<span id="cb598-15"><a href="#cb598-15" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span>(reject) {</span>
<span id="cb598-16"><a href="#cb598-16" aria-hidden="true" tabindex="-1"></a>    cnt<span class="ot">&lt;-</span>cnt<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb598-17"><a href="#cb598-17" aria-hidden="true" tabindex="-1"></a>   }</span>
<span id="cb598-18"><a href="#cb598-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb598-19"><a href="#cb598-19" aria-hidden="true" tabindex="-1"></a>cnt<span class="sc">/</span>N</span></code></pre></div>
<pre><code>## [1] 0.1182</code></pre>
<p>So we approximate the probability of a type I error of this testing procedure to be 0.1182, which is more than twice the nominal <span class="math inline">\(\alpha\)</span> level at which the individual two-sample <span class="math inline">\(t\)</span>-tests are performed.</p>
<p>We repeat the simulation study, but now with <span class="math inline">\(t=5\)</span> levels.</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="#cb600-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17651141</span>)</span>
<span id="cb600-2"><a href="#cb600-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000</span>
<span id="cb600-3"><a href="#cb600-3" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">5</span><span class="co"># number of treatments or levels (t=5)</span></span>
<span id="cb600-4"><a href="#cb600-4" aria-hidden="true" tabindex="-1"></a>ni<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of replicates within each level</span></span>
<span id="cb600-5"><a href="#cb600-5" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span>p<span class="sc">*</span>ni <span class="co"># total sample size</span></span>
<span id="cb600-6"><a href="#cb600-6" aria-hidden="true" tabindex="-1"></a>alpha<span class="ot">&lt;-</span><span class="fl">0.05</span> <span class="co"># significance level of individual t-tests</span></span>
<span id="cb600-7"><a href="#cb600-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb600-8"><a href="#cb600-8" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">rnorm</span>(n),<span class="at">trt=</span><span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>p,ni)))</span>
<span id="cb600-9"><a href="#cb600-9" aria-hidden="true" tabindex="-1"></a>cnt<span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb600-10"><a href="#cb600-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb600-11"><a href="#cb600-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb600-12"><a href="#cb600-12" aria-hidden="true" tabindex="-1"></a>   db<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb600-13"><a href="#cb600-13" aria-hidden="true" tabindex="-1"></a>   tests<span class="ot">&lt;-</span><span class="fu">pairwise.t.test</span>(db<span class="sc">$</span>y,db<span class="sc">$</span>trt,<span class="st">&quot;none&quot;</span>)   </span>
<span id="cb600-14"><a href="#cb600-14" aria-hidden="true" tabindex="-1"></a>   reject<span class="ot">&lt;-</span><span class="fu">min</span>(tests<span class="sc">$</span>p.value,<span class="at">na.rm=</span>T)<span class="sc">&lt;</span>alpha</span>
<span id="cb600-15"><a href="#cb600-15" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span>(reject) {</span>
<span id="cb600-16"><a href="#cb600-16" aria-hidden="true" tabindex="-1"></a>    cnt<span class="ot">&lt;-</span>cnt<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb600-17"><a href="#cb600-17" aria-hidden="true" tabindex="-1"></a>   }</span>
<span id="cb600-18"><a href="#cb600-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb600-19"><a href="#cb600-19" aria-hidden="true" tabindex="-1"></a>cnt<span class="sc">/</span>N</span></code></pre></div>
<pre><code>## [1] 0.2671</code></pre>
<p>With <span class="math inline">\(t=5\)</span> (i.e. 10 partial null hypotheses or pairwise comparisons of means) we find an empirical type I error rate for the overall null hypothesis of 0.2671, which is way larger than the nominal <span class="math inline">\(\alpha=0.05\)</span>.</p>
<p>These simulations illustrate the problem of <strong>multiplicity</strong>. The conventional <span class="math inline">\(p\)</span>-values may only be compared to the nominal significance level <span class="math inline">\(\alpha\)</span> as a threshold if the final conclusion is based on exactly one <span class="math inline">\(p\)</span>-value. In the simulation study the final conclusion was based on multiple two-sample <span class="math inline">\(t\)</span>-tests (multiple <span class="math inline">\(p\)</span>-values). The number of pairwise comparisons is <span class="math inline">\({t \choose 2}\)</span>.</p>
<p>In the next sections we introduce an extension of the notion of the type I error rate and subsequently we give a few solutions.</p>
</div>
<div id="the-familywise-error-rate" class="section level3" number="5.6.2">
<h3><span class="header-section-number">5.6.2</span> The Familywise Error Rate</h3>
<p>The definition of the type I error rate clearly refers to a single hypothesis test. It is the probabilty that the test gives a <strong>false positive</strong> result. Here <em>positive</em> refers to the rejection of the null hypothesis, because in many applications the rejection of the null hypothesis is desired.</p>
<p>When multiple tests are used and in some sense the overall risk of making false conclusions is to be controlled, we need a new concept.</p>
<p>First some new notation.</p>
<ul>
<li><p>In the previous section we introduced partial null hypotheses and their corresponding (two-sample <span class="math inline">\(t\)</span>) tests. We used two indices because each partial null referred to two treatment groups. More generally, we can make use of a single index.</p></li>
<li><p>The overall null hypothesis, <span class="math inline">\(H_0\)</span>, is then the intersection of all partial null hypotheses <span class="math inline">\(H_{0k}\)</span>, <span class="math inline">\(k=1,\ldots, m\)</span> (i.e. the overall null hypothesis states that all partial null hypotheses are simultaneously true). For the pairwise comparisons of means from the previous section, <span class="math inline">\(m=t(t-1)/2\)</span>.</p></li>
</ul>
<p>The <strong>familywise error rate</strong> (FWER) is defined as
<span class="math display">\[
  \text{FWER}=\prob{\text{reject at least one } H_{0k} \mid H_0}.
\]</span></p>
<p>Note that with <span class="math inline">\(m=1\)</span>, the FWER reduces to the type I error rate of a single test. In this sense the FWER is a genuine generalisation of the type I error rate.</p>
<p>The FWER thus expresses the probability of making at least one false positve test result.</p>
<p>Recall the general construction of a (single) statistical test:</p>
<ul>
<li><p>Before observing the data, the significance level is set to a <strong>nominal significance level</strong>, denoted by <span class="math inline">\(\alpha\)</span>. This is the (maximal) type I error rate that is allowed by the statistical test.</p></li>
<li><p>A statistical test is constructed such that it controls the type I error rate to the nominal significance level, i.e. the test must guarantee
<span class="math display">\[
\prob{\text{reject } H_0 \mid H_0} \leq \alpha.
\]</span>
Or, equivalently,
<span class="math display">\[
\prob{T\in {\cal{R}}_\alpha \mid H_0} \leq \alpha.
\]</span>
with <span class="math inline">\({\cal{R}}_\alpha\)</span> the rejection region, or
<span class="math display">\[
\prob{P&lt;\alpha \mid H_0} \leq \alpha,
\]</span>
with <span class="math inline">\(P\)</span> the p-value that corresponds to the test statistsic <span class="math inline">\(T\)</span>.</p></li>
</ul>
<p>For multiple hypotheses testing, we want to have a similar procedure:</p>
<ul>
<li><p>Before observing the data, the FWER level is set to a <strong>nominal FWER level</strong>, which we will also denote by <span class="math inline">\(\alpha\)</span>. This is the (maximal) FWER that is allowed by the multiple testing procedure.</p></li>
<li><p>A multiple testing procedure is constructed such that it controls the FWER to the nominal FWER level <span class="math inline">\(\alpha\)</span>, i.e. the procedure must guarantee
<span class="math display">\[
\text{FWER}=\prob{\text{reject at least one } H_{0k} \mid H_0} \leq \alpha.
\]</span>
Or, equivalently,
<span class="math display">\[
\prob{T_k\in {\cal{R}}_{k,\alpha} \text{ for at least one } k=1,\ldots, m \mid H_0} \leq \alpha.
\]</span>
with <span class="math inline">\({\cal{R}}_{k,\alpha}\)</span> the rejection region for test statistic <span class="math inline">\(T_k\)</span>, or
<span class="math display">\[
\prob{P_k&lt;\alpha^* \text{ for at least one } k=1,\ldots, m\mid H_0} \leq \alpha,
\]</span>
with</p>
<ul>
<li><p><span class="math inline">\(P_k\)</span> the p-value that corresponds to the test statistsic <span class="math inline">\(T_k\)</span></p></li>
<li><p><span class="math inline">\(\alpha^*\)</span> the <strong>per-comparison</strong> significance level (the name comes from: <span class="math inline">\(\alpha^*\)</span> would be the significance level of the test, if only this single test would be considered).</p></li>
</ul></li>
</ul>
<p>Instead of using <span class="math inline">\(P_k&lt;\alpha^*\)</span>, some procedure adjust the <span class="math inline">\(p\)</span>-values so that the adjusted <span class="math inline">\(p\)</span>-values can be directly compared to the nominal FWER level <span class="math inline">\(\alpha\)</span>. With <span class="math inline">\(\tilde{P}_k\)</span> the notation for the adjusted <span class="math inline">\(p\)</span>-value, we get
<span class="math display">\[
  \prob{\tilde{P}_k&lt;\alpha \text{ for at least one } k=1,\ldots, m\mid H_0} \leq \alpha.
\]</span></p>
<p>Hence, multiple testing procedures exist in finding appropriate rejection regions <span class="math inline">\({\cal{R}}_{k,\alpha}\)</span>, or per-comparison significance levels <span class="math inline">\(\alpha^*\)</span>, or <span class="math inline">\(p\)</span>-value adjustments so that the FWER is controlled at <span class="math inline">\(\alpha\)</span>. In the next sections we describe a few of such procedures.</p>
</div>
<div id="the-bonferroni-method" class="section level3" number="5.6.3">
<h3><span class="header-section-number">5.6.3</span> The Bonferroni method</h3>
<p>The <strong>Bonferroni</strong> method is the simplest method for FWER control.</p>
<p>It is based on the following inequality that generally holds in probability theory, for any two events A and B:
<span class="math display">\[
   \prob{A \text{ or } B} = \prob{A} + \prob{B} - \prob{A \text{ and } B} \leq \prob{A} + \prob{B}.
\]</span>
The inequality follows from omitting the positive term <span class="math inline">\(\prob{A \text{ and } B}\)</span>.</p>
<p>The Bonferroni procedure is based on the following expression for the FWER:</p>
<p><span class="math display">\[\begin{eqnarray*}
  \text{FWER}
    &amp;=&amp; \prob{P_k&lt;\alpha^* \text{ for at least one } k=1,\ldots, m\mid H_0} \\
    &amp;=&amp; \prob{P_1&lt;\alpha^* \text{ or } P_2&lt;\alpha^* \text{ or } \cdots \text{ or } P_m&lt;\alpha^* \mid H_0} \\
    &amp;\leq&amp; \sum_{k=1}^m \prob{P_k&lt;\alpha^* \mid H_0} \\
    &amp;\leq&amp; m \alpha^*.
\end{eqnarray*}\]</span></p>
<p>Hence, if we want to control the FWER in the sense that FWER<span class="math inline">\(\leq \alpha\)</span>, we should set the per-comparison significance level to
<span class="math display">\[
  \alpha^* = \frac{\alpha}{m}.
\]</span></p>
<p>An equivalent procedure is obtained with the <strong>adjusted <span class="math inline">\(p\)</span>-values</strong>,
<span class="math display">\[
   \tilde{p} = \min(mp,1).
\]</span>
(The min-operator is used to avoid that the adjusted <span class="math inline">\(p\)</span>-value, which represents a probability, becomes larger than one.)</p>
<p>Thus, either the original <span class="math inline">\(p\)</span>-values are compared to the per-comparison significance level <span class="math inline">\(\alpha^* = \frac{\alpha}{m}\)</span>, or the adjusted <span class="math inline">\(p\)</span>-values <span class="math inline">\(\tilde{p}_k = \min(mp_k,1)\)</span> are compared to the nominal FWER level <span class="math inline">\(\alpha\)</span>.</p>
<p>Another equivalent way for the Bonforroni method, is to adjust the rejection regions or critical values of the test statistics. For multiple comparisons of means, the two-sample <span class="math inline">\(t\)</span>-test statistics have null distribution <span class="math inline">\(t_{n-t}\)</span>. The adjusted critical value for a two-sided test thus becomes
<span class="math display">\[
  t_{n-t,1-\alpha^*/2} = t_{n-t,1-\alpha/(2m)}.
\]</span></p>
</div>
</div>
<div id="example-pwd-5" class="section level2 unnumbered">
<h2>Example (PWD)</h2>
<p>In an earlier analysis of the <em>ADWG0050</em> outcome, we have found a significant effect of the treatment, but the analysis with the <span class="math inline">\(F\)</span>-test did not allow us to indicate which treatments have different means. We now redo the analysis with the Bonferroni method for multiple comparisons of means.</p>
<div class="sourceCode" id="cb602"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb602-1"><a href="#cb602-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairwise.t.test</span>(<span class="at">x=</span>PWD<span class="sc">$</span>ADWG0050, <span class="at">g=</span>PWD<span class="sc">$</span>Treatment,</span>
<span id="cb602-2"><a href="#cb602-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">p.adjust=</span><span class="st">&quot;bonferroni&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  PWD$ADWG0050 and PWD$Treatment 
## 
##   A     B     C     D    
## B 1.000 -     -     -    
## C 0.973 0.067 -     -    
## D 0.891 0.060 1.000 -    
## E 0.481 0.027 1.000 1.000
## 
## P value adjustment method: bonferroni</code></pre>
<p>The output shows, for each combination of two treatments, the Bonforroni-adjusted <span class="math inline">\(p\)</span>-values. Note that some of these adjusted <span class="math inline">\(p\)</span>-values are equal to one (this is because <span class="math inline">\(pm&gt;1\)</span>).
From this analysis we see that only for the comparison of treatments B (normal feed) and E (vaccination and nutraceuticals) the ADWGs are significantly different at the 5% FWER level (<span class="math inline">\(\tilde{p}=0.027\)</span>).</p>
<p>We repeat the analysis, but now with the <em>glht</em> function.</p>
<div class="sourceCode" id="cb604"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb604-1"><a href="#cb604-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">aov</span>(ADWG0050<span class="sc">~</span>Treatment,<span class="at">data=</span>PWD)</span>
<span id="cb604-2"><a href="#cb604-2" aria-hidden="true" tabindex="-1"></a>m.mcp<span class="ot">&lt;-</span><span class="fu">glht</span>(m,<span class="at">linfct=</span><span class="fu">mcp</span>(<span class="at">Treatment=</span><span class="st">&quot;Tukey&quot;</span>))</span>
<span id="cb604-3"><a href="#cb604-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.mcp,<span class="at">test=</span><span class="fu">adjusted</span>(<span class="st">&quot;bonferroni&quot;</span>))</span></code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Linear Hypotheses:
##            Estimate Std. Error t value Pr(&gt;|t|)  
## B - A == 0 -17.7708    15.1015  -1.177   1.0000  
## C - A == 0  25.7292    15.1015   1.704   0.9729  
## D - A == 0  26.4062    15.1015   1.749   0.8913  
## E - A == 0  30.9375    15.1015   2.049   0.4805  
## C - B == 0  43.5000    15.1015   2.881   0.0674 .
## D - B == 0  44.1771    15.1015   2.925   0.0600 .
## E - B == 0  48.7083    15.1015   3.225   0.0273 *
## D - C == 0   0.6771    15.1015   0.045   1.0000  
## E - C == 0   5.2083    15.1015   0.345   1.0000  
## E - D == 0   4.5312    15.1015   0.300   1.0000  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- bonferroni method)</code></pre>
<p>We will demonstrate that the Bonforroni procedure does control the FWER. First we set <span class="math inline">\(t=3\)</span>, resulting in 3 pairwise comparisons.</p>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb606-1"><a href="#cb606-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">186792</span>)</span>
<span id="cb606-2"><a href="#cb606-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000</span>
<span id="cb606-3"><a href="#cb606-3" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">3</span> <span class="co"># number of treatments or levels (t=3)</span></span>
<span id="cb606-4"><a href="#cb606-4" aria-hidden="true" tabindex="-1"></a>ni<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of replicates within each level</span></span>
<span id="cb606-5"><a href="#cb606-5" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span>p<span class="sc">*</span>ni <span class="co"># total sample size</span></span>
<span id="cb606-6"><a href="#cb606-6" aria-hidden="true" tabindex="-1"></a>alpha<span class="ot">&lt;-</span><span class="fl">0.05</span> <span class="co"># nominal FWER level</span></span>
<span id="cb606-7"><a href="#cb606-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb606-8"><a href="#cb606-8" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">rnorm</span>(n),<span class="at">trt=</span><span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>p,ni)))</span>
<span id="cb606-9"><a href="#cb606-9" aria-hidden="true" tabindex="-1"></a>cnt<span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb606-10"><a href="#cb606-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb606-11"><a href="#cb606-11" aria-hidden="true" tabindex="-1"></a>   db<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb606-12"><a href="#cb606-12" aria-hidden="true" tabindex="-1"></a>   tests<span class="ot">&lt;-</span><span class="fu">pairwise.t.test</span>(db<span class="sc">$</span>y,db<span class="sc">$</span>trt,<span class="at">p.adj=</span><span class="st">&quot;bonferroni&quot;</span>)   </span>
<span id="cb606-13"><a href="#cb606-13" aria-hidden="true" tabindex="-1"></a>   reject<span class="ot">&lt;-</span><span class="fu">min</span>(tests<span class="sc">$</span>p.value,<span class="at">na.rm=</span>T)<span class="sc">&lt;</span>alpha</span>
<span id="cb606-14"><a href="#cb606-14" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span>(reject) {</span>
<span id="cb606-15"><a href="#cb606-15" aria-hidden="true" tabindex="-1"></a>    cnt<span class="ot">&lt;-</span>cnt<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb606-16"><a href="#cb606-16" aria-hidden="true" tabindex="-1"></a>   }</span>
<span id="cb606-17"><a href="#cb606-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb606-18"><a href="#cb606-18" aria-hidden="true" tabindex="-1"></a>cnt<span class="sc">/</span>N</span></code></pre></div>
<pre><code>## [1] 0.0419</code></pre>
<p>We find an empirical FWER of 0.0419, which is indeed smaller than <span class="math inline">\(\alpha=0.05\)</span>. We repeat the simulation study with <span class="math inline">\(t=5\)</span>, resulting in <span class="math inline">\(m=10\)</span> pairwise comparisons.</p>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb608-1"><a href="#cb608-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">186792</span>)</span>
<span id="cb608-2"><a href="#cb608-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000</span>
<span id="cb608-3"><a href="#cb608-3" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of treatments or levels (t=5)</span></span>
<span id="cb608-4"><a href="#cb608-4" aria-hidden="true" tabindex="-1"></a>ni<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of replicates within each level</span></span>
<span id="cb608-5"><a href="#cb608-5" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span>p<span class="sc">*</span>ni <span class="co"># total sample size</span></span>
<span id="cb608-6"><a href="#cb608-6" aria-hidden="true" tabindex="-1"></a>alpha<span class="ot">&lt;-</span><span class="fl">0.05</span> <span class="co"># nominal FWER level</span></span>
<span id="cb608-7"><a href="#cb608-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb608-8"><a href="#cb608-8" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">rnorm</span>(n),<span class="at">trt=</span><span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>p,ni)))</span>
<span id="cb608-9"><a href="#cb608-9" aria-hidden="true" tabindex="-1"></a>cnt<span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb608-10"><a href="#cb608-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb608-11"><a href="#cb608-11" aria-hidden="true" tabindex="-1"></a>   db<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb608-12"><a href="#cb608-12" aria-hidden="true" tabindex="-1"></a>   tests<span class="ot">&lt;-</span><span class="fu">pairwise.t.test</span>(db<span class="sc">$</span>y,db<span class="sc">$</span>trt,<span class="at">p.adj=</span><span class="st">&quot;bonferroni&quot;</span>)   </span>
<span id="cb608-13"><a href="#cb608-13" aria-hidden="true" tabindex="-1"></a>   reject<span class="ot">&lt;-</span><span class="fu">min</span>(tests<span class="sc">$</span>p.value,<span class="at">na.rm=</span>T)<span class="sc">&lt;</span>alpha</span>
<span id="cb608-14"><a href="#cb608-14" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span>(reject) {</span>
<span id="cb608-15"><a href="#cb608-15" aria-hidden="true" tabindex="-1"></a>    cnt<span class="ot">&lt;-</span>cnt<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb608-16"><a href="#cb608-16" aria-hidden="true" tabindex="-1"></a>   }</span>
<span id="cb608-17"><a href="#cb608-17" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb608-18"><a href="#cb608-18" aria-hidden="true" tabindex="-1"></a>cnt<span class="sc">/</span>N</span></code></pre></div>
<pre><code>## [1] 0.035</code></pre>
<p>Again the empirical FWER 0.035 is smaller than <span class="math inline">\(\alpha=0.05\)</span>.</p>
<p>The empirical FWER is now also smaller than for the <span class="math inline">\(t=3\)</span> simulations. This is an example of the <strong>conservative</strong> nature of the Bonferroni procedure.</p>
<p>A multiple testing procedure that controls the FWER in the sense of
<span class="math display">\[
  \text{FWER} = \alpha
\]</span>
is said to give <strong>strong control</strong> of the FWER. However, the Bonferroni method only guarantees
<span class="math display">\[
  \text{FWER} \leq \alpha.
\]</span>
Such methods are said to give <strong>weak control</strong> of the FWER. If FWER<span class="math inline">\(\leq \alpha\)</span> we are at the safe side; the method is stricter than what is allowed. Such methods are also referred to as <strong>conservative methods</strong>.</p>
<p>For the Bonferroni method, the conservativeness increases with the number of simultaneous tests (<span class="math inline">\(m\)</span>). This comes from the probability terms that are omitted in the expression of the FWER. Therefore, when <span class="math inline">\(m\)</span> is large, it is no longer recommended to use the Bonferroni method.</p>
<div id="the-sidak-method" class="section level3 unnumbered">
<h3>The Sidak method</h3>
<p>The method of Sidak is based on another way of expressing the FWER. It makes use of the assumption that all tests are mutually independent under <span class="math inline">\(H_0\)</span>, i.e. all test statistics <span class="math inline">\(T_k\)</span> for the partial null hypotheses are mutually independent under <span class="math inline">\(H_0\)</span>. This is an unrealistic assumption for many applications (e.g. test statistics may share the same sample mean), but it simplifies the expression for the FWER (similar as the omission of probability terms in the development of the Bonferroni method).</p>
<p><span class="math display">\[\begin{eqnarray*}
  \text{FWER}
    &amp;=&amp; \prob{P_k&lt;\alpha^* \text{ for at least one } k=1,\ldots, m\mid H_0} \\
    &amp;=&amp; 1- \prob{P_1&gt; \alpha^* \text{ and } P_2&gt;\alpha^* \text{ and } \cdots \text{ and } P_m&gt;\alpha^* \mid H_0} \\
    &amp;=&amp; 1- \prob{P_1&gt; \alpha^* \mid H_0} \prob{P_2&gt;\alpha^*\mid H_0} \ldots \prob{P_m&gt;\alpha^* \mid H_0} \\
    &amp;=&amp; 1- \prod_{k=1}^n \prob{P_k&gt; \alpha^* \mid H_0} \\
    &amp;=&amp; 1- \prod_{k=1}^n \left[1-\prob{P_k&lt; \alpha^* \mid H_0}\right] \\
    &amp;=&amp; 1- (1-\alpha^*)^m.
\end{eqnarray*}\]</span></p>
<p>This expression immediately gives a solution for the per-comparison significance level <span class="math inline">\(\alpha^*\)</span> for controlling the FWER at <span class="math inline">\(\alpha\)</span>:
<span class="math display">\[
   \alpha^* = 1-(1-\alpha)^{1/m}.
\]</span>
This method is known as the Sidak method.</p>
<p>The corresponding adjusted <span class="math inline">\(p\)</span>-values are given by
<span class="math display">\[
  \tilde{p} = 1-(1-p)^m.
\]</span></p>
<p>It can be shown that the method of Sidak is conservative, but less conservative than the Bonferroni method. The latter follows from
<span class="math display">\[
   \frac{\alpha}{m} \leq 1-(1-\alpha)^{1/m},
\]</span>
i.e. the per-comparison significance level of the Bonferroni method is never larger than for the Sidak method. Hence, the power (actually the <em>sensitivity</em>) of the Sidak procedure is generally larger than for the Bonferroni procedure.</p>
</div>
<div id="the-holm-bonferroni-method" class="section level3" number="5.6.4">
<h3><span class="header-section-number">5.6.4</span> The Holm-Bonferroni method</h3>
<p>The method of Holm-Bonferroni is an improvement of the Bonferroni method. It is less conservative than Bonferroni. We give the method here without proof.</p>
<ol style="list-style-type: decimal">
<li><p>Order the <span class="math inline">\(p\)</span>-values from small to large:
<span class="math display">\[
  p_{(1)}\leq p_{(2)} \leq \cdots \leq p_{(m)}
\]</span>
where <span class="math inline">\(p_{(k)}\)</span> denotes the <span class="math inline">\(k\)</span>th order statistic of the set of <span class="math inline">\(m\)</span> <span class="math inline">\(p\)</span>-values.</p></li>
<li><p>If <span class="math inline">\(p_{(1)}&lt;\frac{\alpha}{m}\)</span>, then the partial null hypothesis that corresponds to this <span class="math inline">\(p\)</span>-value is rejected, and the method proceeds (go to step 3). Otherwise no partial hypotheses are rejected and the procedure stops here.</p></li>
<li><p>If <span class="math inline">\(p_{(2)}&lt;\frac{\alpha}{m-1}\)</span>, then the partial null hypothesis that corresponds to this <span class="math inline">\(p\)</span>-value is rejected, and the method proceeds (go to step 4). Otherwise no further partial hypotheses are rejected and the procedure stops here.</p></li>
<li><p>For a general <span class="math inline">\(k=3, \ldots m\)</span>, if <span class="math inline">\(p_{(k)}&lt;\frac{\alpha}{m-(k-1)}\)</span> then the partial null hypothesis that corresponds to this <span class="math inline">\(p\)</span>-value is rejected, and the method proceeds as long as <span class="math inline">\(k\leq m\)</span>. As soon as <span class="math inline">\(p_{(k)}\geq \frac{\alpha}{m-(k-1)}\)</span>, the procedure stops and no further partial hypotheses are rejected.</p></li>
</ol>
<p>In other words: for the smallest <span class="math inline">\(p\)</span>-value the classical Bonforroni method is applied. If this results in significance, then the method proceeds with the next smallest p-value and the method of Bonferroni is used again, but this time on the <span class="math inline">\(m-1\)</span> remaining <span class="math inline">\(p\)</span>-values (thus the Bonforroni method needs to adjust for only <span class="math inline">\(m-1\)</span> tests and is thus less strict). Each time a significant test result is obtained, the method is allowed to proceed at a larger per-comparison significance level. Hence, the method is more powerful than the classical Bonferroni method.</p>
<p>Note that the FWER of the Bonferroni and Holm-Bonferroni methods coincide, because the FWER only depends on the smallest adjusted <span class="math inline">\(p\)</span>-value, and these are the same for both methods.</p>
<p>Adjusted <span class="math inline">\(p\)</span>-values are calculated as
<span class="math display">\[
  \tilde{p}_{(k)} = \max_{j\leq k} \left( \min\left( (m-(j-1)p_{(j)}, 1\right)\right).
\]</span></p>
</div>
</div>
<div id="example-pwd-6" class="section level2 unnumbered">
<h2>Example (PWD)</h2>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="#cb610-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairwise.t.test</span>(<span class="at">x=</span>PWD<span class="sc">$</span>ADWG0050, <span class="at">g=</span>PWD<span class="sc">$</span>Treatment,</span>
<span id="cb610-2"><a href="#cb610-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">p.adjust=</span><span class="st">&quot;holm&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  PWD$ADWG0050 and PWD$Treatment 
## 
##   A     B     C     D    
## B 0.989 -     -     -    
## C 0.535 0.054 -     -    
## D 0.535 0.054 1.000 -    
## E 0.336 0.027 1.000 1.000
## 
## P value adjustment method: holm</code></pre>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="#cb612-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">aov</span>(ADWG0050<span class="sc">~</span>Treatment,<span class="at">data=</span>PWD)</span>
<span id="cb612-2"><a href="#cb612-2" aria-hidden="true" tabindex="-1"></a>m.mcp<span class="ot">&lt;-</span><span class="fu">glht</span>(m,<span class="at">linfct=</span><span class="fu">mcp</span>(<span class="at">Treatment=</span><span class="st">&quot;Tukey&quot;</span>))</span>
<span id="cb612-3"><a href="#cb612-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.mcp,<span class="at">test=</span><span class="fu">adjusted</span>(<span class="st">&quot;holm&quot;</span>))</span></code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Linear Hypotheses:
##            Estimate Std. Error t value Pr(&gt;|t|)  
## B - A == 0 -17.7708    15.1015  -1.177   0.9889  
## C - A == 0  25.7292    15.1015   1.704   0.5348  
## D - A == 0  26.4062    15.1015   1.749   0.5348  
## E - A == 0  30.9375    15.1015   2.049   0.3364  
## C - B == 0  43.5000    15.1015   2.881   0.0540 .
## D - B == 0  44.1771    15.1015   2.925   0.0540 .
## E - B == 0  48.7083    15.1015   3.225   0.0273 *
## D - C == 0   0.6771    15.1015   0.045   1.0000  
## E - C == 0   5.2083    15.1015   0.345   1.0000  
## E - D == 0   4.5312    15.1015   0.300   1.0000  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- holm method)</code></pre>
<p>The smallest adjusted <span class="math inline">\(p\)</span>-value is still <span class="math inline">\(\tilde{p}=0.027\)</span>, as for Bonferroni. The second smallest is now <span class="math inline">\(\tilde{p}=0.054\)</span>, which is smaller than for Bonferroni. However, since <span class="math inline">\(\tilde{p}=0.054&gt;0.05\)</span>, our conclusions at the 5% FWER are the same as for the Bonferroni method.</p>
<div id="the-tukey-kramer-method" class="section level3" number="5.6.5">
<h3><span class="header-section-number">5.6.5</span> The Tukey-Kramer method</h3>
<p>The method is also known under the names: Tukey’s test or Tukey’s HSD (<em>honestly significant difference</em>) test.</p>
<p>This method gives strong control of the FWER, but it is less general than the previous methods. It is only developed for multiple comparisons of means with two-sample <span class="math inline">\(t\)</span>-tests, under the classical assumptions of the <span class="math inline">\(t\)</span>-test.</p>
<p>It is based on the following calculations, in which <span class="math inline">\(T_k\)</span> is the absolute value of the two-sample <span class="math inline">\(t\)</span>-test statistic and <span class="math inline">\(t_\alpha\)</span> is some threshold (defining a critical region of the test):</p>
<p><span class="math display">\[\begin{eqnarray*}
  \text{FWER}
    &amp;=&amp; \prob{T_k&gt;t_\alpha \text{ for at least one } k=1,\ldots, m\mid H_0} \\
    &amp;=&amp; \prob{T_1&gt; t_\alpha \text{ or } T_2&gt; t_\alpha \text{ or } \cdots \text{ or } T_m&gt; t_\alpha \mid H_0} \\
    &amp;=&amp; \prob{\max_{k=1,\ldots, m} T_k &gt; t_\alpha \mid H_0} .
\end{eqnarray*}\]</span></p>
<p>Hence, if we know the null distribution of <span class="math inline">\(Q=\max_{k=1,\ldots, m} T_k\)</span>, we can set <span class="math inline">\(t_\alpha\)</span> to the <span class="math inline">\(1-\alpha\)</span> quantile of that null distribution, and the FWER will be exactly equal to <span class="math inline">\(\alpha\)</span>. Note that this procedure is exactly the procedure for finding the critical value of a single test statistic, when the type I error rate is to be controlled at the nominal significance level <span class="math inline">\(\alpha\)</span>. In our construction, it is the statistic <span class="math inline">\(Q=\max_{k=1,\ldots, m} T_k\)</span> that plays the role of a single test statistic. Tukey and Kramer have developed this method and found the exact null distribution of <span class="math inline">\(Q\)</span> under the classical assumptions of two-sample <span class="math inline">\(t\)</span>-tests (the <em>studentised range distribution</em>). This null distribution may also be used for the calculation of the adjusted <span class="math inline">\(p\)</span>-values.</p>
<p>Once the critical value <span class="math inline">\(t_\alpha\)</span> is found, all individual test statistics <span class="math inline">\(t_k\)</span> may be compared to this threshold and if <span class="math inline">\(t_k&gt;t_\alpha\)</span>, the corresponding partial null hypothesis is rejected at the <span class="math inline">\(\alpha\)</span> nominal FWER level.</p>
<p>The strong FWER control of the method is demonstated in a simulation study.</p>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="#cb614-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">9267</span>)</span>
<span id="cb614-2"><a href="#cb614-2" aria-hidden="true" tabindex="-1"></a>N<span class="ot">&lt;-</span>N10000</span>
<span id="cb614-3"><a href="#cb614-3" aria-hidden="true" tabindex="-1"></a>p<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of treatments or levels (t=5)</span></span>
<span id="cb614-4"><a href="#cb614-4" aria-hidden="true" tabindex="-1"></a>ni<span class="ot">&lt;-</span><span class="dv">5</span> <span class="co"># number of replicates within each level</span></span>
<span id="cb614-5"><a href="#cb614-5" aria-hidden="true" tabindex="-1"></a>n<span class="ot">&lt;-</span>p<span class="sc">*</span>ni <span class="co"># total sample size</span></span>
<span id="cb614-6"><a href="#cb614-6" aria-hidden="true" tabindex="-1"></a>alpha<span class="ot">&lt;-</span><span class="fl">0.05</span> <span class="co"># nominal FWER level</span></span>
<span id="cb614-7"><a href="#cb614-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb614-8"><a href="#cb614-8" aria-hidden="true" tabindex="-1"></a>db<span class="ot">&lt;-</span><span class="fu">data.frame</span>(<span class="at">y=</span><span class="fu">rnorm</span>(n),<span class="at">trt=</span><span class="fu">factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>p,ni)))</span>
<span id="cb614-9"><a href="#cb614-9" aria-hidden="true" tabindex="-1"></a>cnt<span class="ot">&lt;-</span><span class="dv">0</span></span>
<span id="cb614-10"><a href="#cb614-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>N) {</span>
<span id="cb614-11"><a href="#cb614-11" aria-hidden="true" tabindex="-1"></a>   db<span class="sc">$</span>y<span class="ot">&lt;-</span><span class="fu">rnorm</span>(n)</span>
<span id="cb614-12"><a href="#cb614-12" aria-hidden="true" tabindex="-1"></a>   m<span class="ot">&lt;-</span><span class="fu">aov</span>(y<span class="sc">~</span>trt,<span class="at">data=</span>db)</span>
<span id="cb614-13"><a href="#cb614-13" aria-hidden="true" tabindex="-1"></a>   HSD<span class="ot">&lt;-</span><span class="fu">TukeyHSD</span>(m)</span>
<span id="cb614-14"><a href="#cb614-14" aria-hidden="true" tabindex="-1"></a>   reject<span class="ot">&lt;-</span><span class="fu">min</span>(HSD<span class="sc">$</span>trt[,<span class="dv">4</span>],<span class="at">na.rm=</span>T)<span class="sc">&lt;</span>alpha</span>
<span id="cb614-15"><a href="#cb614-15" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span>(reject) {</span>
<span id="cb614-16"><a href="#cb614-16" aria-hidden="true" tabindex="-1"></a>    cnt<span class="ot">&lt;-</span>cnt<span class="sc">+</span><span class="dv">1</span></span>
<span id="cb614-17"><a href="#cb614-17" aria-hidden="true" tabindex="-1"></a>   }</span>
<span id="cb614-18"><a href="#cb614-18" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb614-19"><a href="#cb614-19" aria-hidden="true" tabindex="-1"></a>cnt<span class="sc">/</span>N</span></code></pre></div>
<pre><code>## [1] 0.0498</code></pre>
</div>
</div>
<div id="example-pwd-7" class="section level2 unnumbered">
<h2>Example (PWD)</h2>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="#cb616-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">aov</span>(ADWG0050<span class="sc">~</span>Treatment,<span class="at">data=</span>PWD)</span>
<span id="cb616-2"><a href="#cb616-2" aria-hidden="true" tabindex="-1"></a>m.mcp<span class="ot">&lt;-</span><span class="fu">glht</span>(m,<span class="at">linfct=</span><span class="fu">mcp</span>(<span class="at">Treatment=</span><span class="st">&quot;Tukey&quot;</span>))</span>
<span id="cb616-3"><a href="#cb616-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m.mcp)</span></code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Linear Hypotheses:
##            Estimate Std. Error t value Pr(&gt;|t|)  
## B - A == 0 -17.7708    15.1015  -1.177   0.7643  
## C - A == 0  25.7292    15.1015   1.704   0.4449  
## D - A == 0  26.4062    15.1015   1.749   0.4187  
## E - A == 0  30.9375    15.1015   2.049   0.2650  
## C - B == 0  43.5000    15.1015   2.881   0.0494 *
## D - B == 0  44.1771    15.1015   2.925   0.0445 *
## E - B == 0  48.7083    15.1015   3.225   0.0214 *
## D - C == 0   0.6771    15.1015   0.045   1.0000  
## E - C == 0   5.2083    15.1015   0.345   0.9968  
## E - D == 0   4.5312    15.1015   0.300   0.9981  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="#cb618-1" aria-hidden="true" tabindex="-1"></a>m.Tukey<span class="ot">&lt;-</span><span class="fu">TukeyHSD</span>(m)</span>
<span id="cb618-2"><a href="#cb618-2" aria-hidden="true" tabindex="-1"></a>m.Tukey</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## $Treatment
##            diff          lwr      upr     p adj
## B-A -17.7708333 -61.18848284 25.64682 0.7643182
## C-A  25.7291667 -17.68848284 69.14682 0.4448450
## D-A  26.4062500 -17.01139951 69.82390 0.4187691
## E-A  30.9375000 -12.48014951 74.35515 0.2649677
## C-B  43.5000000   0.08235049 86.91765 0.0493722
## D-B  44.1770833   0.75943382 87.59473 0.0444701
## E-B  48.7083333   5.29068382 92.12598 0.0214520
## D-C   0.6770833 -42.74056618 44.09473 0.9999990
## E-C   5.2083333 -38.20931618 48.62598 0.9968132
## E-D   4.5312500 -38.88639951 47.94890 0.9981470</code></pre>
<p>With the method of Tukey, we find that the following treatments show significantly different ADWG after 50 post-weaning (jointly significant at the 5% FWER level):</p>
<ul>
<li><p>Treatment E has on average an ADWG of 48.7 g/days (SE <span class="math inline">\(15.1\)</span> g/days) larger than treatment B (<span class="math inline">\(\tilde{p}=0.021\)</span>)</p></li>
<li><p>Treatment D has on average an ADWG of 44.2 g/days (SE <span class="math inline">\(15.1\)</span> g/days) larger than treatment B (<span class="math inline">\(\tilde{p}=0.044\)</span>)</p></li>
<li><p>Treatment C has on average an ADWG of 43.5 g/days (SE <span class="math inline">\(15.1\)</span> g/days) larger than treatment B (<span class="math inline">\(\tilde{p}=0.049\)</span>)</p></li>
</ul>
<p>Some notes:</p>
<ul>
<li><p>Thus treatments C, D and E (all the vaccination groups) have a larger ADWG than the standerd feed treatment group B.</p></li>
<li><p>With the Tukey method we found three significant differences, whereas with (Holm-)Bonferroni we found only one.</p></li>
<li><p>The output of the <em>TukeyHSD</em> function also gives limits of confidence intervals. In the next section we will learn how to interpret them (these are not the traditional CIs).</p></li>
</ul>
<div id="simultaneous-ci" class="section level3" number="5.6.6">
<h3><span class="header-section-number">5.6.6</span> Simultaneous CI</h3>
<p>For a single two-sided <span class="math inline">\(t\)</span>-test, we know the equivalence between hypothesis testing and confidence interval (CI): the test rejects <span class="math inline">\(H_0: \beta=0\)</span> at the <span class="math inline">\(\alpha\)</span> level of significance if and only if <span class="math inline">\(0\)</span> is not contained in the <span class="math inline">\(1-\alpha\)</span> confidence interval of <span class="math inline">\(\beta\)</span>.</p>
<p>In this section we generalise the notion of a CI to <strong>simultaneous confidence intervals</strong>, which are applicable in the same settings as for multiple testing.</p>
<p>Suppose we have <span class="math inline">\(m\)</span> parameters <span class="math inline">\(\theta_1, \ldots, \theta_m\)</span>. Then, for each parameter <span class="math inline">\(\theta_k\)</span>, the statistics <span class="math inline">\(L_k\)</span> and <span class="math inline">\(U_k\)</span> denote the lower and upper bounds of simultaneous <span class="math inline">\(1-\alpha\)</span> confidence intervals if
<span class="math display">\[
  \prob{\theta_k \in [L_k, U_k] \text{ for all } k=1,\ldots, m} = 1-\alpha. 
\]</span></p>
<p>For these simultaneous CIs we also have an equivalence between them and tests that control the FWER at the <span class="math inline">\(\alpha\)</span> level: a partial null hypothesis for testing <span class="math inline">\(\theta_k=0\)</span> is rejected at the <span class="math inline">\(\alpha\)</span> FWER level if and only if the corresponding <span class="math inline">\(1-\alpha\)</span> simultaneous CI does not contain <span class="math inline">\(0\)</span>.</p>
</div>
</div>
<div id="example-pwd-8" class="section level2 unnumbered">
<h2>Example (PWD)</h2>
<p>We repeat the analysis for the <em>ADWG0050</em> outcome variable, but now with a focus on the simultaneous CIs. We have to choose for a multiplicity correction method. We choose here for Tukey. I am not showing the results of the <em>TukeyHSD</em> method, because these have been shown before.</p>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="#cb620-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">aov</span>(ADWG0050<span class="sc">~</span>Treatment,<span class="at">data=</span>PWD)</span>
<span id="cb620-2"><a href="#cb620-2" aria-hidden="true" tabindex="-1"></a>m.mcp<span class="ot">&lt;-</span><span class="fu">glht</span>(m,<span class="at">linfct=</span><span class="fu">mcp</span>(<span class="at">Treatment=</span><span class="st">&quot;Tukey&quot;</span>))</span>
<span id="cb620-3"><a href="#cb620-3" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m.mcp)</span></code></pre></div>
<pre><code>## 
##   Simultaneous Confidence Intervals
## 
## Multiple Comparisons of Means: Tukey Contrasts
## 
## 
## Fit: aov(formula = ADWG0050 ~ Treatment, data = PWD)
## 
## Quantile = 2.8753
## 95% family-wise confidence level
##  
## 
## Linear Hypotheses:
##            Estimate  lwr       upr      
## B - A == 0 -17.77083 -61.19225  25.65058
## C - A == 0  25.72917 -17.69225  69.15058
## D - A == 0  26.40625 -17.01517  69.82767
## E - A == 0  30.93750 -12.48392  74.35892
## C - B == 0  43.50000   0.07858  86.92142
## D - B == 0  44.17708   0.75567  87.59850
## E - B == 0  48.70833   5.28692  92.12975
## D - C == 0   0.67708 -42.74433  44.09850
## E - C == 0   5.20833 -38.21308  48.62975
## E - D == 0   4.53125 -38.89017  47.95267</code></pre>
<p>The simultaneous CIs can also be graphically presented. This is quite common in the context of multiple comparisons.</p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="#cb622-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m.mcp)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-192-1.png" width="672" /></p>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="#cb623-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m.Tukey)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-192-2.png" width="672" /></p>
</div>
<div id="exercise-equivalence-simultaneous-ci-and-fwer-control" class="section level2 unnumbered">
<h2>Exercise: equivalence simultaneous CI and FWER control</h2>
<p>Set up a simulation study to demonstrate the equivalence of simultaneous CIs and FWER control.</p>
</div>
<div id="assessment-of-model-assumptions" class="section level2" number="5.7">
<h2><span class="header-section-number">5.7</span> Assessment of model assumptions</h2>
<p>The assessment of the ANOVA model assumptions proceeds very similar as for linear regression models. This can be easily understood: all ANOVA models can be formulated as an equivalent linear regression model. We illustrate the procedure with one example.</p>
</div>
<div id="example-pwd-9" class="section level2 unnumbered">
<h2>Example (PWD)</h2>
<p>We consider the <em>ADWG0050</em> variable and the two-way ANOVA model with <em>Sex</em> and <em>Treatment</em> as factor variables.</p>
<p>First we fit the <strong>saturated model</strong>, i.e. the model with main and interaction effects.</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="#cb624-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Sex<span class="sc">*</span>Treatment, <span class="at">data=</span>PWD)</span></code></pre></div>
<p><strong>Normality</strong></p>
<p>First we check normality. With a sample size of <span class="math inline">\(n=40\)</span>, the normality assumption cannot be completely neglected.</p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="#cb625-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(m<span class="sc">$</span>residuals)</span>
<span id="cb625-2"><a href="#cb625-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(m<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-194-1.png" width="672" /></p>
<p>The normal QQ plot does not show a serious deviation from normality.</p>
<p><strong>Conditional mean model</strong></p>
<p>Next we <em>could</em> show residual plots to assess the conditional mean model assumption, but let’s first think about this. If we have a model with only factor variables and their interaction effects, then the model basically has a parameter for each factor level combination. More specifically, if we have two factors, one with <span class="math inline">\(t\)</span> levels, and another with <span class="math inline">\(a\)</span> levels, then the ANOVA model (with interaction effects) has <span class="math inline">\(t\times a\)</span> paramaters. So this model does not impose restrictions on the model for the conditional mean as a function of the factor levels! And hence there is no need to assess the conditional mean model assumption. On the other hand, if there is no interaction effect, then the model for the conditional mean has only <span class="math inline">\(t+a-1\)</span> parameters whereas there are <span class="math inline">\(t\times a\)</span> factor level combinations. So the additive model does impose restrictions on the conditional mean outcome. This will be assessed a bit later in this section, when we look at the additive model.</p>
<p><strong>Constant variance</strong></p>
<p>The model implies that for all factor level combiniations the variances of the outcomes should be the same. Here we show some plots that can be used for assessing this assumption. We need to keep in mind that for each factor level combination we only have 4 replicates. This makes it alomst impossible to assess the assumption.</p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="#cb626-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(m<span class="sc">$</span>residuals<span class="sc">~</span>PWD<span class="sc">$</span>Treatment<span class="sc">*</span>PWD<span class="sc">$</span>Sex,</span>
<span id="cb626-2"><a href="#cb626-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;Treatment (A-E) * Sex (1,2)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb626-3"><a href="#cb626-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-195-1.png" width="672" /></p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="#cb627-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>((<span class="fu">abs</span>(m<span class="sc">$</span>residuals))<span class="sc">~</span>PWD<span class="sc">$</span>Treatment<span class="sc">*</span>PWD<span class="sc">$</span>Sex,</span>
<span id="cb627-2"><a href="#cb627-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;Treatment (A-E) * Sex (1,2)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)</span>
<span id="cb627-3"><a href="#cb627-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">mean</span>(<span class="fu">abs</span>(m<span class="sc">$</span>residuals)),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-195-2.png" width="672" /></p>
<p>All boxplots, both for the residuals and for the absulute value of the residuals, show quite some variability. Thus is simply caused by the small number of replicates for each combination of factor levels.</p>
<p>We have earlier concluded that there is no evidence in favor of a interaction effects. Here we will assess the model assumptions for the additive model.</p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="#cb628-1" aria-hidden="true" tabindex="-1"></a>m<span class="ot">&lt;-</span><span class="fu">lm</span>(ADWG0050<span class="sc">~</span>Sex<span class="sc">+</span>Treatment, <span class="at">data=</span>PWD)</span></code></pre></div>
<p><strong>Normality</strong></p>
<p>First we check normality. With a sample size of <span class="math inline">\(n=40\)</span>, the normality assumption cannot be completely neglected.</p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="#cb629-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(m<span class="sc">$</span>residuals)</span>
<span id="cb629-2"><a href="#cb629-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(m<span class="sc">$</span>residuals)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-197-1.png" width="672" /></p>
<p>No serious deviation from normality can be concluded.</p>
<p><strong>Conditional mean model</strong></p>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb630-1"><a href="#cb630-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(m<span class="sc">$</span>residuals<span class="sc">~</span>PWD<span class="sc">$</span>Treatment,</span>
<span id="cb630-2"><a href="#cb630-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;treatment&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb630-3"><a href="#cb630-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-198-1.png" width="672" /></p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="#cb631-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(m<span class="sc">$</span>residuals<span class="sc">~</span>PWD<span class="sc">$</span>Sex,</span>
<span id="cb631-2"><a href="#cb631-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;gender&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb631-3"><a href="#cb631-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-198-2.png" width="672" /></p>
<div class="sourceCode" id="cb632"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb632-1"><a href="#cb632-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(m),m<span class="sc">$</span>residuals,</span>
<span id="cb632-2"><a href="#cb632-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab=</span><span class="st">&quot;prediction&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb632-3"><a href="#cb632-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-198-3.png" width="672" /></p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="#cb633-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(m<span class="sc">$</span>residuals<span class="sc">~</span>PWD<span class="sc">$</span>Sex<span class="sc">*</span>PWD<span class="sc">$</span>Treatment,</span>
<span id="cb633-2"><a href="#cb633-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;gender (1,2) * treatment (A-E)&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;residual&quot;</span>)</span>
<span id="cb633-3"><a href="#cb633-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>,<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-198-4.png" width="672" /></p>
<p>None of these graphs shows a serious indication of a model violation, except perhaps the last graph, which shows boxplots of residuals for all combinations of <em>Treatment</em> and <em>Sex</em>. The last two boxplots show averages of residuals that are relatively far away from zero. This agrees with what we have seen in the interaction plot in Section <a href="#S:FInteraction">5.4.5</a>. However, no significance was reached, and looking at the variance of the residuals, the observed deviations does indeed fall within what could be expected from the observed variability.</p>
<p><strong>Constant variance</strong></p>
<div class="sourceCode" id="cb634"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb634-1"><a href="#cb634-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>((<span class="fu">abs</span>(m<span class="sc">$</span>residuals))<span class="sc">~</span>PWD<span class="sc">$</span>Sex,</span>
<span id="cb634-2"><a href="#cb634-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;Treatment&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)</span>
<span id="cb634-3"><a href="#cb634-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">mean</span>(<span class="fu">abs</span>(m<span class="sc">$</span>residuals)),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-199-1.png" width="672" /></p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="#cb635-1" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>((<span class="fu">abs</span>(m<span class="sc">$</span>residuals))<span class="sc">~</span>PWD<span class="sc">$</span>Treatment,</span>
<span id="cb635-2"><a href="#cb635-2" aria-hidden="true" tabindex="-1"></a>        <span class="at">xlab=</span><span class="st">&quot;Sex&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;abs(residual)&quot;</span>)</span>
<span id="cb635-3"><a href="#cb635-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="fu">mean</span>(<span class="fu">abs</span>(m<span class="sc">$</span>residuals)),<span class="at">lty=</span><span class="dv">2</span>,<span class="at">col=</span><span class="dv">4</span>)</span></code></pre></div>
<p><img src="DASM2_files/figure-html/unnamed-chunk-199-2.png" width="672" /></p>
<p>No serious deviations can be detected.</p>
<ul>
<li><p>The boxplots for the two genders are each based on 20 observations. They nicely confirm the homoscedasticity assumption.</p></li>
<li><p>The boxplots for the five treatments are each based on only 8 observations. Although the means of the <span class="math inline">\(|e_i|\)</span> may show some non-constancy, this is within the variability seen from the plot. Perhaps this plot suggests that within the treatment B group (normal feed) the variance of the outcome is smaller as compared to the other groups, but, again, given the rather small number of observations for each boxplot, and given the observed variability, there is no reason be alarmed.</p></li>
</ul>

</div>
</div>
<div id="reporting" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Reporting</h1>
<p>Here we provide some general guidelines for reporting the statistical results of a linear regression analysis. Many of these guidelines are more generally applicable to reporting other types of statistical analysis.</p>
<ul>
<li><p>Always report the study design. This is extremely important to determine what statistical methods can be applied in a valid way, and what kind of conclusions are allowed (e.g. causal conclusion or not).</p></li>
<li><p>Always report the research question.</p></li>
<li><p>It is strongly recommended to always start with a data exploration. If strange or unexpected observations are detected, then identify the observations are report them. It is not up to the statistician to decide to remove observations from the analysis! In no case the dataset must be adapted to the statistical method (by e.g. removing outliers), but in any case the statistical method must be chosen so that the method is valid and can produce an answer to the research question.</p></li>
<li><p>Always report results in their correct units. E.g. in the blood pressure example we have estimated <span class="math inline">\(\beta_1\)</span> as <span class="math inline">\(1.79\)</span>. Thus we say write <span class="math inline">\(\hat\beta_1=1.79\)</span> mmHg / mg/day, or in words: we estimate that the blood pressure reduces on average with <span class="math inline">\(1.79\)</span> mmHg per increase of of the daily dose with 1 mg.</p></li>
<li><p>Always report estimates with confidence intervals. If the confidence interval is centered at the estimate <span class="math inline">\(\hat\beta_1=1.79\)</span> and stretches <span class="math inline">\(0.35\)</span> to either side, then the best way to report to the confidence interval is
<span class="math display">\[
(1.44 \text{ to } 2.14) \text{ mmHg / mg/day}
\]</span>
Some bad examples: <span class="math inline">\((1.44 - 2.14)\)</span> mmHg / mg/day, or (1.44, 2.14) mmHg/ mg/day, or <span class="math inline">\(1.79 \pm 0.35\)</span> mmHg / mg/day.</p></li>
<li><p>When reporting confidence intervals, the nominal coverage should also be mentioned (e.g. <span class="math inline">\(95\%\)</span> or <span class="math inline">\(90\%\)</span> confidence interval).</p></li>
<li><p>Standard errors of estimates may be reported as e.g. <span class="math inline">\(1.79\)</span> (SE=<span class="math inline">\(0.17\)</span>) mmHg / mg/day, or <span class="math inline">\(1.79\)</span> mmHg / mg/day (SE=<span class="math inline">\(0.17\)</span> mmHg / mg/day).</p></li>
<li><p>When reporting the results of a hypothesis test, always mention the null and alternative hypothesis, as well as the significance level and the <span class="math inline">\(p\)</span>-value. A hypothesis test is seldom to be performed without also estimating a population parameter (e.g. an effect size or a regression coefficient). The estimate (with confidence interval) should also be reported to complement the interpretation of the hypothesis test.</p></li>
<li><p>When in a report several results of hypothesis tests or confidence intervals are reported, you may prefer to mention the nominal significance level and confidence level only once, somewhere in the beginning of the report.</p></li>
<li><p>Make sure that you have formulated an answer to the original research question. If this is not possible (e.g. inconclusive results) then also report this.</p></li>
<li><p>The report must be written in neat English. The formulation of a conclusion from a statistical analysis must be very precise and with appropriate nuance.</p></li>
<li><p>Never copy-and-paste software output to the report, but instead extract the relevant information from the output and place it in the report in either the text or in a nicely formatted table. You may list the software output in the appendix. Exception: when the report is meant to be read by other statisticians or data scientists, you may show them the software output (as for example in these course notes).</p></li>
</ul>

</div>



<div id="app:VecDiff" class="section level1" number="7">
<h1><span class="header-section-number">A</span> Vector Differentiation</h1>
<p>For <span class="math inline">\(\mb\beta^t = (\beta_0, \ldots, \beta_{p-1})\)</span>,</p>
<p><span class="math display">\[\begin{eqnarray*}
  \frac{d}{d\beta} = \left(\begin{array}{c}
    \frac{\partial}{\partial \beta_0}\\
    \vdots\\
    \frac{\partial}{\partial \beta_{p-1}}
    \end{array}\right).
\end{eqnarray*}\]</span></p>
<p>Upon using this notation, the following differentiation rules hold for <span class="math inline">\(\mb{a}\)</span> (a <span class="math inline">\((p\times 1)\)</span> vector) and <span class="math inline">\(\mb{A}\)</span> (a symmetric <span class="math inline">\((p\times p)\)</span> matrix):</p>
<ul>
<li><p><span class="math inline">\(\frac{d(\mb\beta^t \mb{a})}{d\mb\beta} = \frac{d(\mb{a}^t\mb\beta)}{d\mb\beta} = \mb{a}\)</span></p></li>
<li><p><span class="math inline">\(\frac{d(\mb\beta^t\mb{A\beta})}{d\mb\beta} = 2\mb{A\beta}\)</span>.</p></li>
</ul>
</div>
<div id="app:LinTrans" class="section level1" number="8">
<h1><span class="header-section-number">B</span> Linear Transformations of MVN</h1>

<div class="lemma">
<span id="lem:LinTransNorm" class="lemma"><strong>Lemma B.1  (Linear transformation of MVN)  </strong></span>If <span class="math inline">\(\mb{Z}\sim \text{MVN}(\mb\mu,\mb\Sigma)\)</span> (<span class="math inline">\(n\)</span>-dimensional stochastic vector), and if <span class="math inline">\(\mb{A}\)</span> is a constant <span class="math inline">\(p\times n\)</span> matrix <span class="math inline">\(\mb{A}\)</span> of rank <span class="math inline">\(p\)</span> and <span class="math inline">\(\mb{a}\)</span> a constant <span class="math inline">\(p\times 1\)</span> vector <span class="math inline">\(\mb{a}\)</span>, then it holds that
<span class="math display">\[ 
  \mb{AZ}+\mb{a} \sim \text{MVN}(\mb{A\mu}+\mb{a},\mb{A\Sigma}\mb{A}^t).
\]</span>
</div>
</div>
<div id="app:Slutsky" class="section level1" number="9">
<h1><span class="header-section-number">C</span> Slutsky’s Theorem</h1>

<div class="theorem">
<span id="thm:Slutsky" class="theorem"><strong>Theorem C.1  (Slutsky’s Theorem)  </strong></span>Consider two sequences of stochastic variables, <span class="math inline">\(X_1,\ldots, X_n\)</span> and <span class="math inline">\(Y_1,\ldots, Y_n\)</span>.
Assume that, as <span class="math inline">\(n\rightarrow \infty\)</span>,
<span class="math display">\[
   X_n \convDistr X \;\;\;\text{ and }\;\;\; Y_n \convProb c
 \]</span>
with <span class="math inline">\(X\)</span> a stochastic variable and <span class="math inline">\(c\)</span> a constant.
Then, it holds that
<span class="math display">\[\begin{eqnarray*}
  X_n + Y_n &amp;\convDistr&amp; X+c \\
  X_n Y_n &amp;\convDistr&amp; cX \\
  X_n / Y_n &amp;\convDistr&amp; X/c \;\;\;\text{(if }1/c\text{ exists)}.
 \end{eqnarray*}\]</span>
</div>
</div>
<div id="types-of-statistical-models" class="section level1" number="10">
<h1><span class="header-section-number">D</span> Types of Statistical Models</h1>
<p>We introduce the concept of a statistical model in the setting of a single outcome <span class="math inline">\(Y\)</span> and a single regressor <span class="math inline">\(x\)</span>.</p>
<p>Given the regressor <span class="math inline">\(x\)</span>, the outcome can be described by the conditional distribution. We use the notation
<span class="math display">\[
  Y \mid x
\]</span>
to refer to the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>. The corresponding distribution function is denoted by <span class="math inline">\(F(y\mid x)\)</span> and the density function <span class="math inline">\(f(y\mid x)\)</span>.</p>
<p>Without any further restrictions on this conditional distribution, we say that it is a <strong>nonparametric model</strong>. However, often we imply further restrictions on the conditional distribution. Let us start with a simple example. Suppose that <span class="math inline">\(x\)</span> is a 0/1 indicator (0: placebo and 1: active treatment). Then we can write <span class="math inline">\(Y \mid x=0 \sim F(y\mid x=0)\)</span> and <span class="math inline">\(Y \mid x=1 \sim F(y\mid x=1)\)</span>. But again, without any further restrictions, this is a nonparametric model, because it only says that for <span class="math inline">\(x=0\)</span> and <span class="math inline">\(x=1\)</span> the distribution of <span class="math inline">\(Y\)</span> is given by two distinct distribution function, but there is no relationship between the distributions assumed. Even in this nonparametric model, we can identify parameters. For example,
<span class="math display">\[
  \mu_0 = E(Y \mid x=0) = \int y f(y\mid x=0) dy  
\]</span>
and
<span class="math display">\[
  \mu_1 = E(Y \mid x=1) = \int y f(y\mid x=1) dy.
\]</span>
So we have two meaningful parameters, but they do not impose any restrictions on <span class="math inline">\(f(y\mid x=0)\)</span> and <span class="math inline">\(f(y\mid x=1)\)</span>.
These nonparametric models can be represented by the set of all proper conditional distribution functions. For the example with <span class="math inline">\(x=0/1\)</span> this can be written as
<span class="math display">\[
  \left\{ F(\cdot \mid x): F(\cdot \mid x) \text{ is a distribution function}, x\in \{0,1\} \right\}.
\]</span></p>
<p>The model can be turned into a <strong>parametric model</strong>, by imposing strong distributional assumptions on <span class="math inline">\(f(y\mid x=0)\)</span> and <span class="math inline">\(f(y\mid x=1)\)</span>. For example, (<span class="math inline">\(x=0,1\)</span>)
<span class="math display">\[
  Y \mid x \sim N(\mu_x, \sigma^2).
\]</span>
This model says that when <span class="math inline">\(x=0\)</span>, the outcome has a normal distribution with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, and when <span class="math inline">\(x=1\)</span>,the outcome has a normal distribution with mean <span class="math inline">\(\mu_1\)</span> and also variance <span class="math inline">\(\sigma^2\)</span>. Thus the parameters <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\mu_1\)</span> have the same interpretation as in the nonparametric model, but now the distributions are explicitly specified and there is the additional restriction that the variances in the two <span class="math inline">\(x=0/1\)</span> groups are identical.</p>
<p>Another example of a parametric model is the simple linear regression model. Now <span class="math inline">\(x\)</span> is a continuous regressor. We write the models as
<span class="math display">\[
   Y \mid x \sim N(\beta_0 +\beta_1 x, \sigma^2).
\]</span>
Thus, for each <span class="math inline">\(x\)</span> the model completely specifies the distribution of the outcome. The model specification also included a description of how the mean of the outcome varies with <span class="math inline">\(x\)</span>.</p>
<p>Let us write this parametric model in a more generic way. Let <span class="math inline">\(\theta\)</span> denote the parameter vector with <span class="math inline">\(p\)</span> elements and let <span class="math inline">\(F_\theta(y \mid)\)</span> denote the distribution function of the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span>. This distribution is thus completely known, up to some parameter <span class="math inline">\(\theta\)</span> that has to be estimated from the data. If <span class="math inline">\(\theta\)</span> contains real-values parameter, we have <span class="math inline">\(\theta \in \mathbb{R}^p\)</span>. The statistical model can then be represented by the following set of conditional distribution functions,
<span class="math display">\[
  \left\{ F(\cdot\mid x) :  F(\cdot\mid x)=F_\theta(\cdot\mid x), \theta\in\mathbb{R}^p, x\in \mathbb{R} \right\}.
\]</span></p>
<p>We can also define <strong>semiparametric models</strong>. These are models that do not complete specify the conditional distribution, but still they imply restrictions. For example, we could impose the following linear model for the conditional mean,
<span class="math display">\[
  E(Y \mid x) = \beta_0 + \beta_1 x
\]</span>
without any further restrictions on the conditional distribution. This is not a nonparametric model, because there is a restriction on some aspects of the distribution (here: the conditional mean). It is neither a parametric model, because up to the parameter <span class="math inline">\(\theta^t=(\beta_0,\beta_1)\)</span>, the distribution is not completely specified. This is an example of a <strong>semiparametric model</strong>. It can be represented by the following set of conditional distribution functions,
<span class="math display">\[
  \left\{ F(\cdot\mid x) :  E_F(Y\mid x)=\beta_0 + \beta_1 x, (\beta_0,\beta_1) \in\mathbb{R}^2, x\in \mathbb{R} \right\},
\]</span>
in which the subscript <span class="math inline">\(F\)</span> in <span class="math inline">\(E_F(Y\mid x)\)</span> is used to stress that the expectation is defined w.r.t. the distribution function <span class="math inline">\(F\)</span>.</p>
<p>Semiparametric models are not limited to include restrictions on the conditional mean. It could for example also involve restrictions on the variance,
<span class="math display">\[
  \left\{ F(\cdot\mid x) :  E_F(Y\mid x)=\beta_0 + \beta_1 x,  \text{ and } \text{Var}(Y\mid x)=\sigma^2, (\beta_0,\beta_1) \in\mathbb{R}^2, \sigma^2 \in \mathbb{R}^+, x\in \mathbb{R} \right\}.
\]</span></p>
<p>So far we have always looked at conditional distribution functions, i.e. <span class="math inline">\(x\)</span> is considered a fixed constant. Sometimes <span class="math inline">\(x\)</span> can be random as well. For example, reconsider the example with <span class="math inline">\(x\)</span> a binary 0/1 indicator for two treatments. In a randomised clinical trial, the treatment allocation happens completely at random. Thus <span class="math inline">\(x\)</span> is a random variable; it is thus better to write it as the capital letter <span class="math inline">\(X\)</span>. In this example,
<span class="math display">\[
  X \sim \text{Binom}(1,1/2).
\]</span>
This marginal distribution for <span class="math inline">\(X\)</span>, together with the model for the conditional distribution of <span class="math inline">\(Y\mid X\)</span>, specifies the joint statistical model of <span class="math inline">\((Y,X)\)</span>.</p>
</div>
<div id="r-session-info" class="section level1" number="11">
<h1><span class="header-section-number">E</span> R Session Info</h1>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="#cb636-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code></pre></div>
<pre><code>## R version 4.1.0 (2021-05-18)
## Platform: aarch64-apple-darwin20 (64-bit)
## Running under: macOS Big Sur 11.3.1
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib
## 
## Random number generation:
##  RNG:     Mersenne-Twister 
##  Normal:  Inversion 
##  Sample:  Rounding 
##  
## locale:
## [1] C
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] multcomp_1.4-17 TH.data_1.0-10  survival_3.2-13 mvtnorm_1.1-2  
##  [5] car_3.0-11      carData_3.0-4   dagitty_0.3-1   GGally_2.1.2   
##  [9] plotly_4.9.4.1  ggplot2_3.3.5   skimr_2.1.3     tidyr_1.1.3    
## [13] dplyr_1.0.7     MASS_7.3-54    
## 
## loaded via a namespace (and not attached):
##  [1] httr_1.4.2         jsonlite_1.7.2     viridisLite_0.4.0  splines_4.1.0     
##  [5] highr_0.9          cellranger_1.1.0   yaml_2.2.1         pillar_1.6.2      
##  [9] lattice_0.20-44    glue_1.4.2         digest_0.6.27      RColorBrewer_1.1-2
## [13] colorspace_2.0-2   sandwich_3.0-1     htmltools_0.5.2    Matrix_1.3-4      
## [17] plyr_1.8.6         pkgconfig_2.0.3    haven_2.4.3        bookdown_0.24     
## [21] purrr_0.3.4        scales_1.1.1       openxlsx_4.2.4     rio_0.5.27        
## [25] tibble_3.1.4       generics_0.1.0     farver_2.1.0       ellipsis_0.3.2    
## [29] withr_2.4.2        repr_1.1.3         lazyeval_0.2.2     cli_3.0.1         
## [33] magrittr_2.0.1     crayon_1.4.1       readxl_1.3.1       evaluate_0.14     
## [37] fansi_0.5.0        forcats_0.5.1      foreign_0.8-81     tools_4.1.0       
## [41] data.table_1.14.0  hms_1.1.0          lifecycle_1.0.0    stringr_1.4.0     
## [45] V8_3.4.2           munsell_0.5.0      zip_2.2.0          compiler_4.1.0    
## [49] jquerylib_0.1.4    rlang_0.4.11       grid_4.1.0         rstudioapi_0.13   
## [53] htmlwidgets_1.5.4  crosstalk_1.1.1    base64enc_0.1-3    labeling_0.4.2    
## [57] rmarkdown_2.10     boot_1.3-28        gtable_0.3.0       codetools_0.2-18  
## [61] abind_1.4-5        reshape_0.8.8      curl_4.3.2         R6_2.5.1          
## [65] zoo_1.8-9          knitr_1.33         fastmap_1.1.0      utf8_1.2.2        
## [69] stringi_1.7.4      Rcpp_1.0.7         vctrs_0.3.8        tidyselect_1.1.1  
## [73] xfun_0.25</code></pre>

</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DASM2.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
